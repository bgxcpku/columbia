\relax 
\citation{Wood2009}
\citation{Teh2006a,Wood2009,Gasthaus2010}
\citation{Wood2009}
\citation{Shannon1951}
\citation{Gasthaus2010}
\citation{Wood2009}
\citation{Wood2009}
\newlabel{fig1a}{{1(a)}{2}}
\newlabel{sub@fig1a}{{(a)}{2}}
\newlabel{fig1b}{{1(b)}{2}}
\newlabel{sub@fig1b}{{(b)}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Memory complexity and estimation computational cost (in units of graphical model ``nodes'') versus $n$ of $n$-gram for the SM and a smoothing $n$-gram model. There is one pair of lines for each of 5 different observation sequence lengths. The horizontal dotted lines are the SM computational requirements. The quadratic solid lines are the computational requirements the smoothing $n$-gram models. The SM always uses all contextual information when performing predictive inference. The $n$-gram model discards contextual observations more distant than $n-1$. None-the-less (a) shows that the computational complexity of each standard $n$-gram model exceeds that of a sequence memoizer trained on the same data for all $n>5$ and for all training observation sequence lengths. This suggests that the sequence memoizer should be considered as an alternative to $n$-gram language modeling if long-range contextual dependencies matter. The data used in this experiment was an excerpt of the New York times corpus. (b) Held-out test perplexity for two language models trained on growing excerpts of the Associated Press news corpus. 5-gram is a hierarchical Pitman-Yor process language model (a generalization of a Kneser Ney smoothed 5-gram model). SM is the sequence memoizer. Training sequence length ranges from 2-12 million tokens which we believe to be too small to characterize the benefit of being able to model and use long range dependencies. Regardless, (b) suggests that held-out perplexity under the SM seems to improve relative to the smoothed $n$-gram model as more data is introduced into the model. We conjecture that this is due to increasing prevalence of meaningful long contexts. Experiments such as those proposed herein on larger corpora would establish the veracity of this conjecture. Both of these figures were taken from \cite  {Wood2009}.}}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{2}}
\newlabel{default}{{1}{2}}
\citation{Gasthaus2010}
\citation{Bartlett2010}
\citation{Bartlett2010}
\bibdata{../../../papers/uber.bib}
\bibcite{Bartlett2010}{{1}{2010}{{Bartlett et~al.}}{{}}}
\bibcite{Gasthaus2010}{{2}{2010}{{Gasthaus et~al.}}{{}}}
\bibcite{Shannon1951}{{3}{1951}{{Shannon}}{{}}}
\bibcite{Teh2006a}{{4}{2006}{{Teh}}{{}}}
\bibcite{Wood2009}{{5}{2009}{{Wood et~al.}}{{}}}
\bibstyle{apalike}
