\documentclass{beamer}
\usepackage[numbers]{natbib}

% \usepackage{beamerthemesplit} // Activate for custom appearance

\title{W4240 Data Mining}
\author{Frank Wood}
\date{\today}

\input{../definitions/definitions.tex}

\begin{document}

\frame[t]{\titlepage}

%\section[Outline]{}
%\frame[t]{\tableofcontents}

\section{Introduction}
\subsection{Overview of Topics}

\section{}
\subsection{}

\frame[t] { 
\frametitle{Introduction}
\begin{itemize}
\item Data mining is the search for patterns in large collections of data
\begin{itemize}
\item Learning models
\item Applying models to large quantities of data
\end{itemize}
\item Pattern recognition is concerned with {\em automatically} finding patterns in data / learning models
\item Machine learning is pattern recognition with concern for computational tractability and full automation
\item Data mining = Machine Learning = Applied Statistics
\begin{itemize}
\item Scale
\item {\em Computation}
\end{itemize}

\end{itemize}
}


\frame[t] { 
\frametitle{Example Application: ALARM, expert diagnostic system}
Goal: Inference in given/known/hand-specified Bayesian network 
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=11cm]{"alarm_network"}\caption{ALARM stands for 'A Logical Alarm Reduction Mechanism'. This is a medical diagnostic system for patient monitoring. It is a nontrivial belief network with 8 diagnoses, 16 findings and 13 intermediate variables. Described in \cite{Beinlich1989}}
\label{fig:1_1}
\end{center}
\end{figure}
}


\frame[t] { 
\frametitle{Graphical Models}

\begin{itemize}
\item ALARM network and most other probabilistic models can be expressed in the ``language'' of graphical models.  
\item Inference procedures such as the \underline{sum-product algorithm} and \underline{belief propagation} are general inference techniques that can be run on {\em any} discrete or linear-Gaussian graphical model.
\end{itemize}

\begin{figure}[htbp]
\begin{center}
\includegraphics{"../prmlfigs-pdf-recolored/Figure8_22a"}\caption{Directed Graphical Model : Chapter 8, Figure 22a, PRML \cite{Bishop2006}}
\label{fig:8_22a}
\end{center}
\end{figure}
}

\frame[t] { 
\frametitle{Graphical Models Cont.}
Results
\begin{itemize}
\item Ability to compute marginal distribution of any subset of variable in the graphical model conditioned on any other subset of variables (values observed / fixed)
\item Generalizes many, many inference procedures such as Kalman filter, forward-backward, etc.
\item Can be used for parameter estimation in the case where all latent, unknown variables are ``parameters'' and all observations are fixed, known variables.
\end{itemize}

}

\frame[t] { 
\frametitle{Another Application: Classification of handwritten digits}
Goal
\begin{itemize}
\item Build a machine that can identify handwritten digits automatically
\end{itemize}
Approaches
\begin{itemize}
\item Hand craft a set of rules that separate each digit from the next
\item Set of rules invariably grows large and unwieldy and requires many ``exceptions''
\item ``Learn'' a set of models for each digit automatically from labeled training data, i.e.~{\em mine} a large collection of handwritten digits and produce a model of each
\item Use model to do classification
\end{itemize}
Formalism
\begin{itemize}
\item Each digit is 28x28 pixel image
\item Vectorized into a 784 entry vector $\x$ 
\end{itemize}
}

\frame[t] { 
\frametitle{Handwritten Digit Recognition Training Data}
\begin{figure}[htbp]
\begin{center}
\includegraphics{"../prmlfigs-pdf-recolored/Figure1_1"}\caption{Hand written digits from the USPS}
\label{fig:1_1}
\end{center}
\end{figure}
}


\frame[t] { 
\frametitle{Machine learning approach to digit recognition}
Recipe
\begin{itemize}
\item Obtain a of $N$ digits $\{\x_1, \ldots, \x_N\}$ called the {\em training set}.
\item Label (by hand) the training set to produce a label or ``target'' $\t$ for each digit image $\x$
\item Learn a function $\y(\x)$ which takes an image $\x$ as input and returns an output in the same ``format'' as the target vector.
\end{itemize}
Terminology
\begin{itemize}
\item The process of determining the precise shape of the function $\y$ is known as the ``training'' or ``learning'' phase.
\item After training, the model (function $\y$) can be used to figure out what digit unseen images might be of.  The set comprised of such data is called the ``test set''
\end{itemize}

}

\frame[t] { 
\frametitle{Tools for the handwriting recognition job}
Supervised Regression/Classification Models
\begin{itemize}
\item Logistic regression
\item Neural networks
\item Support vector machines
\item Naive Bayes classifiers
\end{itemize}
Unsupervised Clustering
\begin{itemize}
\item Gaussian mixture model
\end{itemize}

Model Parameter Estimation
\begin{itemize}
\item Maximum likelihood / Expectation Maximization
\item Variational inference
\item Sampling
\item Sequential Monte Carlo
\begin{itemize}
\item... for all, batch or online
\end{itemize}
\end{itemize}

}


\frame[t] { 
\frametitle{Example Application: Trajectory Inference From Noisy Data}
Goal
\begin{itemize}
\item Build a machine that can uncover and compute the true trajectory of an indirectly and noisily observed moving target
\end{itemize}
Approaches
\begin{itemize}
\item Hand craft a set of rules that govern the possible movements of said target
\item Set of rules invariably grows large and unwieldy and requires many ``exceptions''
\item ``Learn'' a model of the kind of movements such a target can make and perform inference in said model
\end{itemize}
Formalism
\begin{itemize}
\item Example observed trajectories $\{\xbf_{n}\}_{n=1}^N$
\item Unobserved latent trajectories $\{\zbf_{n}\}_{n=1}^N$ 
\end{itemize}
}

\frame[t] { 
\frametitle{Latent trajectory Inference}
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=8cm]{"bomb_tracking"}\caption{Schematic of trajectory inference problem}
\label{fig:1_1}
\end{center}
\end{figure}
}

\frame[t] { 
\frametitle{Tools for Latent Trajectory Inference}
Known/hand-crafted model, inference only
\begin{itemize}
\item Belief propagation
\item Kalman filter
\item Particle filter
\item Switching variants thereof
\item Hidden Markov Models
\end{itemize}
Learning too / Model Parameter Estimation
\begin{itemize}
\item Maximum likelihood / Expectation Maximization
\item Variational inference
\item Sampling
\item Sequential Monte Carlo
\begin{itemize}
\item... for all, batch or online
\end{itemize}
\end{itemize}

Trajectory need not be ``physical,'' could be an economic indicator, completely abstract, etc.
}

\frame[t] { 
\frametitle{Cool Trajectory Inference Application : Neural Decoding}
\begin{figure}[htbp]
\begin{center}
\includegraphics[trim = 20mm 20mm 20mm 50mm, clip, width=10cm]{"neural_decoding"}\caption{Actual and predicted hand positions (predicted from neural firing rates alone using a Kalman filter) \cite{Wu2002}}
\label{fig:1_1}
\end{center}
\end{figure}
}


\frame[t] { 
\frametitle{Another Application: Unsupervised Clustering}
Forensic analysis of printed documents, infer printer used to print document from visual features.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=6cm]{"printer_clustering"}\caption{PCA projection of printer features \cite{Ali2004}}
\label{fig:1_1}
\end{center}
\end{figure}

}

\frame[t] { 
\frametitle{Another Unsupervised Clustering Application}
Automatic discovery of number of neurons and assignment of waveforms to neurons.  Essential to electrophysiological study of the brain.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=6cm]{"sorted_waveforms"}\caption{Automatically sorted action potential PCA projections \cite{Wood2008b}}
\label{fig:1_1}
\end{center}
\end{figure}

}

\frame[t] { 
\frametitle{A Big Unsupervised Clustering Application}
Multinomial mixture model automatic document clustering for information retrieval.

\begin{eqnarray*}
z_n | \pibf &\sim& \Discrete(\pibf) \\
\xbf_n | z_n = k, \Thetabf &\sim& \Mult(\thetabf_{z_n})
\end{eqnarray*}

where $\xbf_n$ is a bag of words or feature representation of a document, $z_n$ is a per document class indicator variable, $ \Thetabf = \{\thetabf_k\}_{k=1}^K$ is a collection of probability vectors over types (or features) (per cluster $k$), and $\pibf = [\pi_1, \ldots, \pi_K], \sum_k \pi_k = 1$ is the class prior.
\newline

Such a model can be used to cluster similar documents together for information retrieval (Google, Bing, etc.) purposes.
}

\frame[t] { 
\frametitle{Tools for Unsupervised Clustering}
Known/hand-crafted model, inference only
\begin{itemize}
\item K-means
\item Gaussian mixture models
\item Multinomial mixture models
\end{itemize}
Learning too / Model Parameter Estimation
\begin{itemize}
\item Maximum likelihood / Expectation Maximization
\item Variational inference
\item Sampling
\item Sequential Monte Carlo
\begin{itemize}
\item... for all, batch or online
\end{itemize}
\end{itemize}
}

\frame[t] { 
\frametitle{Tools for All}
\begin{itemize}
\item Maximum likelihood / Expectation Maximization
\item Variational inference
\item Sampling
\item Sequential Monte Carlo
\begin{itemize}
\item... for all, batch or online
\end{itemize}
\end{itemize}
}

\frame[t] { 
\frametitle{Links and Syllabus}
	Course home page :  \href{http://www.stat.columbia.edu/~fwood/w4240/}{http://www.stat.columbia.edu/~fwood/w4240/}
	
 	 Guest lectures may be sprinkled throughout the course.
}

\frame[t] { 
\frametitle{Prerequisites}
\begin{itemize}
\item Linear Algebra
\item Multivariate Calculus (Matrix and Vector calculus)
\item Probability and Statistics at a Graduate Level
\item Programming experience in some language like pascal, matlab, c++, java, c, fortran, scheme, etc.
\end{itemize}

Good idea to familiarize yourself with PRML \cite{Bishop2006} Chapter 2 and Appendices B,C,D, and E.

In particular
\begin{itemize}
\item Multivariate Gaussian distribution
\item Discrete, Multinomial, and Dirichlet distributions
\item Lagrange Multipliers
\item Matlab
\end{itemize}


}



	\bibliographystyle{plainnat}
	\begin{frame}[t,allowframebreaks]{Bibliograpy}

\bibliography{../../../../../papers/uber.bib}
\end{frame}



\end{document}