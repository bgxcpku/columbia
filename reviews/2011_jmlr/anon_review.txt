Multi-assignment Clustering for Boolean DataReviewSummaryThis paper is a well-written, straightforward maximum likelihood probabilistic binary matrix factorization result in which a several slight modifications on bit-flipping likelihoods are shown to be roughly equivalent parametrically.  An annealed expectation maximization (EM) estimation algorithm is derived for a common model.  Unnecessarily expansive experimental evidence is provided that shows good performance of the resulting estimator and model on synthetic data and real-world user role clustering tasks.Big criticisms:This reader respectfully suggests that the balance of the paper is a little off and that some very important details are missing that should prohibit the paper from being published as is (will recommend minor revision with the understanding that some of what will be suggested is not perhaps as minor as the authors might desire).  Likewise, in particular, the experimentation is full of experiments that are valuable to the experimenter but not necessarily as much to a reader.  Computational cost:The first missing bit is any experimentation or understanding of the computational complexity of the suggested inferential strategy.  As the EM algorithm requires summing over \Lambda repeatedly (an exponential sum), the cost of inference would seem to grow exponentially in K, and, if as hinted, a constraint on the number of clusters M is imposed, it is not at all clear how to implement this.   This computational complexity issue should be explained and/or compared between the different models.Implicit regularizationThis reader’s largest concern, and something that will require some additional experimentation to explore is the implicit regularization imposed by the equation on 14.  While the given algorithm looks like regular ML EM, there is some question about what model is actually being learned.  Another way of putting this is: what’s the implied prior on \beta and P(\Lambda)?  Or, why not explicitly specify a prior and do type-two maximum likelihood?  I can imagine B_k,d \sim Beta(1,1) as being the max ent. prior.  The corresponding prior for Lambda is a little harder to imagine except to define it as you suggest it might be, i.e. 1/{0,1}^K.  It would help convince this reader if a prior equivalent to the implied regularization could be explicitly provided. On this same topic, if either experimental or theoretical evidence could be provided that this estimator actually achieves a maximum likelihood solution rather than a MAP solution under some implied regularization.  Perhaps this could be done in simulation.No mention is made of model selection (how to select K).  This reader considers a set of experiments on this imperative.   How is the proposed model / algorithm behaved as K gets large?  For instance, if one is to properly compare against the non-parametric IBP-based prior art, then both the dimension of the factorization and the factorization itself should be learned simultaneously.  The implicit prior on \Lambda gets even more curious when thinking about the model in this way.Lastly, the IBP-based method is a Bayesian method.  No mention is made of how results for this method were computed.  Are the IBP results expectations or MAP results?  Often taking expectations improved the non-parametric modeling results dramatically.  I believe that with some effort, all of the performance criteria examined (or close variants thereof) could be expressed as posterior averaging computations.  I believe that the resulting comparison (along with model selection) would make the experimental results far more engaging.In general, to make room for many of these suggestions, this reader believes that many of the synthetic data experiments and the explanatory prose interpreting those results could safely and without loss of value, be removed.Little criticisms:The text introducing D in paragraph 2 of page three is unnecessarily complicated and confusing.  D is a dimension of x.  In definition 2 the circle enclosing a multiply (I can’t remember the latex for this symbol) is introduced without definition.  This could be read as a Hadamard product potentially confusing a reader.  I don’t think an extra symbol is needed to indicate matrix multiplication.  z and u are matrices and they are being multiplied.  Regular multiplication should be fine.Page 5: A Dirichlet prior -> A Dirichlet process prior.In addition to the graphical model, this reviewer always appreciates a labeled matrix multiplication figure.  The first thing I do when reading one of these papers is draw such a figure to figure out what’s going on.  Why not provide one?  Page 7, first full paragraph: missing superscript s’s on p_S(x_{I,d}^S = | …) (two)‘proxy’ -> `` proxy’’Page 7: second to last paragraph: When, during the course of the paper, is the number of clusters to which an object can be simultaneously assigned bounded by M?  It isn’t clear that this is ever done which could potentially hurt SAC, particularly in head-to-head comparisons.Figure 1 : include arrows in the directed graphical model.  \Lambda should have a subscript.Bottom of page 13, one could at least make mention of the fact that parameters could be tied in a hierarchical way, potentially eliminating the identified problem.  I think that section 4.1 could be moved behind 4.2-> 4.4.  I understand the reasons for putting this section here, but, if it is to remain, I think that more help for the reader should be provided.  For instance, “homotopy method” should either be defined or replaced with less jargony prose.  Is the vertical axis on Fig. 4b correct?  In general I suggest removing references to plot colors in the text and refer solely to the differentiating glyphs.  Most people will not print the paper in color.  Referring to color simply causes confusion.Fig. 6: Instability could not be computed for INO: this raises several flags and suggests that instability might not be the best measure to be using.  Is this really the best way of examining stability? Page 27 grammar “as” -> “than” twice.  Page 30 “looses” -> “loses” 