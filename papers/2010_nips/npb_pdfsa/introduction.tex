% !TEX root = main.tex
\section{Introduction}
All learning algorithms must strike a compromise between computational efficiency, model complexity, and generalization performance.  One extreme in terms of searching for minimally complex models is to search for the shortest computer program that can reproduce the observed data exactly. Unfortunately performing this search is well known to be computationally challenging, and, even if found, any single resulting program is not likely to generalize well given only small amounts of data generated by a complex mechanism.   A mixture of such programs (each being penalized by length), on the other hand, is the universal prior defined by Solomonoff and would reasonably be expected to generalize well.  Of course, given that searching for even a single such program is costly, learning a mixture of such programs becomes staggeringly expensive.  

The focus of this paper is a similar in spirit but significantly simpler task.  We wish to learn a mixture of probabilistic deterministic finite automata any of which could have generated the observed data.  of Bayesian learning of a probabilistic deterministic infinite automata (PDIA).  A probabilistic deterministic automata with an infinite number of states is a  a class of models that includes variable and fixed order Markov models as a special case, as well as simpler models.  

%At a high level, a PDFA can be thought of as a hidden Markov model for which, given an observed sequence, there is only one possible path through the hidden states.  A more formal definition follows in section 2.

We motivate our choice of model class several ways.  First, it can be shown by a simple argument that, given an infinite and stationary sequence, the minimal sufficient statistics of the past for predicting the future form a PDFA \cite{Crutchfield1999}.  That is, given two pasts that map to the same statistic, the concatenation of those pasts with the same symbol will map to the same statistic.  This is not the case in general hidden Markov models, where many transitions are possible from a hidden state, though observed data may change the posterior probability of those transitions.  Existing algorithms for learning these statistics use tests that have asymptotic guarantees but may not work well for reasonable amounts of data \cite{Shalizi2004}.

Another argument is that we are trying to generalize the class of variable-length Markov models, which have had great empirical success in sequence prediction.

N-gram models have had great empirical success in sequence prediction.  However, there is no clear way to trade off model complexity with prediction accuracy.  In extensions to n-gram models which can learn from arbitrarily long contexts, the model complexity will grow without bound, even for trivially simple sequences such as repetitions of a few characters.  Bounded memory models perform well in practice, but we would much prefer to learn a model that is as small as possible for very simple data, while growing large for more complex data.  A natural class of models to explore is probabalistic deterministic finite automata (PDFA), which contains n-grams as a special case, as well as simpler models.  We define a prior over PDFAs with a finite number of states and describe a Metropolis-Hastings algorithm to generate samples from the posterior.  We then generalize to the case of PDFAs with a (potentially) infinite number of states and show that the generative model is a type of Hierarchical Dirichlet Process (HDP).  We then (I hope!) describe a state splitting/merging algorithm for posterior inference that mixes more efficiently than the original Metropolis-Hastings algorithm, and show that it defines a natural hierarchy of states for smoothing (fingers crossed...)  The set of strings produced by a PDFA constitutes a probabilistic regular language, thus our inference procedure can also be viewed as a non-greedy algorithm for regular grammar induction.

Finally, 