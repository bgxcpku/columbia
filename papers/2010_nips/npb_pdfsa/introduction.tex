% !TEX root = main.tex
\section{Introduction}
\comment{All learning algorithms must strike a compromise between computational efficiency, model complexity, and generalization performance.  One extreme in terms of searching for minimally complex models is to search for the shortest computer program that can reproduce the observed data exactly. Unfortunately performing this search is well known to be computationally challenging, and, even if found, any single resulting program is not likely to generalize well given only a small number of observations\comment{generated by a complex mechanism}.   A mixture of such programs weighted by length, on the other hand, is the universal prior defined by Solomonoff \cite{Solomonoff1964,Solomonoff1978} and has good theoretical generalization properties.  Of course, given that searching for a single such program is costly, learning a mixture of programs becomes practically impossible.  

The focus of this paper is a similar in spirit but restricts the search in ways that render our approach computable.  Instead of learning a mixture of programs that can reproduce the data exactly, we, in effect, learn a mixture of probabilistic deterministic finite automata (PDFA), each of which can reproduce the data exactly.   We do this using a novel Bayesian framework for PDFA learning.  In this framework we first specify a prior over the parameters of a single large PDFA that encourages state reuse.  The inductive bias introduced by the prior can be interpreted as a kind of soft constraint that limits the number of states actually used by the PDFA to generate the observed data (its ``complexity'').   Being a Bayesian approach, we retain and average over our uncertainty about both the cardinality of states in the automata used to generate the data, the links between those states, and the emission distributions.  The posterior distribution over PDFA parameter settings can be interpreted as an infinite mixture over PDFAs.  A set of samples drawn from this distribution via, for instance, Markov chain Monte Carlo (MCMC) can be interpreted as a finite sample approximation to this infinite mixture, where again each sample is a PDFA of potentially varying complexity.  When performing inference we average over this posterior distribution, yielding a novel approach to smoothing over PDFAs, a technique known to produce good generalization results  \cite{pdfa_smoothing_approaches}.} % with a different number of states (only those that are used to generate the data matter), different emission distributions, and so forth.  
%The expressivity of a single PDFA with, to be redundant but necessarily pedantic, a finite number of states, is restrictive.  A mixture of PDFAs is less restrictive, in fact, it is know

The focus of this paper is a novel Bayesian framework for learning probabilistic deterministic finite automata (PDFA).  A PDFA is a generative model for sequential data.  It is similar to a hidden Markov model in that there is a set of states, each of which has a probability of emitting an observed symbol.  It differs from an HMM in how a state transitions to the next state, as described in detail in Section \ref{sec:PDFA}.  In our framework for PDFA learning we first specify a prior over the parameters of a single large PDFA that encourages state reuse.  The inductive bias introduced by the prior can be interpreted as a soft constraint on the number of states used to generate the data.  We then take the limit as the number of states becomes infinite, yielding a model we call the probabilistic deterministic infinite automata (PDIA).  The posterior distribution over all possible parameters of the PDIA is equivalent to an infinite mixture of PDFAs.  Samples drawn from this distribution via MCMC or other method form a finite sample approximation to this infinite mixture.  This makes it possible to average over our uncertainty about the model parameters in a Bayesian way.  Averaging over a finite number of PDFAs leads to significantly better predictive performance than a single estimated PDFA.  We are interested in PDFAs because they are intermediate in expressive power between HMMs and finite-order Markov models, and thus we suspect strike a good balance between ease of learning and generalization performance.

A single PDFA is known to have relatively limited expressivity.  We demonstrate that a finite mixture of PDFAs has greater expressivity than that of a single PDFA but is not as expressive as a probabilistic nondeterministic finite automata (PNFA)\footnote{An equivalent PNFA can be constructed for any given hidden Markov model \cite{Dupont2005} }.  A PDFA with an infinite number of states (a PDIA) is clearly highly expressive; an infinite mixture over the same is even more so.  Even though we will define a Bayesian approach to PDIA learning, the fact that we will only ever observe a finite amount of data and draw a finite number of posterior samples stipulates that we will only ever compute with finite mixtures of finite PDFAs and thus limit our discussion about expressivity to such models.   

While model expressivity is a concern, computational considerations often dominate model choice.  We show that inference in a trained mixture of PDFAs has significant computational advantages relative to doing forward inference in the PNFA (hidden Markov model, HMM, see previous footnote) class of models.   In tandem with this result, we present preliminary empirical evidence that suggests that on various natural data prediction tasks averaging over PDFAs gives performance comparable to state-of-the art HMM performance at reduced asymptotic cost.  While HMM computation is highly optimized, alternate ways to structure (approximate) computation may encourage cross-field.

The remainder of the paper is organized as follows:  Section \ref{sec:PDFA} reviews PDFAs, Section \ref{sec:BPDFAs} introduces Bayesian PDFA inference, Section \ref{sec:theory} discusses the theoretical expressive power of mixtures of PDFAs and experimental results on DNA and natural language are presented in Section \ref{sec:results}.  Supplemental figures and a comparison to related work can be found in the appendix.


\comment{
 We wish to learn a mixture of probabilistic deterministic finite automata any of which could have generated the observed data.  of Bayesian learning of a probabilistic deterministic infinite automata (PDIA).  A probabilistic deterministic automata with an infinite number of states is a  a class of models that includes variable and fixed order Markov models as a special case, as well as simpler models.  

%At a high level, a PDFA can be thought of as a hidden Markov model for which, given an observed sequence, there is only one possible path through the hidden states.  A more formal definition follows in section 2.
We motivate our choice of model class several ways.  First, it can be shown by a simple argument that, given an infinite and stationary sequence, the minimal sufficient statistics of the past for predicting the future form a PDFA \cite{Crutchfield1999}.  That is, given two pasts that map to the same statistic, the concatenation of those pasts with the same symbol will map to the same statistic.  This is not the case in general hidden Markov models, where many transitions are possible from a hidden state, though observed data may change the posterior probability of those transitions.  Existing algorithms for learning these statistics use tests that have asymptotic guarantees but may not work well for reasonable amounts of data \cite{Shalizi2004}.

Another argument is that we are trying to generalize the class of variable-length Markov models, which have had great empirical success in sequence prediction.

N-gram models have had great empirical success in sequence prediction.  However, there is no clear way to trade off model complexity with prediction accuracy.  In extensions to n-gram models which can learn from arbitrarily long contexts, the model complexity will grow without bound, even for trivially simple sequences such as repetitions of a few characters.  Bounded memory models perform well in practice, but we would much prefer to learn a model that is as small as possible for very simple data, while growing large for more complex data.  A natural class of models to explore is probabalistic deterministic finite automata (PDFA), which contains n-grams as a special case, as well as simpler models.  We define a prior over PDFAs with a finite number of states and describe a Metropolis-Hastings algorithm to generate samples from the posterior.  We then generalize to the case of PDFAs with a (potentially) infinite number of states and show that the generative model is a type of Hierarchical Dirichlet Process (HDP).  We then (I hope!) describe a state splitting/merging algorithm for posterior inference that mixes more efficiently than the original Metropolis-Hastings algorithm, and show that it defines a natural hierarchy of states for smoothing (fingers crossed...)  The set of strings produced by a PDFA constitutes a probabilistic regular language, thus our inference procedure can also be viewed as a non-greedy algorithm for regular grammar induction.

Finally, }