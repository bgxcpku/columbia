\section{Introduction}
All learning algorithms must strike a compromise between computational efficiency, model complexity and predictive accuracy.  In this paper we focus on optimizing the last two on the task of discrete sequence prediction.  In the extreme limit, the best solution would be to find the shortest computer program that can reproduce the data exactly, but this sacrifices computational efficiency to the point that it is incomputable.  Instead, we tackle the much more pragmatic problem of learning probabilistic deterministic finite automata (PDFA), a class of models that includes variable and fixed order Markov models as a special case, as well as simpler models.  At a high level, a PDFA can be thought of as a hidden Markov model for which, given an observed sequence, there is only one possible path through the hidden states.  A more formal definition follows in section 2.

We motivate our choice of model class several ways.  First, it can be shown by a simple argument that, given an infinite and stationary sequence, the minimal sufficient statistics of the past for predicting the future form a PDFA \cite{Crutchfield1999}.  That is, given two pasts that map to the same statistic, the concatenation of those pasts with the same symbol will map to the same statistic.  This is not the case in general hidden Markov models, where many transitions are possible from a hidden state, though observed data may change the posterior probability of those transitions.  Existing algorithms for learning these statistics use tests that have asymptotic guarantees but may not work well for reasonable amounts of data \cite{Shalizi2004}.

Another argument is that we are trying to generalize the class of variable-length Markov models, which have had great empirical success in sequence prediction.

N-gram models have had great empirical success in sequence prediction.  However, there is no clear way to trade off model complexity with prediction accuracy.  In extensions to n-gram models which can learn from arbitrarily long contexts, the model complexity will grow without bound, even for trivially simple sequences such as repetitions of a few characters.  Bounded memory models perform well in practice, but we would much prefer to learn a model that is as small as possible for very simple data, while growing large for more complex data.  A natural class of models to explore is probabalistic deterministic finite automata (PDFA), which contains n-grams as a special case, as well as simpler models.  We define a prior over PDFAs with a finite number of states and describe a Metropolis-Hastings algorithm to generate samples from the posterior.  We then generalize to the case of PDFAs with a (potentially) infinite number of states and show that the generative model is a type of Hierarchical Dirichlet Process (HDP).  We then (I hope!) describe a state splitting/merging algorithm for posterior inference that mixes more efficiently than the original Metropolis-Hastings algorithm, and show that it defines a natural hierarchy of states for smoothing (fingers crossed...)  The set of strings produced by a PDFA constitutes a probabilistic regular language, thus our inference procedure can also be viewed as a non-greedy algorithm for regular grammar induction.

Finally, 