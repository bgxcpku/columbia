% !TEX root = main.tex
\section{Probabilistic Deterministic Finite Automata}
\label{sec:PDFA}

A PDFA is formally defined as a 5-tuple $M = (Q,\Sigma,\delta,\pi,\state_0)$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet of observable symbols, $\delta\,:\,Q\times\Sigma\rightarrow Q$ is the transition function from a state/symbol pair to the next state, $\pi\,:\,Q\times\Sigma\rightarrow[0,1]$ is the probability of the next symbol given a state and $\q_0$ is the initial state.\footnote{In general $\state_0$ may be replaced by a distribution $q_0$ over initial states.  \comment{We are interested in inference when the path is known exactly, and so restrict ourselves to the case of a single initial state.}}  Throughout this paper we will use $i$ to index over elements of $Q$, $j$ to index over elements of $\Sigma$ and $t$ to index elements of an observed string.  For example, $\delta_{ij}$ is shorthand for $\delta(\state_i,\symb_j)$, where $\state_i \in Q$ and $\symb_j \in \Sigma$.

Given a state $\state_i$, the probability that the next symbol takes the value $\symb_j$ is given by $\pi(\state_i,\symb_j)$.  We use the shorthand $\pi_i$ for the state-specific discrete distribution over symbols for state $\state_i$.  We could equivalently write $\sigma|\state_i \sim \pi_i$ where $\sigma$ is a random variable that takes values in $\Sigma$.  Given a state $\state_i$ and a symbol $\symb_j$, however, the next state $\state_{i'}$ is {\it deterministic} $\state_{i'} = \delta(\state_i,\symb_j).$   The process for generating from a PDFA involves repeatedly generating a symbol stochastically given which state the process is in, then given that state and the emitted symbol transitioning deterministically to the next state.  This is the reason for the confusing ``probabilistic deterministic'' name for these models.  Turning this around, given data, as as long as the initial state is known this means that there is no uncertainty about the path through the states during inference and prediction. 

PDFAs are more general than $n$th-order Markov models (i.e. $m$-gram models, $m=n+1$), but less expressive than hidden Markov models (HMMs).  For the case of $n$th-order Markov models, if we consider a state to be the suffix $x_1 x_2 \ldots x_n$, then given a state and a symbol $x_{n+1}$, the unique next state is $x_2 \ldots x_{n+1}$.  Thus $n$th-order Markov models are a subclass of PDFAs with $\mathcal{O}(|\Sigma|^n)$ states.  For an HMM, given an initial distribution over states $p(q_0)$ and data $x_{0:T}$, it is possible to recursively calculate the distribution over states at time $t$, $p(q_t|x_t,q_{t-1})$.  PDFAs are those HMMs which, given $P(\q_0 = q_0) = 1$, all $p(\q_t|x_t,\q_{t-1})$ are zero except at a single state $\delta(\q_{t-1},x_t) \in Q$.  The expressive power of our model class can be expanded if we consider {\em mixtures} of PDFAs rather than a single PDFA.  This becomes significant during posterior inference over the class of PDFAs, and we discuss it in more detail in section \ref{theory}.