\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{pslatex}
\usepackage{fullpage}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Bayesian Infinite Automata}
\author{David Pfau, Nicholas Bartlett, Frank Wood}
\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}
For predicting sequences of text or other finite-alphabet data, $n^{th}$-order Markov models are commonly employed.  However, as the context length increases, the number of possible contexts grows exponentially, leading to cumbersome models requiring ever greater amounts of training data.  We are interested in learning models with fewer states that still predict well, and do this by expanding the class of models considered.  We develop a novel Bayesian framework for learning with probabilistic deterministic finite automata (PDFA) \cite{Rabin1963}.  A PDFA is a generative model for sequential data similar to a hidden Markov model (HMM) \cite{Rabiner1989} in that it consists of a set of states, each of which when visited emits a symbol according to an emission probability distribution.  It differs from an HMM in how state-to-state transitions occur; transitions are deterministic in a PDFA and probabilistic in an HMM. We focus specifically on PDFA because they contain $n^{th}$-order Markov models as a subclass, and like $n^{th}$-order Markov models (but unlike HMMs) the path through the state space is deterministic given the data.
In our framework for learning with PDFAs we specify a prior over the parameters of a single large PDFA that encourages state reuse.  The inductive bias introduced by the PDFA prior provides a soft constraint on the number of states used to generate the data.  As the number of states becomes infinite the model has a well-defined limit which we call the probabilistic deterministic infinite automata (PDIA).  
Formally, a PDIA is a quintuple $M = (Q,\Sigma,\delta,\{\vec{\pi}_i\},q_0)$ where $Q$ is a countable set of states, $\Sigma$ is a finite set of symbols, $\delta: Q \times \Sigma \rightarrow Q$ is a deterministic transition function from state/symbol pairs to another state, $\{\vec{\pi}_i : i \in Q\}$ is a set of probability vectors over symbols, one for each state, and $q_0$ is the initial state.  Starting in $q_0$, a sequence is generated by sampling a symbol from the appropriate $\vec{\pi}_i$ and then deterministically transitioning to the next state according to $\delta$.  To generate a PDIA deterministically we need to define a distribution over $\delta$ and $\vec{\pi}$.  Each $\vec{\pi}_i$ is sampled independently from a Dirichlet distribution.  To generate $\delta$, first we sample $\mathcal{G}_0$ from a Pitman-Yor process \cite{Pitman1997} with some base distribution over the natural numbers (geometric in our case).  Then, for each symbol $\sigma \in \Sigma$ we sample another Pitman-Yor process with base distribution $\mathcal{G}_0$.  This construction is known as the Hierarchical Pitman-Yor Process (HPYP).  Finally each $\delta(q_i, \sigma)$ is sampled i.i.d. from the appropriate $\mathcal{G}_{\sigma}$.  This construction leads to certain states appearing more frequently in the transition function, which keeps learned models from becoming too complex.  Formally:

\begin{equation}
\mathcal{G}_0 \sim \mathrm{PY}(\gamma,d_0,H) \qquad
\mathcal{G}_\sigma \sim \mathrm{PY}(\alpha, d, \mathcal{G}_0) \qquad
\delta(q_i,\sigma) \sim \mathcal{G}_\sigma \qquad
\vec{\pi}_i \sim \mathrm{Dir}(\beta/|\Sigma|,\ldots,\beta/|\Sigma|)
\end{equation}
Posterior samples are drawn via a version of Gibbs sampling where the conditional probability of each element of the transiton function is approximated by Metropolis-Hastings.  This avoids certain complications due to the way we represent $\delta$.  MCMC samples form a finite sample approximation to this infinite mixture.  Using such a mixture we can average over our uncertainty about the model parameters (including state cardinality) in a Bayesian way during prediction and other inference tasks.  We find that averaging over a finite number of PDFAs trained on naturalistic data leads to better predictive performance than using a single “best” PDFA.  Because PDFAs are intermediate in expressive power between HMMs and finite-order Markov models they strike a good balance between generalization performance and computational efficiency.  

We tested PDIA inference on a sample of text from Alice's Adventures in Wonderland (AIW) and a section of DNA from mouse chromosome 2.  We found that predictive performance was better than the best cross-validated EM-trained HMMs and comparable to a smoothed 3$^{rd}$-order Markov model, but not as good as an incrementally-trained sequence memoizer (SSM) \cite{Wood2009}.  These results are shown in Figure 1, where the top row is the perplexity of held out data, and the second row is the number of states of each trained model.  The posterior mean number of states for the PDIA was lower than that of the Markov model.  While not exceeding the performance of the best arbitrary-order Markov models, the performance is strong given the small number of states and the lack of smoothing compared to the Markov models.

\begin{table}[t]
    \begin{center}
    \setlength{\tabcolsep}{1.3mm}
    \begin{small}
\begin{tabular}{r|cccccccccc}
\hline
& PDIA  & PDIA-MAP &  HMM-EM & 3-MM & SSM \\
\hline
AIW & 5.13 & 5.46 &  7.89 & 5.13 & 4.78 \\
  & 365.6 & 379 & 52 & 2,023 & 19,358 \\
\hline
\hline
%AIWL & 4.08 & 4.13 &  7.89 & 9.45 & 5.72 & 4.05 & 3.51 & 3.32 & 3.24 \\
 %AIWL & 1,231.6 & 1,247 &  52 & 28 & 444 & 3,249 & 12,324 & 31,990 & 177,232 \\
%\hline
%\hline
DNA & 3.72 & 3.72 &  3.76 & 3.74 & 3.56 \\
 & 64.7 & 54 & 19 &  85 & 314,166 \\
\hline
\end{tabular}
\end{small}
\caption{\small Evaluation on Alice in Wonderland (AIW) and mouse DNA.  Top row is perplexity of test set, bottom is number of states in the model (average for PDIA).  PDIA: average over multiple posterior samples.  PDIA-MAP: best posterior sample.  HMM-EM: Hidden Markov Model.  3-MM: 3$^{rd}$ order Markov Model.  SSM: sequentially trained sequence memoizer.}
\end{center}
\vspace{-8mm}
\end{table}
We show that a finite mixture of PDFAs has greater expressivity than that of a single PDFA but is not as expressive as a probabilistic nondeterministic finite automata (PNFA), which are equivalent to HMMs as long as they have no final state \cite{Dupont2005}.  A PDIA is clearly highly expressive; an infinite mixture over the same is even more so.  Even though ours is a Bayesian approach to PDIA learning, the fact that we will only ever observe a finite amount of data and draw a finite number of posterior samples stipulates that we will only ever compute with finite mixtures of finite PDFAs and thus limit our discussion about expressivity to such models.   
While model expressivity is a concern, computational considerations often dominate model choice.  We show that prediction in a trained mixture of PDFAs can have lower asymptotic cost than forward prediction in the PNFA/HMM class of models.  We also present evidence that averaging over PDFAs gives predictive performance superior to HMMs trained with standard methods on naturalistic data.  We find that PDIA predictive performance is competitive with that of fixed-order, smoothed Markov models with the same number of states.  While sequence learning approaches such as the HMM and smoothed Markov models are well known and now highly optimized, our PDIA approach to learning is novel and as such is amenable to future improvement.  

\begin{small}\bibliographystyle{plain}
\bibliography{../../uber} 
\end{small}

\end{document}  