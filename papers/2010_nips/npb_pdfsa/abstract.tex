% !TEX root = main.tex
\begin{abstract}
%A common desire is to learn minimally complex, maximally predictive models for data.  
We propose a novel Bayesian solution to the problem of probabilistic deterministic finite automata (PDFA) learning.  We  consider the problem of learning PDFAs with infinite numbers of states (probabilistic deterministic infinite automata (PDIA)) and extend our Bayesian inference methodology thereto.  Posterior inference using the resulting model can be interpreted as averaging over PDFA of differing complexity and structure, where each is both constrained to ensure that it could have generated the training data and biased to have few states that are visited frequently.  We demonstrate how averaging over automata on natural language and DNA data prediction tasks achieves performance comparable to with more well-established smoothing methods applied to the same.  %We introduce a nonparametric prior distribution over a PDIA and a Markov chain Monte Carlo sampling procedure for posterior estimation.  We show that the posterior distribution over the PDIA consists of an infinite mixture of PDFA and as a result the model has the same expressive power as a \fix{hidden Markov model, while retaining properties of a variable or fixed order Markov model that are attractive for predicting sequences}.  We demonstrate the performance of our approach on natural language and DNA, achieving results compatible with state of the art models.  \fix{We argue that this approach opens the door to new approaches to new ways of computing.}{Say something about new way of computing}
\end{abstract}