% !TEX root = main.tex
\begin{abstract}
%A common desire is to learn minimally complex, maximally predictive models for data.  
We revisit probabilistic deterministic finite automata (PDFA) and develop a novel Bayesian approach to PDFA structure induction wherein we learn about which components of a PDFA with an infinite number of states (probabilistic deterministic infinite automata, PDIA) are responsible for having generated the observed data under an inductive bias that prefers simple PDFA.  Inference in this model can be interpreted as averaging over PDFA of differing complexity and structure, where the posterior distribution is constrained to ensure that it only assigns mass to PDFA that could have generated the training data.  We introduce a nonparametric prior distribution over a PDIA and a Markov chain Monte Carlo sampling procedure for posterior estimation.  We show that the posterior distribution over the PDIA consists of an infinite mixture of PDFA and as a result the model has the same expressive power as a \fix{hidden Markov model, while retaining properties of a variable or fixed order Markov model that are attractive for predicting sequences}.  We demonstrate the performance of our approach on natural language and DNA, achieving results compatible with state of the art models.  \fix{We argue that this approach opens the door to new approaches to new ways of computing.}{Say something about new way of computing}
\end{abstract}