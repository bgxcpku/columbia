% !TEX root = main.tex
\begin{abstract}
%A common desire is to learn minimally complex, maximally predictive models for data.  
We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic finite automata (PDFA).  We  define a PDFA with an infinite number of states (probabilistic deterministic infinite automata (PDIA)) and show how to do Bayesian inference over its connectivity structure and state specific emission distributions.  Posterior inference in the PDIA can be interpreted as averaging over PDFAs of differing complexity and structure, where each PDFA could have generated the training data but is biased to have few states that are visited frequently.  We demonstrate surprising empirical results that suggest that this Bayesian PDIA approach to learning produces excellent results when trained on both natural language and DNA data.  Specifically the performance of our approach exceeds that of models such as HMMs and smoothed Markov models but at reduced inference-time computational cost.  %We introduce a nonparametric prior distribution over a PDIA and a Markov chain Monte Carlo sampling procedure for posterior estimation.  We show that the posterior distribution over the PDIA consists of an infinite mixture of PDFA and as a result the model has the same expressive power as a \fix{hidden Markov model, while retaining properties of a variable or fixed order Markov model that are attractive for predicting sequences}.  We demonstrate the performance of our approach on natural language and DNA, achieving results compatible with state of the art models.  \fix{We argue that this approach opens the door to new approaches to new ways of computing.}{Say something about new way of computing}
\end{abstract}