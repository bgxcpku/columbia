\begin{abstract}
We wish to learn minimally complex, maximally predictive models for finite alphabet sequence data.  To do this, we describe a nonparametric prior distribution over probabilistic deterministic finite automata and an MCMC algorithm for posterior inference.  We take the Bayesian perspective to avoid overfitting and escape local minima in the space of models.  In the limit of many samples, the posterior estimate has the same expressive power as a hidden Markov model, while retaining properties of a variable or fixed order Markov model that are attractive for predicting sequences.  We demonstrate the performance on natural language and DNA, achieving superior prediction rates to much larger models.
\end{abstract}