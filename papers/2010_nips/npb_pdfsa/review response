Reviewer 1 notes that n-gram models outperform the PDIA for large enough n.  This improved performance comes at the cost of greater model complexity.  Given that emission distributions are not smoothed in the PDIA (only transition distributions are) we were frankly pleasantly surprised to find that the PDIA is competitive with n-gram models while using so few states.  

With regards to the interpretability of states, a state in a PDFA is not latent as it is in an HMM.  Instead, it is the PDFA itself which is latent.  We will look at an example best PDFA (as defined in the paper) and summarize the set of prefixes that lead to a particular set of states, which may provide some interpretability. 

When we refer to improved computational efficiency for prediction, we mean storage complexity lower (number of states) relative to Markov models, and lower computational complexity for prediction relative to HMMs.  We will clarify this point.

Reviewer 1 expressed concern about how we initialize the HMM, while Reviewer 2 was curious about error bars.  Initial parameters were sampled once for each HMM with up to 200 hidden states.  In continuing experiments 80 random HMM training restarts on AIW with 52 hidden states were run, the minimum resulting perplexity was 7.66, the mean was 8.16 and the standard deviation was 0.22.  The log loss estimates from the n-gram models used typically vary on the order of 0.01.  The paper will be changed to include this information.

One sampler pass means a cycle through all entries of delta.

It is possible to integrate out mu in the finite case, however, related citations in the literature do not [7,15].

Reviewer 1 had many useful suggestions on the organization of 3.2 which we will incorporate.  The terms marginalization, ignoring, and just-in-time sampling were seen as confusing.  Marginalization and ignoring are the same thing.  Sampling one transition has a closed form, but because changing one transition may lead to ignored transitions being visited, Gibbs sampling requires summing over all possible combinations of ignored transitions, which is intractable.  Just-in-time is necessary for MH sampling when ignoring: instead of summing over all possible ignored transitions, we sample transitions when necessary.

With regards to sampling the posterior: while we wish to generate samples from (7)*(3), we propose from (7) and accept according to (8), which gives an MH sampler for the posterior.

Gamma(1,1) priors are used for all hyperparameters.  We will state this in the paper.  The choice of hyperparameter had negligible effect on performance.

To bias inference towards larger or smaller models, we adjust \gamma, which sets the probability of choosing a new state.

Regarding smoothing over emission distributions: currently, emission distributions are unsmoothed estimates from the counts that occur in each state.  If statistical strength could be shared between states in a principled way it should improve prediction.  N-gram models with backoff do this using a natural hierarchy.  Finding such a hierarchy for backing off the emission distributions in a PDFA is a direction for future research.

Regarding concerns about efficiency and empirical benefits voiced by Reviewers 2 and 3, prediction is more efficient in time with a PDIA than with an HMM.  For n-gram models, as the second row of Table 1 shows, models with equal predictive power had far more states than the PDIA.

Reviewer 3 was uncertain what kind of smoothing was used in our experiments.  This information is on line 305-306.

Figure 3 was provided for the PDFA learning community to suggest it may have some application to their domain as well.  Previous approaches [1,13] have similar accuracy to that shown in figure 3 on the same grammars with the same amount of data.  One direction for future work is comparing the efficiency of PDIA inference versus standard PDFA techniques.