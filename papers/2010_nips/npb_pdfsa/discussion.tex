\section{Discussion}

We believe the most promising approach to improving generalization beyond the current results is to construct a smoothing hierarchy over the states, as in $n$-gram modeling.  For an $n$-gram, where every state corresponds to a suffix of the sequence, the predictive distributions given suffixes with very few observations are smoothed by the predictive distributions for shorter suffixes, for which there are presumably more observations.  This makes it possible to increase the size of the model indefinitely without generalization performance suffering.  In the PDIA, by contrast, the predictive probabilities for states are not tied together.  Since states of the PDIA are not uniquely identified by suffixes, it is no longer clear what the natural smoothing hierarchy is.  It is impressive that PDIA learning works nearly as well as $n$-gram modeling even without a smoothing hierarchy.  However, if such a hierarchy does exist, using it as the generative model for the emission distributions $\boldsymbol\pi$ should lead to superior performance, even surpassing $n$-gram models.

Another path for further research is to improve MCMC mixing.  Two PDFAs that differ by only a single transition $\delta_{ij}$ may generate very different probabilistic languages.  A more natural measure of the distance between PDFAs is given by the KL divergence between the languages they generate \cite{Carrasco1994}.  An MCMC sampler that proposes PDFAs that are similar in the KL divergence sense may mix more efficiently than one that randomly reassigns $\delta_{ij}$.  Similar to existing PDFA induction algorithms \cite{Shalizi2004,Thollard2000}, merging and splitting states may be an effective way to achieve this.

Much of the original work on finite automata was motivated by early studies of neural nets\cite{Hopcroft1979}.  It was shown that the very earliest neural nets, with binary output, fixed thresholds, fixed topology and deterministic evolution, were equivalent to finite automata\cite{Kleene1956}, with the result that much research in neural networks shifted to automata theory.  Now there is renewed interest in stochastic neural networks, for machine learning \cite{Hinton2006}, data analysis of biological neurons \cite{Paninski} and models of biological computation \cite{Pouget? Lengyel? Fiser?}.  The expressive power of these models is not well characterized.  The theory of probabilistic automata may provide a framework for analyzing these models and perhaps even a route to generalization.  We hope that our results demonstrate that finding novel approaches to learning simple models can be as fruitful as tackling more complex models, and may even open a route to new models of computation.