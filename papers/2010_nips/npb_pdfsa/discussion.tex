% !TEX root = main.tex
\section{Discussion}
\label{sec:discussion}

Our Bayesian approach to PDIA inference can be interpreted as a stochastic search procedure for PDFA structure learning where the number of states is unknown.  In Section~\ref{theory} we presented evidence that PDFA samples from our PDIA inference algorithm have the same characteristics as the true generative process and suggest that the MAP sample is often the exact, or equivalent after transformation, generative PDFA.  This in and of itself may be of interest to the PDFA induction community.

We ourselves are more interested in the trade-off between model simplicity in terms of number of states, computational complexity in terms of asymptotic cost of prediction, and predictive perplexity.  While our PDIA approach may not yet outperform the best smoothing Markov model approaches in terms of predictive perplexity alone, it does outperform them in terms of number of states and computational complexity (for the same predictive perplexity).  This gives researchers another tool to choose from when building models.  If very fast prediction is desirable and the predictive perplexity difference between the PDIA and, for instance, the most competitive $n$-gram is insignificant from an application perspective, then doing finite sample inference in the PDIA offers a significant computational advantages in terms of cost of prediction.

There are obvious shortcomings to the PDIA approach proposed herein that we intend to rectify in future research.  We believe the most promising approach to predictive performance beyond the is to construct a smoothing hierarchy over the state specific emission distributions, as is done in the smoothing $n$-gram models.  For an $n$-gram, where every state corresponds to a suffix of the sequence, the predictive distributions given suffixes with very few observations are smoothed by the predictive distributions for shorter suffixes, for which there are presumably more observations.  This makes it possible to increase the size of the model indefinitely without generalization performance suffering.  In the PDIA, by contrast, the predictive probabilities for states are not tied together.  Since states of the PDIA are not uniquely identified by suffixes, it is no longer clear what the natural smoothing hierarchy is.  It is somewhat surprising that PDIA learning works nearly as well as $n$-gram modeling even without a smoothing hierarchy for its emission distributions.  However, if such a hierarchy does exist, using it as the generative model for the emission distributions $\boldsymbol\pi$ should lead to superior performance, even surpassing $n$-gram models.

Another path for further research is to improve MCMC mixing.  Two PDFAs that differ by only a single transition $\delta_{ij}$ may generate very different probabilistic languages.  A more natural measure of the distance between PDFAs is given by the KL divergence between the languages they generate \cite{Carrasco1994}.  An MCMC sampler that proposes PDFAs that are similar in the KL divergence sense may mix more efficiently than one that randomly reassigns $\delta_{ij}$.  Similar to existing PDFA induction algorithms \cite{Shalizi2004,Thollard2000}, merging and splitting states may be an effective way to achieve this.

Much of the original work on finite automata was motivated by early studies of neural nets \cite{Hopcroft1979}.  It was shown that the very earliest neural nets, with binary output, fixed thresholds, fixed topology and deterministic evolution, were equivalent to finite automata \cite{Kleene1956}, with the result that much research in neural networks shifted to automata theory.  Now there is renewed interest in stochastic neural networks, for machine learning \cite{Hinton2006}, data analysis of biological neurons \cite{Pillow2008} and models of biological computation \cite{Pouget? Lengyel? Fiser?}.  The expressive power of these models is not well characterized.  The theory of probabilistic automata may provide a framework for analyzing these models and perhaps even a route to generalization.  We hope that our results demonstrate that finding novel approaches to learning simple models can be as fruitful as tackling more complex models, and may even open a route to new models of computation.