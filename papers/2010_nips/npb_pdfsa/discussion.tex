\section{Discussion}

Much of the original work on finite automata was motivated by early studies of neural nets.  It was shown that the very earliest neural nets, with binary output, fixed thresholds, fixed topology and deterministic evolution, were equivalent to finite automata \cite{Hopcroft}, with the result that many neural network researchers moved to automata theory \cite{?}.  Now there is renewed interest in stochastic neural networks, for machine learning \cite{Hinton}, data analysis of biological neurons \cite{Paninski} and models of biological computation \cite{Pouget? Lengyel? Fiser?}.  The expressive power of these models is not well characterized.  The theory of probabilistic automata may provide a framework for analyzing these models and perhaps even a route to generalization.  We hope that our results demonstrate that finding novel approaches to learning simple models can be as fruitful as tackling more complex models, and may even open a route to new models of computation.