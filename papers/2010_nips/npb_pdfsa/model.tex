% !TEX root = main.tex
\section{Bayesian PDFA Inference}

To do Bayesian PDFA inference we start by defining a prior distribution over finite PDFAs and show how to marginalize out many nuisance parameters.  Using this collapsed representation we derive a Metropolis-Hastings sampler for posterior inference.  We then discuss the infinite states limit and define the probabilistic deterministic infinite automaton (PDIA).  We develop a PDIA sampler that carries over from the finite case in a natural way.

%, and show that many of the elements of the transition matrix can be ignored without affecting the correctness of sampling

\subsection{A Prior over PDFAs}

We assume that the set of states $Q$, set of symbols $\Sigma$, and initial state $q_0$ of a PDFA are known.  The PDFA prior then consists of a prior over both the transition function $\delta$ and the emission probability function $\pi$.  In the finite case $\delta$ and $\pi$ are representable as finite matrices, with one column per element of $\Sigma$ and one row per element of $Q$.  For each column $j$ ($j$ co-indexes columns and set elements) of the transition matrix $\delta$, our prior stipulates that the row elements of that column are  i.i.d. draws from a discrete distribution $\bphi_j$ over $Q$, that is, $\delta_{ij} \sim [\bphi_1,\ldots,\bphi_{|\Sigma|}], \; 0 \leq i\leq|Q|-1$.  The $\bphi_j$ represent state specific transition tendencies, if the $i$th element of $\phi_{j}$ is large then state $q_i$ is likely to be transitioned to anytime the last symbol was $j.$   The $\phi_{j}$'s are   themselves given a shared Dirichlet prior with parameters $\alpha\bmu$, where $\alpha$ is a concentration and $\bmu$ is a template transition probability vector.   If the $i$th element of $\bmu$ is large then the $i$th state is likely to be transitioned to regardless of the emitted symbol.  We place a uniform Dirichlet prior on $\bmu$ itself, with $\gamma$ total mass and average over $\bmu$ during inference.  This hierarchical Dirichlet construction encourages both general and context specific state reuse.
 We also place a uniform Dirichlet prior over the per-state emission probabilities $\bpi_i$ with $\beta$ total mass which smooths emission distribution estimates.  Formally:

\begin{eqnarray}
\bmu|\gamma,|Q| & \sim & \mathrm{Dir}\left(\gamma/|Q|,\ldots,\gamma/|Q|\right) \label{gen:mu} \\
\bphi_{j}|\alpha,\bmu  & \sim & \mathrm{Dir}(\alpha\bmu) \label{gen:phi} \\
\bpi_{i}|\beta,|\Sigma| & \sim & \mathrm{Dir}(\beta/|\Sigma|,\ldots,\beta/|\Sigma|) \label{gen:pi}\\
\delta_{ij} & \sim & \bphi_{j} \label{gen:delta}
\end{eqnarray}

where $0 \leq i \leq |Q|-1$ and $1 \leq j \leq |\Sigma|$.  Given a sample from this model we can run the PDFA to generate a sequence of $T$ symbols:

 \fix{font issues}
\begin{eqnarray}
\q_0 & = & q_0 \label{gen:q0} \\
x_0 & \sim & \bpi_0 \label{gen:x0} \\
\q_t & = & \delta(\q_{t-1},x_{t-1}) \label{gen:q} \\
x_t & \sim & \bpi_t \label{gen:x}
\end{eqnarray}

We choose this particular inductive bias, with transitions tied together within a column of $\delta$, because we wanted the most recent symbol to be informative about what the next state is.  If we instead had a single Dirichlet prior over all elements of $\delta$, transitions to a few states would be highly likely no matter the context and those states would dominate the behavior of the automata.  If we tied together rows of $\delta$ instead of columns, being in a particular state would tell us more about the sequence of states we came from than the symbols that got us there.  
%As we would like states to be good statistics of the past for predicting the future, a bias that depends on the most recent symbol is preferable.

\subsection{PDFA likelihood}

Given observational data, we are interested in learning a posterior distribution over PDFAs.  To start inference we need the likelihood function for a fixed PDFA. It is given by 

\[ p(x_{0:T}|\delta,\pi) = \pi(\q_0,x_0)\prod_{t=1}^T \pi(\q_t,x_t) \label{x:def} \].

Remember that $\q_t | \q_{t-1}, x_{t-1}$ is deterministic and given by $\delta$.  Note that our prior is a fully connected PDFA in which all states may transition to all others and all symbols may be emitted from each state.  This is slightly different that the cannonical finite state machine literature where sparse connectivity is usually the norm.

We can marginalize out $\pi$ and express the likelihood in a form that depends only on the counts of symbols emitted from each state.  Define the count matrix $c$ for the sequence $x_{0:T}$ and transition matrix $\delta$ as $c_{ij} = \displaystyle\sum_{t=0}^T I_{ij}(\q_t,x_t)$, where $I_{ij}(\q_t,x_t)$ is an indicator function that returns 1 if the automaton was in state $q_i$ when it generated $x_t$ at step $t$ in the sequence, i.e. $\q_t = q_i$ and $x_t = \sigma_j$, and 0 otherwise. This matrix $c = [c_{ij}]$ gives the number of times each symbol is emitted from each state.  Due to multinomial-Dirichlet conjugacy we can analytically marginalize out $\pi$ and express the probability of a sequence given the transition function $\delta$ solely in terms of $c$ and $\beta$:

\begin{eqnarray}
 p(x_{0:T}|\delta,c,\beta) & = & \int p(x_{0:T}|\pi,\delta) p(\pi|\beta) d\pi \label{x:factor} \\
 %& = &  \prod_{i=0}^{|Q|-1} \frac{\Gamma(\beta)}{\Gamma(\frac{\beta}{|\Sigma|})^{|\Sigma|}} \int\pi_{i1}^{\frac{\beta}{|\Sigma|}+c_{i1}-1} \pi_{i2}^{\frac{\beta}{|\Sigma|}+c_{i2}-1} \ldots \pi_{i|\Sigma|}^{\frac{\beta}{|\Sigma|}+c_{i|\Sigma|}-1} d\bpi_i \label{x:int}\\
 & = & \prod_{i=0}^{|Q|-1} \frac{\Gamma(\beta)}{\Gamma(\frac{\beta}{|\Sigma|})^{|\Sigma|}} \frac{\prod_{j=1}^{|\Sigma|}\Gamma(\frac{\beta}{|\Sigma|} + c_{ij})}{\Gamma(\beta + \sum_{j=1}^{|\Sigma|} c_{ij})} \label{x:end}
 \end{eqnarray}
 
 
 
 Given that the state to state transitions are deterministic, we can treat them like observations and express the likelihood of a particular transition matrix $\delta$ given $\bmu$.  Let $v_{ij}$ be the number of times state $q_i$ is transitioned to given that $\sigma_j$ was the last emission, i.e. $\delta_{i'j} = q_i$ for all $i'$ in the column $j$, that is, $v_{ij} = \displaystyle\sum_{i' = 0}^{|Q|-1} I_{i}(\delta_{i'j})$, $I_i(q_{i'})$ being the indicator function that is only 1 when $q_i' = q_i$.  Given $\bmu$, we can integrate out $\phi$ and express the likelihood of $\delta$ in terms of $\bmu$:
 
 \begin{eqnarray}
 p(\delta|\bmu,\alpha) & = & \int p(\delta|\phi)p(\phi|\bmu,\alpha) d\phi \label{delta:factor}\\
  & = & \prod_{j=1}^{|\Sigma|} \frac{\Gamma(\alpha)}{\prod_{i=0}^{|Q|-1}\Gamma(\alpha\mu_i)} \int \phi_{0j}^{\alpha\mu_0+v_{0j}-1} \phi_{1j}^{\alpha\mu_1+v_{1j}-1} \ldots \phi_{|Q|-1,j}^{\alpha\mu_{|Q|-1}+v_{|Q|-1,j}-1} d\bphi_j \label{delta:int}\\
  & = &  \prod_{j=1}^{|\Sigma|} \frac{\Gamma(\alpha)}{\prod_{i=0}^{|Q|-1}\Gamma(\alpha\mu_i)} \frac{\prod_{i=0}^{|Q|-1} \Gamma(\alpha\mu_i + v_{ij})}{\Gamma(\alpha + |Q|)} \label{delta:end}
  \end{eqnarray}

%Finally, the posterior probability of $\bmu$ given $\delta$ is

%\begin{eqnarray}
%p(\bmu|\delta,\alpha, \gamma) & = & \frac{p(\delta|\bmu,\alpha)p(\bmu|\gamma)}{\int p(\delta|\bmu,\alpha)p(\bmu|\gamma) d\bmu}
%\end{eqnarray}

These are the ingredients we need for posterior inference.
 
 \subsection{Posterior Inference in the Finite Model}
 
 The dependence of $c$ on $\delta$ is only through $\q_{0:T}$, and changing a single element of $\delta$ can have a complex effect on $c$.  By changing a single transition $\delta_{ij}$, all states downstream of the first time $q_i$ emits $\sigma_j$ are affected, and some elements of $c$ that were zero may become nonzero, and vice versa.
 
We perform posterior inference in the finite model by Gibbs sampling elements of $\delta$ and the vector $\bmu$.  From the formulas above, it is straightforward to write down the conditional probability for one element of the transition matrix $\delta$.  If $\delta_{ij}$ is the element we are sampling, and $\delta_{-ij}$ is the rest of the matrix, with $\delta_{ij}$ removed, then

\begin{equation}
p(\delta_{ij}|\delta_{-ij},x_{0:T},\bmu,\alpha) \propto p(x_{0:T}|\delta_{ij},\delta_{-ij})p(\delta_{ij}|\delta_{-ij},\bmu,\alpha) \label{delta:cond}
\end{equation}

Both terms on the right hand side of this equation have closed-form expressions, the first given in \eqref{x:end}.  The second can be found from \eqref{delta:end} to be

\begin{equation}
P(\delta_{ij} = q_{i'}|\delta_{-ij},\alpha,\bmu) = \frac{\alpha\mu_{i'} + v_{i'j}}{\alpha + |Q| - 1} \label{delta:pred}
\end{equation}

Where $v_{i'j}$ is the number of elements in column $j$ equal to $q_{i'}$ {\em excluding} $\delta_{ij}$.  As $|Q|$ is finite, we can construct and normalize the conditional probability vector for $\delta_{ij}$ and sample.

We can simplify inference by ignoring transitions $\delta_{ij}$ for which the corresponding count $c_{ij}$ are 0.  Note that the likelihood of the data on the right hand side of \eqref{delta:cond} does not depend on $\delta_{ij}$ if $c_{ij} = 0$, so sampling conditioned on the data is the same as sampling without conditioning on the data.  Thus, if changing some other transition means $c_ij$ becomes 0, we can remove $\delta_{ij}$ until another transition is changed so the count again is nonzero, and we sample a new value for $\delta_{ij}$ from \eqref{delta:pred}, just as we would have during Gibbs sampling had we not removed it.  Note that the joint distribution over a column of $\delta$ is exchangeable, and so removing an observation is the same as marginalizing it out, meaning that sampling the other elements of $\delta$ is still correct, but now conditioned on the model where superfluous transitions are marginalized out.  When we remove multiple elements from a column of $\delta$, we have to replace the $|Q| - 1$ in the denominator of \eqref{delta:pred} with $D^+_j = \sum_{i=0}^{|Q|-1}v_{ij} \leq |Q|$, the number of entries in the $j$th column of $\delta$ that are {\em not} marginalized out.

The posterior for $\bmu$ up to a normalization constant is

\begin{eqnarray}
p(\bmu|\delta,\alpha,\gamma) & \propto & p(\bmu|\gamma) p(\delta|\bmu,\alpha)\nonumber \\
& = & \frac{\Gamma(\gamma)}{\Gamma(\frac{\gamma}{|Q|})^{|Q|}}(\mu_1\ldots\mu_{|Q|})^{\frac{\gamma}{|Q|}-1}\prod_{j=1}^{|\Sigma|} \frac{\Gamma(\alpha)}{\prod_{i=0}^{|Q|-1}\Gamma(\alpha\mu_i)} \frac{\prod_{i=0}^{|Q|-1} \Gamma(\alpha\mu_i + v_{ij})}{\Gamma(\alpha + D^+_j)}
\end{eqnarray}

We can sample this by Metropolis-Hastings, for example using Dir($\eta\hat\bmu$) as the proposal, where $\hat\bmu$ is our current estimate and $\eta$ is a large concentration parameter, or take the MAP estimate as in \cite{Mackay1995}.  Fortunately in the infinite limit a more natural way to sample presents itself.
 
 \subsection{The Probabilistic Deterministic Infinite Automaton}
 
We now consider what happens when $|Q|\rightarrow\infty$.  Given a finite amount of data, there can only be nonzero counts for a finite number of state/symbol pairs, so we can marginalize out all but at most $T$ elements of $\delta$.  Denote these active entries by $\delta^T$.  The predictive probability for a new $\delta_{ij} = q_{i'}$ given $\delta^T$ is given by $\frac{\alpha\mu_{i'} + v_{i'j}}{\alpha + D^+_j}$.  Note that this only depends on $|Q|$ through $\mu_{i'}$, which is well behaved as $|Q|$ grows.  In the limiting case, most of the mass of $\bmu$ will concentrate on a handful of elements, and $\bmu$ becomes a draw from a {\em Dirichlet process} (DP), which is commonly used as a prior in Bayesian models with infinite parameters.  The hierarchical Dirichlet construction given in \eqref{gen:mu} and \eqref{gen:phi} becomes a {\em hierarchical Dirichlet process} (HDP), where the $\bphi_j$ are draws from a Dirichlet process whose parameters are given by $\alpha$ and $\bmu$, which is itself a draw from a Dirichlet process.  An attractive property of HDPs is that both the $\bphi_j$ and $\bmu$ can be integrated out, which makes sampling more straightforward than in the finite case.  This  representation of the model, with all draws from a DP integrated out, is known as the {\em Chinese Restaurant Franchise} (CRF).  This curious name needs some explanation, which we happily provide.
 
Consider each column of $\delta$ to be a restaurant with an infinite number of tables.  The individual $\delta_{ij}$ are customers, who upon entering the restaurant (i.e. when being sampled) must choose a table to sit at.  Each table serves one dish, which is the element of $Q$ that $\delta_{ij}$ maps to if it is seated at that table.  Multiple tables can serve the same dish, even in the same restaurant.  Let $w_{ij}$ be the table $\delta_{ij}$ sits at, let $n_{kj}$ be the number of customers sitting at table $k$ in restaurant $j$, and let $K^+_j$ be the total number of tables for which $n_{kj} \ne 0$.  Then a new $\delta_{ij}$ sits at table $k$ with probability proportional to $n_{kj}$ and sits at an unoccupied table $K^+_j + 1$ with probability proportional to $\alpha$:

\begin{equation}
 p(t_{ij} = k | w_{-ij}) = 
 \begin{cases} 
 \frac{n_{kj}}{\alpha + \sum_{k'=1}^{K^+_j}n_{k'j}} & \text{if } k < K^+_j \\
 \frac{\alpha}{\alpha + \sum_{k'=1}^{K^+_j}n_{k'j}} & \text{if } k = K^+_j + 1\label{crf:low}
 \end{cases}
\end{equation}

This sampling procedure is known as the Chinese Restaurant Process (CRP).  There is no notion of "dishes" in the CRP, and it is this extension that turns the CRP into the CRF.

When we create a new table in the CRF, we must also assign a dish to it.  \comment{Consider {\em another} restaurant, where each table corresponds to a unique element of $Q$.  Every table in the regular restaurants is a customer in this high-level restaurant, and the dish served at each low-level table is the element of $Q$ on the high-level table where the low-level table sits.  Note that this means multiple low-level tables can serve the same dish, even within the same restaurant, while there is a {\em unique} high-level table with that dish.}  Let $m_i$ denote the number of tables across all restaurants serving the dish $q_i$, and $Q^+$ denote the number of unique elements of $Q$ for which $m_i \ne 0$.  If we seat a customer at a new table, assign $q_i$ to that table with probability proportional to $m_i$ and assign it a new element of $Q$ with probability proportional to $\gamma$:

\begin{equation}
p(\delta_{ij} = q_{i'} | \delta_{-ij}, w_{-ij}, w_{ij} = K^+_j + 1) = 
\begin{cases}
\frac{m_{i'}}{\gamma + \sum_{i'' = 1}^{Q^+} m_{i''} } & \text{if } q_{i'} < Q^+ \\
\frac{\gamma}{\gamma + \sum_{i'' = 1}^{Q^+} m_{i''} } & \text{if } q_{i'} = Q^+ + 1\label{crf:high}
\end{cases}
\end{equation}

This is {\em another} CRP, which comes as a result of integrating out $\bmu$, while the first CRP came from integrating out the $\bphi_j$.  When the number of observations grows large, the probability of sitting at a new table, and the probability of drawing a new dish given a new table, becomes small.  Thus a new $\delta_{ij}$ is most likely to be assigned to a state already seen in column $j$, then has some probability of being assigned to a state seen in another column but not in $j$, and then with small, but not zero, probability, will be assigned to a new state.  This coupling across multiple levels of generality is one of the attractive features of HDPs for machine learning.

Having defined a prior over a transition matrix with an infinite number of rows, we can now generate a string from the probabilistic deterministic infinite automaton (PDIA).  First, draw $\bpi_0$ from \eqref{gen:pi} as before, and draw $\sigma_j|q_0 \sim \bpi_0$.  Draw $\delta_{0j}$ from the CRF, which for the first observation will always be seated at a new table with a new dish.  From there, if the current state $\q_t = q_i$ is a new one, draw $\bpi_i$, otherwise $\bpi_i$ is already known, and draw $x_t$ from $\bpi_i$.  If $(\q_t,x_t) \in \Delta^t$, the set of state/symbol pairs that have been visited by time $t$, $\q_{t+1} = \delta(\q_t,x_t)$, otherwise draw $\delta(\q_t,x_t)$ from the CRF given $\delta^t = \{\delta_{ij}|(q_i,\sigma_j)\in\Delta^t\}$ and let $\Delta^{t+1} = \Delta^t \cup (\q_t,x_t)$.

Posterior inference in the PDIA is similar to the finite case above, but with an important modification.  Consider the case where the transition we are sampling, $\delta_{ij} = q_{i'}$, is the only one that goes to $q_{i'}$.  Removing this $\delta_{ij}$ from $\delta^T$ means that the conditional probability that $\delta_{ij} = q_{i'}$ given $\delta^T_{-ij}$ is zero: with no elements of $\delta^T_{-ij}$ going to $q_{i'}$, it is just another of the infinite unseen states, all of which are equivalent.  To avoid forgetting a good estimate of $\delta$, we perform Metropolis-Hastings sampling from the conditional rather than sampling directly.  This way there is always some chance of returning to the previous state of the model.  We propose a new value $q_{i*}$ for $\delta_{ij}$ according to $P(\delta_{ij} = q_{i*}|\delta^T_{-ij})$, that is, the CRF.  The posterior probability of this sample is proportional to $p(x_{0:T}|\delta_{ij}=q_{i*},\delta^T_{-ij})P(\delta_{ij} = q_{i*}|\delta^T_{-ij})$, and the second term in this cancels out with the proposal probability in the accept/reject ratio. Thus we accept the new $\delta_{ij}$ with probability

\begin{equation}
 \alpha(\delta_{ij}=q_{i'},\delta_{ij}=q_{i*}) =  \mathrm{min}\left(1,\frac{p(x_{0:T}|\delta_{ij}=q_{i*},\delta_{-ij}^T)}{p(x_{0:T}|\delta_{ij}=q_{i'},\delta_{-ij}^T)}\right) \label{delta:mh}
\end{equation}

The likelihood of a sequence $p(x_{0:T}|\delta^T)$ takes the same form as in the finite case \eqref{x:end}.  However, changing $\delta_{ij}$ may lead to the data visiting state/symbol pairs that are not in $\delta^T$, and we cannot evaluate the numerator of \eqref{delta:mh} exactly.  Instead we approximate it by sampling the missing elements of $\delta$ according to the CRF.  This can be seen as a single-sample Monte Carlo integration.  If we accept the new $\delta_{ij}$ we then delete all the $\delta_{i'j'}$ for which the counts $c_{i'j'} = 0$, while keeping any elements of $\delta$ we had to sample to evaluate the numerator of \eqref{delta:mh}.  In the terminology of the CRF, this reseats a customer at a new table.  We also resample the dish served at each table, otherwise mixing may be prohibitively slow.  We propose a new dish for the table $k$ in restaurant $j$ by sampling \eqref{crf:high} (with that table removed from the counts) and accept with probability \eqref{delta:mh}, with the individual $\delta_{ij}$ replaced by all $\delta_{ij}$ at table $k$, and add or remove elements of $\delta^T$ in the same way as when reseating customers.  For more on posterior sampling in the hierarchical Dirichlet process, we direct the reader to \cite{Teh2006}.
  
%We can sample incrementally from the joint distribution over $x_{0:T}$ and $\delta$ when $\phi_j$, $\mu$ and $\pi_i$ are integrated out in the $|Q|\rightarrow\infty$ limit.  From the start state $q_0$, we sample a symbol $s_{j_0}$ uniformly and assign $\delta_{0j_0}$ to a new state.  If $q^t = q_i$ then $x_t$ has the probability

% \[P(x_t=s_j|q_i,x_{0:t-1},q^{0:t-1}) = \frac{c_{ij}+\frac{\beta}{|\Sigma|}}{c_{i\cdot} + \beta}\]
 
% where $c_{ij}$ is the number of times so far $s_j$ was emitted from $q_i$ and $c_{i\cdot}$ is the total number of times $q_i$ has been visited so far.  
 
%If the state/symbol pair $(q_i,s_j)$ has not been visited before, we have to sample $\delta_{ij}$.  The two-stage generative procedure for elements of $\delta$ means that we have to keep track of counts at two levels.  Each $\delta_{ij}$ belongs to a cluster $v_{kj}$ that contains other $\delta_{i'j}$, while each $v_{kj}$ belongs to a top-level cluster $w_{l}$ that has elements across all $j$.  Each top level cluster has one $q \in Q$ assigned to it, and $\delta_{ij}$ is equal to that $q$ in the top cluster that the cluster with $\delta_{ij}$ belongs to (*might want to make this part clearer...add a figure*).  Let $\delta^t$ denote the elements of $\delta$ that have been visited at time $t$.  as follows:
 
%\[P(\delta_{ij} = k|\delta^t) \propto \begin{cases} & if $k \leq |\delta^t|$ \cr  & if $k > |\delta^t|$ \end{cases}\]
 
% This process for sampling from an HDP when $\mu$ and $\phi_j$ are integrated out is known as the {\em Chinese Restaurant Franchise Process}.