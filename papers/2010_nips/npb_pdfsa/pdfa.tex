\section{Probabilistic Deterministic Finite Automata}
A PDFA is formally defined as a 5-tuple $M = (Q,\Sigma,\delta,\pi,q_0)$\footnote{In general $q_0$ may be replaced by a distribution over initial states.  We are interested in inference when the path is known exactly, and so restrict ourselves to the case of a single initial state.}, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet of observable symbols, $\delta\,:\,Q\times\Sigma\rightarrow Q$ is the transition function from a state/symbol pair to the next state, $\pi\,:\,Q\times\Sigma\rightarrow[0,1]$ is the probability of the next symbol given a state and $q_0$ is the initial state.  We use $i$ to index over elements of $Q$, $j$ to index over elements of $\Sigma$ and $t$ to index elements of an observed string.  For example, $\delta_{ij}$ is shorthand for $\delta(q_i,\sigma_j)$, where $q_i \in Q$ and $\sigma_j \in \Sigma$.

Given a state $q_i$, the probability that the next symbol is $\sigma_j$ is given by $\pi(q_i,\sigma_j)$.  We use the shorthand $\pi_i$ for the discrete distribution over symbols given state $q_i$, so we could equivalently say $\sigma|q_i \sim \pi_i$.  Given a state $q_i$ and a symbol $\sigma_j$, however, there is a {\it deterministic} state $q_{i'} = \delta(q_i,\sigma_j)$ that follows it.  This two-stage process for generating a string, where a symbol is generated stochastically but the next state is generated deterministically, is the motivation behind the odd-sounding ``probabilistic deterministic" in the name for these models.  

In a PDFA there is no uncertainty about the path through the states given the data, as long as the initial state is known.  In terms of expressive power, this means PDFAs are more general than $n$th-order Markov models (i.e. $m$-gram models), but less expressive than hidden Markov models (HMMs).  For the case of $n$th-order Markov models, if we consider a state to be the suffix $x_1 x_2 \ldots x_n$, then given a state and a symbol $x_{n+1}$, the unique next state is $x_2 \ldots x_{n+1}$.  Thus $n$th-order Markov models are a subclass of PDFAs with $\mathcal{O}(|\Sigma|^n)$ states.  For an HMM, given an initial distribution over states $p(\q_0)$ and data $x_{0:T}$, it is possible to recursively calculate the distribution over states at time $t$, $p(\q_t|x_t,\q_{t-1})$.  PDFAs are those HMMs which, given $P(\q_0 = q_0) = 1$, all $p(\q_t|x_t,\q_{t-1})$ are zero except at a single state $\delta(\q_{t-1},x_t) \in Q$.  The expressive power of our model class can be expanded if we consider {\em mixtures} of PDFAs rather than a single PDFA.  This becomes significant during posterior inference over the class of PDFAs, and we discuss it in more detail in section \ref{theory}.