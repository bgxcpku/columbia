\section{Probabilistic Deterministic Finite Automata}
A PDFA is formally defined as a 5-tuple $M = (Q,\Sigma,\delta,\pi,q_0)$.  $Q$ is a finite set of states. $\Sigma$ is a finite alphabet of observable symbols. $\delta\,:\,Q\times\Sigma\rightarrow Q$ is the transition function from a state/symbol pair to the next state.  $\pi\,:\,Q\times\Sigma\rightarrow[0,1]$ is the probability of the next symbol given a state.  $q_0$ is the initial state.  For clarity and minimal clutter, we use $i$ to index over elements of $Q$, $j$ to index over elements of $\Sigma$ and $t$ to index elements of an observed string.  For example, $\delta_{ij}$ is shorthand for $\delta(q_i,s_j)$.

The odd-sounding ``probabilistic deterministic" refers to the fact that a string of symbols is generated by a two-stage process.  Given a state $q_i$, the probability that the next symbol is $s$ is given by $\pi(q_i,s)$, hence ``probabilistic."  We denote the multinomial distribution over symbols given a state $q_i$ as $\pi_i$, so we could equivalently say $s \sim \pi_i$.  Given a state $q_i$ and a symbol $s$, however, there is a {\it unique} state $\delta(q_i,s)$ that follows it, hence ``deterministic."  

We can see that n-grams or nth-order Markov models are a particular class of PDFAs.  If we consider a state to be the prefix $x_1 x_2 \ldots x_{n-1}$, then given a state and a symbol $x_n$, the unique next state is $x_2 \ldots x_n$.  Thus nth-order Markov models are a subclass of PDFAs with $|\Sigma|^n$ states.

PDFAs themselves are a subclass of hidden Markov models (HMMs).  Specifically, they are the class of HMMs for which, given a sequence of observed data, there is only one possible path through the hidden states.  Our approach to inference, taking posterior samples over the space of models, while forcing the path through these models to be deterministic, can be seen as an extreme case of the tradeoff inherent in most model learning.  Normally, one has to estimate both the model and the parameters of the model, but in this case all of the uncertainty is shifted onto the model itself.