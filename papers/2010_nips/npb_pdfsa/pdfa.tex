% !TEX root = main.tex
\section{Probabilistic Deterministic Finite Automata}
\label{sec:PDFA}

A PDFA is formally defined as a 5-tuple $M = (Q,\Sigma,\delta,\pi,\state_0)$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet of observable symbols, $\delta\,:\,Q\times\Sigma\rightarrow Q$ is the transition function from a state/symbol pair to the next state, $\pi\,:\,Q\times\Sigma\rightarrow[0,1]$ is the probability of the next symbol given a state and $\q_0$ is the initial state.\footnote{In general $\state_0$ may be replaced by a distribution $q_0$ over initial states.  \comment{We are interested in inference when the path is known exactly, and so restrict ourselves to the case of a single initial state.}}  Throughout this paper we will use $i$ to index over elements of $Q$, $j$ to index over elements of $\Sigma$ and $t$ to index elements of an observed string.  For example, $\delta_{ij}$ is shorthand for $\delta(\state_i,\symb_j)$, where $\state_i \in Q$ and $\symb_j \in \Sigma$.

Given a state $\state_i$, the probability that the next symbol takes the value $\symb_j$ is given by $\pi(\state_i,\symb_j)$.  We use the shorthand $\pi_i$ for the state-specific discrete distribution over symbols for state $\state_i$.  We could equivalently write $\sigma|\state_i \sim \pi_i$ where $\sigma$ is a random variable that takes values in $\Sigma$.  Given a state $\state_i$ and a symbol $\symb_j$, however, the next state $\state_{i'}$ is {\it deterministic}: $\state_{i'} = \delta(\state_i,\symb_j).$   The process for generating from a PDFA involves repeatedly generating a symbol stochastically given the state the process is in, then given that state and the emitted symbol transitioning deterministically to the next state.  This is the reason for the confusing ``probabilistic deterministic'' name for these models.  Turning this around, given data, as as long as the initial state is known then there is no uncertainty about the path through the states during inference and prediction. 
This is a primary source of computational savings relative to HMMs.

PDFAs are more general than $n$th-order Markov models (i.e. $m$-gram models, $m=n+1$), but less expressive than hidden Markov models (HMMs)\cite{Dupont2005}.  For the case of $n$th-order Markov models, we can construct a PDFA with one state per suffix $x_1 x_2 \ldots x_n$.  Given a state and a symbol $x_{n+1}$, the unique next state is the one corresponding to the suffix $x_2 \ldots x_{n+1}$.  Thus $n$th-order Markov models are a subclass of PDFAs with $\mathcal{O}(|\Sigma|^n)$ states.  For an HMM, given data and an initial distribution over states, it is possible to recursively calculate a distribution over states at time $t$.  PDFAs are those HMMs which, given a unique start state, the distribution over states at time $t$ is degenerate at a single $\q_t \in Q$ for all $t$.  As we explain in Section \ref{sec:theory}, mixtures of PDFAs are strictly more expressive than single PDFAs, but still less expressive than PNFAs.