% !TEX root = main.tex
%\subsection*{Abstract}

We interpret results from a study where data was modeled using constant space approximations to the sequence memoizer.   The sequence memoizer (SM) is a non-constant-space, Bayesian nonparametric model in which the data are the sufficient statistic in the streaming setting.  We review approximations to the probabilistic model underpinning the SM that yield the computational asymptotic complexities necessary for modeling very large (streaming) datasets with fixed computational resource. Results from modeling a benchmark corpus are shown for both the effectively parametric, approximate models and the fully nonparametric SM.  We find that the approximations perform nearly as well in terms of predictive likelihood.  We argue from this single example that, due to the lack of sufficiency, Bayesian nonparametric models may, in general, not be suitable as models of streaming data, and propose that  nonstationary parametric models and estimators for the same  inspired by Bayesian nonparametric models may be worth investigating more fully.

 %are too large for batch inference procedures on a fixed computer, and still achieve good performance . 

%In order to make deplump stream-capable%To make deplump stream-capable we introduce a new approximation for inference in the underlying probabilistic model.  Combined with existing inference techniques the new approximation yields an algorithm with the computational asymptotics required for streaming compresion.  We demonstrate the performance of this streaming deplump variant on large text corpora.
