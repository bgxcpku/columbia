% !TEX root = main.tex
\section{Discussion}
\label{sec:discussion}

Whac-a-Mole\texttrademark~is a frustrating yet fun game common to American video-game parlors.  Game play consists of using a hammer to bash the head of a mole that randomly pops up out of an array of holes in front of the player.  Our experience with scaling the sequence memoizer was like a game of whac-a-mole with the nonparametric nature of the SM playing the role of the mole -- it kept popping up and ruining our attempts, sometimes in unexpected ways, despite all our bashing.  Ultimately we succeeded in producing an asymptotically scalable version of the SM for streaming data, however, to do so we had to abandon essentially all of its nonparametric features.  Intuitively the result can be thought of as some kind of estimator for a non-stationary, parametric model; however the exact nature of this model has yet to be characterized.  The lessons learned by playing this game are obvious in retrospect and extend to all fundamentally nonparametric models where the data is the sufficient statistic: asymptotic scalability is a problem.   For testing and statistical discovery of patterns in finite datasets, perhaps Bayesian nonparametric models have a role to play; however, our experience suggests a no-free-lunch result: it seems that one can not abandon sufficiency and have scalability too.

%%In this paper we developed a new streaming variant of deplump.  We showed that the approximations introduced to make deplump stream-capable had almost no effect on compression performance relative to batch deplump, meaning that streaming deplump likewise outperforms unbounded context CTW, PPMZ, among others yet can be used to compress streaming data of unbounded length.  It remains the case that streaming deplump could be integrated into the PAQ family of compressors. 
%We empirically demonstrated that streaming deplump continues to improve as the stream length increases up to 1,000Mb.  Furthermore, we show that a reasonable bound on the total count in each suffix tree node has no effect on compression performance. We demonstrate that the depth of the tree can be a fixed low constant (10 - 32) without adverse effect on performance and we note that the algorithm performs better on long sequences when the number of nodes in the tree ($L$) is set high ($10^6$ or $10^7$).

%There are both theoretical and empirical questions to pursue in future work.  Of theoretical interest is the exploration of more expressive graphical models. Useful extensions might be made either by introducing sharing structures other than suffix trees, or by considering latent distributed (feature) representations of the stochastic generative process.  We would also like a more explicit mathematical characterization of how the approximations we make effect the inference procedure.  On the empirical side 
%Future work includes expanding the experimentation to include studies of compressing other datatypes.  We intend to purse PAQ-like model mixing.  