% !TEX root = deplump.tex
\section{Methodology}
\newcommand{\T}{\ensuremath{\mathcal{T}}}
\newcommand{\N}{\ensuremath{\mathcal{N}}}
\newcommand{\M}{\ensuremath{\mathcal{M}}}
\newcommand{\PP}{\ensuremath{\mathcal{P}}}
\newcommand{\nc}{\ensuremath{nc}}
\newcommand{\RS}{\ensuremath{\mathcal{R}}}
\newcommand{\D}{\ensuremath{\mathcal{D}}}
\newcommand{\la}{\ensuremath{\leftarrow}}
\newcommand{\G}{\ensuremath{\mathcal{G}}}
\newcommand{\IS}{\ensuremath{\mathcal{I}}}
\newcommand{\Seq}{\ensuremath{\mathcal{S}}}
\newcommand{\dd}{\ensuremath{\delta}}
\newcommand{\U}{\mathcal{U}}
\label{section:methodology}

In this section we first review the sequence memoizer model and the inference algorithm used in the batch deplump compressor. We then review two approximations to this inference algorithm that we combine for the first time in this paper.  Taken together the two approximations ensure asymptotically constant model storage complexity.  A third approximation, novel to this paper, ensures that the asymptotic computational complexity of the inference algorithm is linear in the length of the input sequence.

\subsection{Review}

Note that a distribution $P$ over sequences can be factorized as $P(S = [s_0, s_1, \ldots, s_m]) = P(s_0)P_{[s_0]}(s_1)P_{[s_0,s_1]}(s_2) \ldots P_{[s_0,s_1,\ldots,s_{m-1}]}(s_m)$, where $P_\U (s) = P(s | \U)$.  The sequence memoizer \citep{Wood2009} models these conditional distributions using a hierarchical Bayesian framework in which non-negative, integer parameters $\{ c^\U_\sigma, t^\U_\sigma \}_{\sigma \in \Sigma, \U \in \Sigma^{+}}$ are used to characterize each $P_\U$.  If we define $c^\U = \sum_{\sigma \in \Sigma} c^\U_\sigma$ and $t^\U = \sum_{\sigma \in \Sigma} t^\U_\sigma$ then the model is
%
\begin{eqnarray*}
	P_\U(\sigma) &=& \frac{c^\U_\sigma - t^\U_\sigma \delta^\U}{c^\U} +  \frac{t^\U \delta^\U  P_{ \xi(\U)}(\sigma) }{c^\U} \\
\end{eqnarray*}
%
\noindent where $\xi([s_1, s_2, s_3, \ldots]) = [s_2,s_3, \ldots]$ and $P_{\xi([])}$ is the uniform distribution over $\Sigma$.

%The hierarchy used to define the model makes use of the Pitman-Yor \citep{Pitman1997} distribution over distributions. If $P \sim \PY(d,c,G)$ then we say $P$ follows a Pitman-Yor distribution with discount parameter $d$, concentration parameter $c$, and mean distribution $G$.  The mean distribution $G$ can be understood as the average $P$ from this distribution, i.e. $\mathbb{E}(P(\sigma)) = G(\sigma)$ for $\sigma \in \Sigma$.  The discount parameter is a real value between 0 and 1.  If the discount is close to 1 it is an indication that $P$ is likely to be very close to $G$; d close to 0 indicates $P$ may vary significantly from $G$.  In the sequence memoizer the concentration parameters are restricted to 0 and will be omitted from the remaining discussion. If we define the suffix operator $\sigma$ on discrete sequences as $\sigma([s_0, s_1, \ldots, s_m]) = [s_1,s_2, \ldots, s_m])$ then we can write the sequence memoizer model as 
%%
%\[
%\begin{array}{rcl}
%	P_{[ ]} 	|	\delta_0 					& \sim & \PY(\delta_0,0,\mathcal{U}_{\Sigma})\\
%	P_{\bf u} 	| 	\delta_{|{\bf u}|}, P_{\sigma({\bf u})} 	& \sim & \PY(\delta_{|{\bf u}|}, 0, P_{\sigma({\bf u})}) \\
%	s_n 		|  	P_{[s_0, s_1, \ldots, s_{n-1}] }	& \sim & P_{[s_0, s_1, \ldots, s_{n-1}] }
%\end{array}
%\]
%%
%\noindent where $\mathcal{U}_{\Sigma}$ is the uniform distribution over $\Sigma$.  The hierarchy used in the model smooths each conditional distribution $P_{\bf u}$ towards a related, more general distribution $P_{\sigma({\bf u})}$.  Intuitively the hierarchy indicates that the most recent context is the most informative for modeling the conditional distributions.

Given a model and observed data,  the task of inference is to learn likely values of the latent parameters.  The latent parameters of the sequence memoizer are the counts $\{c^\U_\sigma, t^\U_\sigma \}$ and the discount parameters $\delta^\U$ for $\sigma \in \Sigma$ and  $\U \in \Sigma^{+}$ \citep{Gasthaus2011}. While $|\Sigma^{+}| = \infty$, \citep{Wood2009} show that inference only requires computation on $\{c^\U_\sigma, t^\U_\sigma \}$ for $\sigma \in \Sigma$ and $\U \in \mathcal{H} \subset \Sigma^{+}$ for any finite training sequence $\mathcal{S}$ such that $|\mathcal{H} | \leq 2 |\mathcal{S}|$.  
%Furthermore, they parameterize the discounts such that $\delta_n$ is a function of $\{\delta_0, \delta_1, \ldots, \delta_{10} \}$ for $n \geq 0$.   
Unfortunately the linear growth of $|\mathcal{H}|$ makes using the model intractable for streaming sequences. \citep{Bartlett2010} demonstrate an approximate inference method that maintains a constant bound on $|\mathcal{H}| $ for arbitrary length sequences yet yields results comparable to inference in the full model. Combining these model representations yields an effective parameter space of at most $2|\Sigma| |\mathcal{H}|$ unique counts. Finally, if the discount values $\delta^\U$ are parameterized by a finite number of parameters these results yield an inference algorithm with asymptotically constant storage complexity.  

%Given a model and observed data,  the task of inference is to learn likely values of the latent parameters.  The latent parameters of the sequence memoizer are  $\mathcal{G} = \{P_{\bf u} \ | \ {\bf u} \in \Sigma^{+} \}$ and $\delta_n$ for $n \geq 0$.  While $| \mathcal{G}| = \infty$, \citep{Wood2009} show that inference for the full model only requires computation on a set $\mathcal{H} \subset \mathcal{G}$ for any finite training sequence $\mathcal{S}$ such that $|\mathcal{H} | \leq 2 |\mathcal{S}|$.  Furthermore, they parameterize the discounts such that $\delta_n$ is a function of $\{\delta_0, \delta_1, \ldots, \delta_{10} \}$ for $n \geq 0$.   Unfortunately the linear bound on $|\mathcal{H}|$ makes using the model intractable for streaming sequences. \citep{Bartlett2010} and \citep{Gasthaus2010} demonstrate that approximate inference using a single particle filter and a pruning scheme yields results comparable to inference for the full model, while maintaining a constant bound on $|\mathcal{H}| $ for arbitrary length sequences.  Finally, \cite{Gasthaus2011} show that computation on $\mathcal{H}$ requires maintaining at most most $2|\Sigma| |\mathcal{H}|$ unique counts.  If these techniques are combined the result is an inference algorithm with a constant space representation of the parameters $P \in \mathcal{H}$.  

%Inference in the model is performed using a compressed representation in which only two counts for each symbol need to be maintained for a subset of the conditional distributions.  We will refer to this subset at element $t$ of the sequence as $\mathcal{H}$.  The cardinality of $\mathcal{H}$ grows linearly in the length of the sequence.  Bayesian inference is typically performed by averaging over the posterior distributions of parameters, but \cite{Gasthaus2010} demonstrate that using a single particle approximation yields excellent empirical results. \cite{Bartlett2010}  show that by making some independence assumptions the approximation can be extended and the cardinality of $\mathcal{H}$ can be bounded from above to provide a constant space inference procedure without much loss in  empirical performance.
%
\subsection{Approximation}

We introduce two more approximating procedures that, combined with those above, yield streaming deplump.  First, because the data structure used by batch deplump is a suffix tree its storage complexity is linear in the input sequence length.  For this reason, streaming deplump cannot maintain a suffix tree representation of the entire input sequence.  Instead a fixed-length ``reference sequence''  is maintained (see Algorithm~\ref{alg:pmfnextsymbol}), along with a dynamically updated suffix tree referencing only the suffixes found therein.  Deleting nodes to constrain the size of this context tree forces node-removal inference approximations of the sort defined in \citep{Bartlett2010}, operations that are justifiable from a statistical perspective but whose net practical effect on compression performance is characterized for the first time in this paper. %This means we make the data structure underlying streaming deplump an approximation to the data structure used in the published inference algorithms, which allows our algorithm to maintain constant space complexity.

As noted, the efficient representation introduced in \cite{Gasthaus2011} maintains only the counts $\{ c^\U_\sigma,t^\U_\sigma\} $ for each $\U \in \mathcal{H}$ and $\sigma \in \Sigma$.  During incremental estimation, for each symbol $s$ in the input sequence, $c^\U_s$ is incremented in at least one node on the tree.  Clearly $\max_{\U \in \mathcal{H}, \sigma \in \Sigma} \{ c^\U_\sigma \}$ then grows monotonically as a function of the length of the input sequence. Unfortunately incremental construction of the model includes an operation on node elements known as fragmentation (Alg.~\ref{alg:getnode}) that requires computation proportional to $\max_{\sigma \in \Sigma} \{ c_\sigma^\U \}$ for node $\U$ \citep{Gasthaus2011}. Therefore, to ensure computational complexity that is independent of the sequence length, $c^\U_\sigma$ must be bounded for all $\U \in \mathcal{H}$.  The effect of imposing this restriction on compression performance is also characterized for the first time in this paper. 

%The counts $c_{\sigma}$ and $t_{\sigma}$ are constrained such that $0 \leq t_{\sigma} \leq c_{\sigma} $. 
%The $c^\U_{\sigma}$ are counts of atoms in the estimation of a discrete distribution over $\Sigma$.  The $t^\U_{\sigma}$ regularize the estimation and create smoothing towards the distribution at a more general context.  Intuitively, limiting $\max_{P \in \mathcal{H}} \{ c^\U =  \sum_{\sigma \in \Sigma} c^\U_{\sigma} \} < k$  has the effect of using at most $k$ observations to estimate each discrete distribution in $\mathcal{H}$.  If $k$ is set high relative to $|\Sigma|$ this approximation should have little effect on the estimated distribution. We implement this in Algorithm~\ref{alg:thincounts}.  Using a fixed $k$ for all $P \in \mathcal{H}$ forces the computational complexity of the algorithm to be a linear function of the input sequence while maintaining constant space complexity.





\begin{algorithm}[t!]
    \caption{Deplump/Plump} \label{alg:deplump/plump}
    \begin{algorithmic}[1]
    	\Procedure{$\mathcal{O} \la $ Deplump/Plump}{$\IS$}
		\State $\RS \la [$ $]$ ;  $\mathcal{O} \la  [$ $]$ \Comment{global reference and output sequence}
%		\State Initialize $[$ $]$ node of \T \Comment{suffix tree}
		\State $\nc \la 1$ \Comment{global node count}
		\State $\D \la  \{ \dd_0, \dd_1, \dd_2, \dots, \dd_{10}, \alpha \}$ \Comment{global discount parameters}
% \Comment{output sequence}
		\For{i = 1: $| \IS|$}
			\State $\G \la \vec 0$ \Comment{discount parameter gradients, $|\G| = |\D|$}
			\State $ \{ \pi, \N  \} \la$  \textsc{PMFNext} (\RS)
			\If{Plump}
%				\State $s \la $ 
				\State $\mathcal{O} \la [\mathcal{O}$,  \textsc{Decode}$(\pi, \IS)]$
			\Else
				%\State $s \la \IS[i]$
%				\State $b \la$
				\State $\mathcal{O}\la [\mathcal{O}$,   \textsc{Encode}$(\sum_{j= 0}^{\textsc{prev}(\IS[i])} \pi_j, \sum_{j = 0}^{\IS[i]} \pi_j)]$		
			\EndIf
			\State \textsc{UpdateCountsAndDiscountGradients}($\N,s,\pi_s,$TRUE)
			\State $\D \la \D + \G \eta / \pi_s$ \Comment{update discount parameters}
			\State $\RS \la [\RS$ $s]$ \Comment{append symbol to reference sequence}
		\EndFor
%		\State \Return $\mathcal{O}\mathcal{S}$
	\EndProcedure
	\end{algorithmic}
\end{algorithm}

