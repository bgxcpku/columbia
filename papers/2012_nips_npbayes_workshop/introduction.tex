% !TEX root = main.tex
\section{Introduction}
\label{sec:introduction}

%Deplump \citep{Gasthaus2010} is a general purpose, lossless, batch compressor based on a probabilistic model of discrete sequences called the sequence memoizer \citep{Wood2009}.   \citeauthor{Gasthaus2010} showed that although deplump is algorithmically similar to the PPM and CTW compression algorithms, particularly their unbounded context lengths variants \citep{Cleary1997,Willems1998}, the coherent probabilistic model underlying deplump gives it a consistent empirical advantage.  In particular, \citeauthor{Gasthaus2010} showed that deplump generally outperformed CTW  \citep{Willems2009}, PPMZ \citep{Peltola2002}, and PPM* \citep{Cleary1997} on the large Calgary corpus, the Canterbury corpus, Wikipedia, and Chinese text.  Deplump was shown to underperform in comparison to the PAQ family of compressors \citep{Mahoney2005}, but the point was made that deplump (or more specifically the sequence memoizer) could replace one or all of the mixed, finite-order Markov-model predictors included in PAQ.  

The sequence memoizer \citep{Wood2009, Wood-2011-CACM} (SM) is a Bayesian nonparametric model for sequential stochastic processes that generate discrete observations.  Because the SM is a model in which the data is the sufficient statistic, it has space complexity that grows linearly as a function of the number of observations.  Since it was first introduced, two complementary modifications to the sequence memoizer have emerged \citep{Bartlett2010,Gasthaus2011} that, when combined as they were in \citep{Bartlett-2011-DCC}, together result in a constant space approximation to the sequence memoizer.  %Practically speaking, these approximations result in a model capable of actually incorporating streaming observations while still being representable on a fixed capacity computer.  

To review: the SM can be thought of as a hierarchical smoothing prior for multiple simultaneous conditional distribution estimation. 
% It can also, for instance, be viewed as a smoothing $n$-gram model in the limit of $n\rightarrow\infty$. 
 ``Observations'' in the SM are the discrete (countable) symbols generated by some underlying stochastic process and situated in the full sequence or ``context'' of observations that were already generated.  It has been shown that the space complexity of the sequence memoizer is on the order of the number of nodes in a suffix-tree representation of this sequence of observations times the storage required to represent the conditional density estimate at each node. An approximation to the sequence memoizer in which the number of nodes in the suffix-tree remains of asymptotically constant order was introduced in \citep{Bartlett2010}.  Unfortunately, in that work the storage requirement at each node grew as an uncharacterized but not-constant function of the input sequence length.  A method for constraining the memory to be of constant order at each node was later introduced in \citep{Gasthaus2011}.  Unfortunately, the representation they proposed resulted in the computational cost of inference growing as a super-linear function of the length of the training sequence.  \cite{Bartlett-2011-DCC} combined both and introduced two more approximations that rendered the computational cost of inference asymptotically linear in the observation sequence length and cost of storage asymptotically constant in the same.  We intrepret results from that paper here.  

%The primary contribution of this work is to marry these two approximations such that constant memory, linear time approximate inference in the sequence memoizer is achieved, thereby rendering deplump appropriate for streaming lossless compression applications.  In addition to combining these two approximations, a third and final approximation is introduced to constrain the computation required by the model representation introduced in  \citep{Gasthaus2011}. As the asymptotic statistical characteristics of the combined approximations are difficult to mathematically characterize, we include significant experimental exploration of the approximation parameter space and its effect on compression performance.  Additionally, as the resulting deplump compression algorithm is of fairly high implementation complexity, we have included a nearly comprehensive algorithmic description of it in this paper.  This accompanies a reference implementation which can be explored at \texttt{http://www.deplump.com/}.

%Despite the approximations required to achieve algorithmic asymptotic complexity appropriate for streaming compression, we find that our constant-space deplump compressor performs nearly as well as the original described in \citep{Gasthaus2010}.  %Furthermore, as a somewhat unexpected consequence of being able to expose the underlying probabilistic model to more data, we find evidence that the constant-space deplump compressor achieves extremely good long-run streaming compression performance.
