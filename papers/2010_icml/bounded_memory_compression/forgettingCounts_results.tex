\section{Experiments and results}
\label{results}

The data we used for the first experiment was the Calgary Corpus \cite{calgary corp}.  The Calgary Corpus is made up of 14 separate documents meant to simulate the task of compression in a general setting. Documents were modeled separately as a sequence of bytes.  Results are reported as corpus wide average log-loss in bits.  In the compression setting this corresponds directly to compression ratio.

%\label{figResultsCC}

To baseline our model we ran two other constant space sequence models and two standard compressors.  The first was an adaptation of the SM model to finite depths.  By bounding the depth of the SM we create a finite depth HPYP model with concentration parameters zero.  Inference was performed using the single particle particle filter. The second was a naive constant space modeling technique using the SM to model.  When the number of instantiated restaurants in the model reached the threshold the remainder of the sequence was modeled with a separate SM model.  We also compressed each file using gzip and bzip2 \cite{gzip} \cite{bzip2}.  Default parameters were used for the commercial compressors.

The first step of the experiment was to model each document in the corpus with finite depth SM models of depths 2 through 9.  For each document and model we recorded the number of instantiated restaurants in the final state of the model.  The document requiring the largest number of instantiated restaurants provides an implicit bound on the space required to model the corpus as a whole. The implicit bound specific to each depth was then used as a threshold in the SM model paired with the two deletion schemes proposed in Section~\ref{inference}. This method of comparison was used because the standard finite depth models do not permit a space upper bound specification a priori.

Each of the four sequence models were fit ten times to understand the variance of the inference procedure.  Our results show the standard deviation of the average log loss to be less than $0.002$ for all of the methods.  All differences detectable in the graph are significant at the $\alpha = 0.01$ level. We note that the SM model paired with either deletion scheme consistently out performs the finite depth model.  Furthermore, the greedy deletion scheme shows improvement over the random deletion scheme, especially for low thresholds.  The SM model paired with either deletion scheme outperformed the commercial compressors at every threshold tested.