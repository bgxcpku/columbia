\section{Experiments and Results}
\label{results}

In tests of predictive probability using incremental inference, the constant space deletion schemes outperformed commercial compressors and simpler constant space models, while approaching the performance of linear space methods even with significantly less space.  The data we used in our experiments was the Calgary Corpus (CC) \cite{Bell1989}.  The CC is a compression benchmark corpus that consists of 14 files chosen to cover a typical range of file types.  For each model tested (naive, finite depth, random deletion, greedy deletion) a separate instance of that model was trained on each file.  Each file was treated as a sequence of bytes, estimation was performed incrementally using a single-particle filter, and the predictive probability of each byte was calculated before incorporation into the model.  Each run of each model for each file was initialized with a different random number generator seed.  Figure~\ref{figResultsCC} shows the average predictive performance of several sequential prediction models.  The reported results are the log-loss in bits averaged over all documents in the corpus.  In the compression setting this corresponds directly to compression ratio \cite{Cover1991}.

%\label{figResultsCC}

To evaluate our two procedures for constant space inference, we compared to the full SM, two other simple constant space versions of the SM and two standard compressors.  The SM does not delete any observations and incremental inference is linear in space and time.  The two inference procedures of interest (random and greedy deletion) are both schemes for deleting observations from the model, one which randomly deletes observations and one which greedily deletes the observations which make the smallest change possible to the approximate likelihood of the data already observed, as described in Section~\ref{inference}.  Inference in the naive constant space SM runs the same as for a normal SM until the maximum number of restaurants is reached, at which point all observations are removed and inference continues along the remainder of the sequence.  The finite depth model takes the SM and bounds the length of a preceding sequence included in an observation.  Thus, it is a HPYP model with finite tree depth.  Since we cannot know the number of restaurants in a finite depth model before estimation, we perform inference with this model first and use the number of restaurants as an upper bound on the space allowed by the other models, as described below.  We also compressed each file using gzip and bzip2 \cite{Deutsch1996, Seward1999}.  Default parameters were used for the commercial compressors.

\comment{To baseline our model we ran two other constant space sequence models and two standard compressors.  The first was an adaptation of the SM model to finite depths.  By bounding the depth of the SM we create a finite depth HPYP model with concentration parameters zero.  Inference was performed using the single particle particle filter. The second was a naive constant space version of the SM.  Whenever the number of instantiated restaurants in the model reached the threshold a new SM with zero observations was created and used to model the remainder of the sequence, until the memory bound was again reached.  We also compressed each file using gzip and bzip2 \cite{Deutsch1996, Seward1999}.  Default parameters were used for the commercial compressors.}

We first modeled each file with a finite depth SM model and recorded the number of instantiated restaurants for each file.  The model complexity was allowed to grow without bound, meaning the finite depth model was at its full representational capacity.  The maximum number of instantiated restaurants gave an upper bound on the number of restaurants needed to model any file in the corpus with an HPYP model of a given depth.  We calculated this upper bound for depths 2 through 9.  This set of bounds was then used as thresholds for the naive model and the random and greedy deletion models.  Results are listed in Figure~\ref{figResultsCC}.  For comparison, the full SM model instantiated up to 1,160,765 restaurants when trained on the same documents.  We chose this method of comparison because the standard finite depth models do not permit an upper bound specification on space a priori.

Each of the four sequence models were fit ten times.  From that the variance of the inference procedure was estimated.  Our results show the standard deviation of the average log loss to be less than $0.002$ for all of the methods.  All differences detectable in the graph are significant at the $\alpha = 0.01$ level. We note that the SM model paired with either deletion scheme consistently outperforms the finite depth model and the naive model.  We also note that both deletion schemes approach quite close to the performance of the full SM with only one sixth as many restaurants.  Furthermore, the greedy deletion scheme shows improvement over the random deletion scheme, especially for low thresholds.  The SM model paired with either deletion scheme outperformed the commercial compressors at every threshold tested.