\section{Experiments and Results}
\label{results}

In tests of predictive probability using incremental inference, the constant space deletion schemes outperformed commercial compressors and simpler constant space models, while approaching the performance of linear space methods even with significantly less space.  The data we used in our experiments was the Calgary Corpus (CC) \cite{Bell1989}.  The CC is a compression benchmark corpus that consists of 14 files chosen to cover a typical range of file types.  A separate instance was trained for every combination of model tested (naive, finite depth, random deletion, greedy deletion) and file.  Files were treated as a sequences of bytes.  Estimation was performed incrementally using Algorithm~\ref{alg1} with $K=1$. The posterior predictive probability of each byte was calculated before that byte was incorporated into the model.  The random number generator was initialized with a new seed for every instance at the start of each run.  Figure~\ref{figResultsCC} shows the average predictive performance of several sequential prediction models (error bars were too small to be included).  The reported results are the per-byte log-loss in bits averaged over all files in the corpus.  \comment{In the compression setting this corresponds directly to compression ratio \cite{Cover1991}.}

%\label{figResultsCC}

To evaluate our two procedures for constant space inference, we compared to the full SM, two other simple constant space versions of the SM and two standard compressors.  The SM does not delete any restaurants and incremental inference is linear in space and time.  The two inference procedures of interest (random and greedy deletion) are both schemes for deleting restaurants from the model, one which randomly deletes restaurants and one which greedily deletes the restaurants which make the smallest change possible to the approximate likelihood of the data already observed, as described in Section~\ref{inference}.  Inference in the naive constant space SM runs the same as for a normal SM until the maximum number of restaurants is reached, at which point all restaurants are removed and inference continues along the remainder of the sequence.  The finite depth model takes the SM and bounds the length of a preceding sequence included in an observation.  Thus, it is a HPYP model with finite tree depth.  Since we cannot know the number of restaurants in a finite depth model before estimation, we perform inference with this model first and use the number of restaurants as an upper bound on the space allowed by the other models, as described below.  We also compressed each file using gzip and bzip2 \cite{Deutsch1996, Seward1999}.  Default parameters were used for the commercial compressors.

\comment{To baseline our model we ran two other constant space sequence models and two standard compressors.  The first was an adaptation of the SM model to finite depths.  By bounding the depth of the SM we create a finite depth HPYP model with concentration parameters zero.  Inference was performed using the single particle particle filter. The second was a naive constant space version of the SM.  Whenever the number of instantiated restaurants in the model reached the threshold a new SM with zero observations was created and used to model the remainder of the sequence, until the memory bound was again reached.  We also compressed each file using gzip and bzip2 \cite{Deutsch1996, Seward1999}.  Default parameters were used for the commercial compressors.}

We first modeled each file with a finite depth SM model and recorded the number of instantiated restaurants.  The model complexity was allowed to grow without bound, meaning the finite depth model was at its full representational capacity.  The maximum number of instantiated restaurants gave an upper bound on the number of restaurants needed to model any file in the corpus with an HPYP model of a given depth.  We calculated this upper bound for depths 2 through 9.  This set of bounds was then used as the maximum number of restaurants for the naive model and the random and greedy deletion models.  Results are listed in Figure~\ref{figResultsCC}.  For comparison, the full SM model instantiated 1,160,765 restaurants when trained on the same documents.  

Each of the four sequence models were fit ten times.  From that the variance of the inference procedure was estimated.  Our results show the standard deviation of the average log loss to be less than $0.002$ for all of the methods.  All differences detectable in the graph are significant at the $\alpha = 0.01$ level. We note that the SM model paired with either deletion scheme consistently outperforms the finite depth model and the naive model.  We also note that both deletion schemes approach quite close to the performance of the full SM with only one sixth as many restaurants.  Furthermore, the greedy deletion scheme shows improvement over the random deletion scheme, especially for low thresholds.  The SM model paired with either deletion scheme outperformed the commercial compressors at every threshold tested.