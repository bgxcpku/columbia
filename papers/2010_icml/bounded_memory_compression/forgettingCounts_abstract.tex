\begin{abstract} 
We propose a hierarchical Bayesian nonparametric model for discrete data that can be represented in constant space and estimated incrementally in linear time.  The inference procedure can be viewed as either approximating a model whose complexity grows as a function of the data or as estimation of a sequence of dependent nonparametric distributions.  We devise two estimation schemes corresponding to the two interpretations, a random deletion scheme and a greedy deletion scheme.  When tested on general compression corpora both procedures approach the performance of linear space methods while using significantly less memory, as well as outperforming both finite-depth models with the same memory bounds and standard compression methods.
\end{abstract} 