\begin{abstract} 
We propose a novel {\em dependent} hierarchical Pitman Yor process model for discrete data.   An incremental Monte Carlo inference procedure for this model is developed.  We show that inference in this model can be performed in {\em constant space} and linear time.  The model is demonstrated in a discrete sequence prediction task where it is shown to achieve state of the art sequence prediction performance while using significantly less memory.  
%can be viewed as either approximating a model whose complexity grows as a function of the data or as estimation of a sequence of dependent nonparametric distributions.  We devise two estimation schemes corresponding to the two interpretations, a random deletion scheme and a greedy deletion scheme.  When tested on general compression corpora both procedures approach the performance of linear space methods while using significantly less memory, as well as outperforming both finite-depth models with the same memory bounds and standard compression methods.
\end{abstract} 