\section{Inference}
\label{inference}

%The basic model we use is the SM model following the advice of \cite{Gasthaus}.  We parameterize the SM model with a unique discount parameter for each of the first ten depths.  The discount parameter for all depths below ten is set equal to the discount parameter at depth ten.  Furthermore, our generative process specifies that when the number of restaurants reaches a threshold a restaurant is chosen uniformly at random from the set of leaf restaurants and deleted.  Deletion is continued until the number of instantiated restaurants is below the threshold.

Given the motivation for this paper we suggest that inference should be incremental.  An incremental technique known as particle filtering has been explored in several non-parameteric Bayesian settings \cite{Fearnhead2004, Wood2007}.  The sequential nature of the generative process we propose makes inference using a particle filter approach straightforward. A particle filter works by using a swarm of particles to approximate the posterior distribution over the model space conditional on observed data.  The approximation is represented as a weighted sample, each particle embodying a possible state of the model.  In view of a new data point the weight of each particle is adjusted to reflect the new information.  In the non-parameteric Bayesian setting, since the parameter space is growing as a function of the length of the data, values for new parameters must be incorporated into each particle.  New parameter values are produced through a proposal distribution which is taken into account when the weight of each particle is updated.  To make the inference more efficient proposal distributions are usually chosen to approximate the posterior distribution of the new parameter given the current set of observed data.

We propose here an inferential procedure for the dependent memory-constrained hierarchical Pitman-Yor process model performed in the modified dependent Chinese restaurant franchise representation.  Using this representation, each observation corresponds to a customer seated in the appropriate restaurant.  The latent parameters in the model are the table at which each customer is seated and the restaurants which are deleted at each deletion step.  For the table assignments, a particle filter in this representation will, for each particle, seat the customer corresponding to the most recently observed data in the appropriate restaurant at a table produced from a proposal distribution.  Since seating a customer may indicate the presence of an additional customer in the parent restaurant, a table for a new customer may need to be proposed in the parent restaurant as well.  It is possible that customers will need to be seated all the way up the path until the restaurant corresponding to the distribution over symbols conditional on an empty context.  The proposal distribution we suggest for choosing a table for each customer is the posterior distribution conditional on the new observation and the current state of the particle.  Details of this inference scheme applied to the SM can be found in \cite{Gasthaus2010}.

\input{forgettingCounts_algorithm}

When state of the model represented in each particle reaches the memory constraint we will need to propose values for the the latent parameters indicating which restaurants were deleted. We suggest two different proposal distributions to correspond with the two interpretations of the model specification already discussed.  The first is complementary to the understanding of the model as a sequence of dependent distributions with Pitman-Yor process distributions which vary sequentially across the sequence.  The second is complementary to the understanding of the model as a finite state approximation of a model representation which grows linearly as a function of the length of the data.

The first proposal distribution for the deletion of restaurants is uniform over the leaf nodes of the current state of the model.  Using the generative process as the proposal distribution is standard in particle filtering approaches \cite{Doucet2001}.  We refer to the use of this proposal distribution as using a random deletion scheme.

For the second proposal distribution we note that a fixed state of the model represents a likelihood conditional on any context.  We can use this likelihood to approximate the probability of observing the sequence we used to to build the model.  Furthermore, by deleting different leaf restaurants the probability of the sequence, given the updated state of the model, changes and can be approximated in the same way.  The second proposal distribution deterministically proposes the leaf restaurant whose deletion least negatively impacts the likelihood of the observed sequence.  We refer to the use of this proposal distribution as using a greedy deletion scheme. 

While the sequential nature of particle filtering fills the incremental requirement, a particle filter with a large particle swarm could still require too much space.  It is possible to implement particle filtering algorithms where the aspects of the particles which overlap are represented only once and thus induce memory savings.  Using such an algorithm here could be extremely complicated.  We suggest the alternative approach of only using one particle in the particle filter.  This approach was shown to be effective in practice by \cite{Gasthaus2010} and simplifies implementation a great deal.


%\subsection{Complexity}

%A little consideration will show that both of the algorithms suggested for inference in this model require constant space, in the sense of the turing machine, and linear time.  The claim that the algorithms are linear in time stems from the fact that each observation must be seated, but now each seating operation is a constant time operation as the length of any path one must traverse in order to seat an observation is bounded by the total number of instantiated restaurants.  Furthermore, each deletion step requires, at worst, visiting every instantiated restaurant, which if done recursively is a constant time algorithm given that the number of instantiated restaurants is limited.  

%The claim that the algorithm requires constant space requires a little more thought.  It is clear that we have limited the space required by restaurant objects, but what about the actual construction of the tree?  Currently our implementation labels each edge between nodes with two integers which index into the original sequence in order to describe that particular edge.  That is, if the parent restaurant corresponds to the distribution over bytes following $oc$,  and the child restaurant corresponds to the distribution over bytes following $acdoc$, the connecting path may be described by the integer array $[17,20]$ if in the sequence being seated, the entries 18-20 are $acd$.  This type of algorithm requires only constant space for each edge, though it works best if the entire sequence is held in memory.  That being said, considering the sequence being seated to be a semi-infinite tape as in the turing machine stipulation, reversals of the tape are allowable.  Thus, the entire sequence need not be held in memory, it is only necessary that we can reverse the tape to access previous entries in the sequence if we need them.

%As a practical side note, an alternative approach to implementation could store the full connecting context on the edge between nodes.  In the above example this would correspond to labeling the edge with the byte array $[acd]$.  While this does not theoretically require constant space, typically when implementing the algorithm on real data only a short section at the end the array is used.  Caching a fixed length section of the each array on the appropriate edge could drastically reduce the number of times the algorithm requires the tape to reverse.  As an example, if one was fitting the entire model on each document of the calgary corpus separately, as we do in the results section, and were willing to cache arrays of length 6,000, a tape reversal would never be required.  This number could only decrease using either of the deletion schemes suggested.  Finally, if we use a fixed depth model, caching the contexts on the edges requires only constant memory when enforcing an upper bound on the number of restaurants.
