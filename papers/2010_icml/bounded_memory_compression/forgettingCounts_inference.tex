\section{Inference}

Typically in these types of bayesian hierarchical models we would like to perform inference using MCMC sampling methods.  Here, the complexity of the model, and the desired use require online methods.  One natural approach is to use a particle filter given the sequential nature of the generative model.  The base SM model in our implementation is specified and estimated following the outline by [Gasthaus] using their 1PF approach.  That is, we use eleven unique, depth specific discount parameters where the eleventh discount is also the discount for larger depths.  The model is fit using a single particle particle filter and the discount parameters are optimized in a greedy sequential manner.  In the process of seating each observation we calculate the gradient of the predictive probability for the current observation with respect to the discount parameters and then step the discount parameters in the direction of the gradient.

To infer which restaurants to delete we have implemented two different methods.  Both methods are exploratory and are by no means exhaustive.  The first deletes leaf nodes completely at random.  Since we are using a single particle particle filter, this means that as soon as we delete the restaurants, we instantly free up memory.  At first, this deletion scheme seems a bit crude, but note that since we have only one particle, the available information for inferring which restaurants to delete is minimal.  

Our second method takes into account the log probability of the observed data given the current state of the model.  That is, given the current state of the model we can calculate the probability of generating exactly the sequence of data used to build the model.  By deleting different leaf restaurants the probability of the sequence, given the updated state of the model, changes.  We can rank the leaf restaurants in order by which deletions are least helpful regarding predicting the observed sequence and then delete those at the bottom of the ranking.  Since the current state of the model can be seen as a point estimate of the posterior distribution over the model space we consider this deletion scheme to be using, roughly, the useful idea of Bayes factors when deciding which restaurants to delete.

Given that the primary goal of this model is to limit the amount of memory required in the inference algorithm, we parameterized the implementation with an upper limit on the number of instantiated restaurants.  The implementation we used deletes 100 restaurants when the number of instantiated restaurants goes above the max number of restaurants allowed minus two in order to maintain a strict upper bound.  The number 100 was chosen arbitrarily and experimentation might be useful.

\subsection{Complexity}

A little consideration will show that both of the algorithms suggested for inference in this model require constant space, in the sense of the turing machine, and linear time.  The claim that the algorithms are linear in time stems from the fact that each observation must be seated, but now each seating operation is a constant time operation as the length of any path one must traverse in order to seat an observation is bounded by the total number of instantiated restaurants.  Furthermore, each deletion step requires, at worst, visiting every instantiated restaurant, which if done recursively is a constant time algorithm given that the number of instantiated restaurants is limited.  

The claim that the algorithm requires constant space requires a little more thought.  It is clear that we have limited the space required by restaurant objects, but what about the actual construction of the tree?  Currently our implementation labels each edge between nodes with two integers which index into the original sequence in order to describe that particular edge.  That is, if the parent restaurant corresponds to the distribution over bytes following $oc$,  and the child restaurant corresponds to the distribution over bytes following $acdoc$, the connecting path may be described by the integer array $[17,20]$ if in the sequence being seated, the entries 18-20 are $acd$.  This type of algorithm requires only constant space for each edge, though it works best if the entire sequence is held in memory.  That being said, considering the sequence being seated to be a semi-infinite tape as in the turing machine stipulation, reversals of the tape are allowable.  Thus, the entire sequence need not be held in memory, it is only necessary that we can reverse the tape to access previous entries in the sequence if we need them.

As a practical side note, an alternative approach to implementation could store the full connecting context on the edge between nodes.  In the above example this would correspond to labeling the edge with the byte array $[acd]$.  While this does not theoretically require constant space, typically when implementing the algorithm on real data only a short section at the end the array is used.  Caching a fixed length section of the each array on the appropriate edge could drastically reduce the number of times the algorithm requires the tape to reverse.  As an example, if one was fitting the entire model on each document of the calgary corpus separately, as we do in the results section, and were willing to cache arrays of length 6,000, a tape reversal would never be required.  This number could only decrease using either of the deletion schemes suggested.  Finally, if we use a fixed depth model, caching the contexts on the edges requires only constant memory when enforcing an upper bound on the number of restaurants.
