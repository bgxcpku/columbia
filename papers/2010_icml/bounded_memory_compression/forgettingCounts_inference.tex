\section{Inference and model specification}

\subsection{Model specification}

The basic model we use is the SM model following the advice of \cite{Gasthaus}.  We parameterize the SM model with a unique discount parameter for each of the first ten depths.  The discount parameter for all depths below ten is set equal to the discount parameter at depth ten.  Furthermore, our generative process specifies that when the number of restaurants reaches a threshold a restaurant is chosen uniformly at random from the set of leaf restaurants and deleted.  Deletion is continued until the number of instantiated restaurants is below the threshold.

\subsection{Inference}

Typically in these types of Bayesian hierarchical models we would like to perform inference using MCMC sampling methods.  Here, the complexity of the model and the sequential nature of the generative process suggest a particle filter approach. The model is fit using a single particle particle filter and the discount parameters are optimized in a greedy sequential manner.  In the process of seating each observation we calculate the gradient of the predictive probability for the current observation with respect to the discount parameters and then step the discount parameters in the direction of the gradient \cite{Gasthaus}.  Using a single particle in the particle filter means that the deletion scheme immediately results in memory savings.

A particle filter works by using a proposal distribution to step each particle forward and then making the appropriate re-weighting of particles to give a weighted sample from the posterior distribution. We have implemented two different proposal distributions.  These methods are exploratory and by no means exhaustive.  Our first proposal distribution is a uniform distribution over leaf nodes in the model representation.  

Our second proposal distribution deterministically proposes leaf restaurants least predictive of the observed sequence. More specifically, given the current state of the model we can calculate the probability of generating exactly the sequence of data used to build the model.  By deleting different leaf restaurants the probability of the sequence, given the updated state of the model, changes.  We can rank the leaf restaurants and then delete those at the bottom.  Since the current state of the model can be seen as a point estimate of the posterior distribution over the model space we consider this proposal to be an approximate MAP proposal.

\subsection{Complexity}

A little consideration will show that both of the algorithms suggested for inference in this model require constant space, in the sense of the turing machine, and linear time.  The claim that the algorithms are linear in time stems from the fact that each observation must be seated, but now each seating operation is a constant time operation as the length of any path one must traverse in order to seat an observation is bounded by the total number of instantiated restaurants.  Furthermore, each deletion step requires, at worst, visiting every instantiated restaurant, which if done recursively is a constant time algorithm given that the number of instantiated restaurants is limited.  

The claim that the algorithm requires constant space requires a little more thought.  It is clear that we have limited the space required by restaurant objects, but what about the actual construction of the tree?  Currently our implementation labels each edge between nodes with two integers which index into the original sequence in order to describe that particular edge.  That is, if the parent restaurant corresponds to the distribution over bytes following $oc$,  and the child restaurant corresponds to the distribution over bytes following $acdoc$, the connecting path may be described by the integer array $[17,20]$ if in the sequence being seated, the entries 18-20 are $acd$.  This type of algorithm requires only constant space for each edge, though it works best if the entire sequence is held in memory.  That being said, considering the sequence being seated to be a semi-infinite tape as in the turing machine stipulation, reversals of the tape are allowable.  Thus, the entire sequence need not be held in memory, it is only necessary that we can reverse the tape to access previous entries in the sequence if we need them.

As a practical side note, an alternative approach to implementation could store the full connecting context on the edge between nodes.  In the above example this would correspond to labeling the edge with the byte array $[acd]$.  While this does not theoretically require constant space, typically when implementing the algorithm on real data only a short section at the end the array is used.  Caching a fixed length section of the each array on the appropriate edge could drastically reduce the number of times the algorithm requires the tape to reverse.  As an example, if one was fitting the entire model on each document of the calgary corpus separately, as we do in the results section, and were willing to cache arrays of length 6,000, a tape reversal would never be required.  This number could only decrease using either of the deletion schemes suggested.  Finally, if we use a fixed depth model, caching the contexts on the edges requires only constant memory when enforcing an upper bound on the number of restaurants.
