\section{Theory}

While the SM model demonstrates excellent empirical results \cite{Gasthaus}, the linear space requirement is theoretically unsettling and practically unmanageable for long sequences.  We propose a framework for limiting the memory required to represent the model in a way which maintains the theoretical validity of our inference scheme while allowing the model to adapt dynamically to the observed data.

\subsection{Time-varying dependent Pitman-Yor process \cite{caron}} 

The seating arrangement in the Chinese restaurant after seating all the customers can be thought of as a random partition of $n$ unordered object.  Customers at the same table represent objects in the same partition.  The sequential process is a way in which to induce a certain distribution over the possible partitions.  Thus, any process which induces this distribution will result in draws from the correct Pitman-Yor process.  The distribution over random partitions induced by the Pitman-Yor Chinese restaurant process is known as the two parameter Ewen's sampling distribution ($\ES_n(d, c)$).  This distribution satisfies a consistence condition such that when a random object is removed from the partition, the remaining partition follows the $\ES_{n-1}(d,c)$ distribution \cite{pitman}.  Note that the consistence property of the single parameter Ewen's sampling distribution known as the species-deletion property \cite{kingman} does not hold in two parameter case \cite{pitman}.

The consistence property allows the restaurant process to be generalized to produce samples at time points $t = 1 \dots T$ from a time-varying random distribution $\G^t_1$ such that $\G^t_1 \sim \PY(d_1, c_1, \G_0)$ for all $t$.   The fact that $\G^t_1$ has the same distribution over time is known as stationarity in the time-series literature \cite{davis and brockwel?}.  The process works by marginalizing out $\G^t_1$ at all time points and drawing dependent samples.

The process, an extension of the analogous process for Dirichlet processes \cite{caron}, starts with an empty restaurant and generates $x^1_1 \dots x^1_{n(1)}$ through the typical Chinese restaurant process.  Between time points customers are deleted uniformly at random from the state of the restaurant.  After deletion, the sample for the next time point is drawn using the Chinese restaurant process starting with the state of the restaurant after the deletion step.  Because of the consistence property, if $j$ customers are deleted after time step 1 the random partition represented by the starting state of the restaurant at time step 2 has a $\ES_{n(1) - j}(d,c)$ distribution.  The seating process at time step 2 maintains the correct $\ES$ distribution, creating samples drawn from $\G^2_1 \sim \PY(d,c,\G_0)$.  Not that the number of customers removed from the restaurant between time steps is independent of the consistence result and can thus be either stochastic or deterministic.

Dependence between $\G_t^1$ arises from the generalized urn scheme through the undeleted customers.  Exact characterization of the dependence induced by the process is non-trivial, though it is clear that fewer customers removed between times steps induces higher dependence.  In the extreme cases, removing all the customers between time steps gives draws from independent $\G_t^1$, while removing no customers induces a $\G_1$ that is not varying with time. Some exploration is done in \cite{caron} of the dependence structure for the analogous Dirichlet process restaurant scheme.

%TODO give graphical model
\subsection{Time-varying Pitman-Yor Process in a hierarchical setting}

The time-varying framework can be extended to a hierarchical setting at the lowest level of hierarchy in a natural way.  Considering the simple hierarchical setting $\G_2 \sim \PY(d_2, c_1, \G_1)$ and $\G_1 \sim \PY(d_1, c_1, \G_0)$, the model may be generalized to $\G^t_2 \sim  \PY(d_2, c_1, \G_1)$ for all $t  = 1 \dots T$.  One specification of this model is to combine the Chinese restaurant franchise process with the generalized restaurant process.  In this representation customers are removed from the restaurant correspond to $\G_2$ between time steps.  The restaurant corresponding the $\G_1$ is unchanged between time steps. Samples are drawn according the Chinese restaurant franchise already outlined at each time step.

Only simple extensions allow for time-varying distributions higher on the hierarchy are clear.  For a model specification such as $\G_2^t \sim \PY(d_2, c_2, \G_1^t)$ and $\G_1^t  \sim \PY(d_1,d_2,\G_0)$ for all $t$, if we assume independence of the $\{ \G_2^t \}$ then we can employ the generalized restaurant scheme to create a time-varying dependent sequence of restaurant representations at the  $\G_1^t$ level and at each time start with an empty restaurant for the representation used to marginalize out $\G_2^t$.  However, it is not clear how to create dependence at the $\G_2^t$ level through the generalized restaurant process.  Such a restaurant scheme would require a process to update the restaurant state in the $\G_2^t$ level restaurant to reflect changes made to the state of the $\G_1^t$ level restaurant while maintaining a level of dependence.  While it is likely that such a process exists, it is neither intuitive nor necessary for this discussion.

\subsection{Time-varying model applied to SM}

The time-varying model will not only allow for a sequence of time-varying distributions, but can also serve to limit the complexity of the model representation. This framework provides a basis for algorithmic control of the complexity of hierarchical Pitman-Yor models like the SM.  As noted earlier, the number of instantiated restaurants in the SM is the fundamental limiting factor regarding memory usage, thus we will consider a single instantiated restaurant as a unit of memory when discussing the model. For the deletion scheme to limit the amount of memory used by the SM model, we must be able to limit the number of instantiated restaurants.

The theory presented above indicates that between time steps we can only delete customers in restaurants at the lowest level.  In the tree which represents the state of the SM at any given time, this corresponds to the leaf restaurants.  To achieve memory savings we will need to delete all of the customers at a given restaurant.  Restaurants without people need not be represented in the model state.  This deletion operation gives rise to implicit model assumptions. If, at time step $t$, we delete all the customers in a given leaf restaurant, the distribution over types given the particular context pertaining to that path down the tree, it is implicitly assumed to be independent of

, at all subsequent time steps, is, conditionally on the base distribution, independent of this particular distribution prior to time $t$.  Furthermore, it will often be the case in the SM model that the parent restaurant for a leaf restaurant is not instantiated, thus to actually attain memory savings by deleting a leaf restaurant we must effectively delete all the restaurants in this particular path up until the nearest instantiated restaurant.  This means the implicit model assumption is that all of the distributions after time $t$ corresponding to those many deleted restaurants are conditionally independent of their previous states.

This is the basic framework for all of the bounded memory algorithms we present in the results section and is the main contribution of this paper.  The assumption that, for many contexts, the distribution over bytes changes over time seems appropriate in the field of compression as we are interested in compressing general sets of documents. It is not unreasonable to assume that structure varies across the many types of documents a user may wish to compress, or even internally to each particular document.  The assumption of independence required for us to justify our particular deletion process is primarily of practical motivation.  However, the amount of information being disregarded by the deletion process is likely to be minimal as nearly all leaf restaurants will contain only one observation and thus contribute as minimally as one could hope to any given predictive distribution.

Finally, we point out that the theory behind these deletion operations holds for general hierarchical Pitman-Yor processes and thus also for finite depth n-gram style models.  In the results section we show some results concerning this type of model as well.
