\section{Discussion}
\label{discussion}

In the course of this paper we developed one way to incrementally generate dependent HPYP's.  Our method of incrementally generating dependent HPYP's is amenable to sequential Monte Carlo inference techniques.  We know that other ways to generate dependent HPYP's will emerge, and hope that others too will be amenable to efficient incremental inference techniques.  We explained how dependency in our specific dependent HPYP formulation arises from deleting whole restaurants in a Chinese restaurant franchise representation.  We demonstrate that this restaurant deletion scheme can be either be deterministic or probabilistic.  In contrast to others who have used deletion in Chinese restaurant representations to induce dependence between processes we suggested using a deterministic deletion policy to control the memory complexity of inference.  The utility of our predictive model seems promising, particularly when considering how it might be used in probabilistic lossless compressor.

The techniques highlighted in this paper point to interesting avenues for future research.  In essence the restaurant forgetting scheme amounts to a greedy stochastic approach to graphical model structure learning.  As data continually arrives, only those graphical model nodes corresponding to contexts that are both meaningful and continually reappear will remain in the graphical model after many deletion operations.   While the deletion scheme utilized in this work is highlighted in the context of a single family of models corresponding to a single graphical model architecture, it may be possible to use deletion operations in models of more general architecture.