\section{Discussion}
\label{discussion}

In the course of this paper we developed a generative model for a dependent HPYP. This model is amenable to sequential Monte Carlo estimation.  We know that other ways to generate dependent HPYP's will emerge, and hope that others too will be amenable to efficient incremental inference techniques.  We explained how dependency in our specific dependent HPYP formulation arises from deleting whole restaurants in a Chinese restaurant franchise representation.  We show that this restaurant deletion scheme can be either be deterministic or probabilistic.  In contrast to others who have used deletion in Chinese restaurant representations to induce dependence between processes we suggested using a deterministic deletion policy to control the memory complexity of inference.  The utility of our predictive model seems promising, particularly when considering how it might be used in a probabilistic lossless compressor.  This is because the performance of the constant memory models is nearly indistinguishable from that of state of the the art sequence models and significantly better than that of existing commercial compressors.

The techniques highlighted in this paper point to interesting avenues for future research.  In essence the restaurant forgetting scheme amounts to a greedy stochastic approach to graphical model structure learning.  As data continually arrives, only those graphical model nodes corresponding to contexts that are both meaningful and continually reappear will remain in the graphical model after many deletion operations.   While the deletion scheme utilized in this work is highlighted in the context of a single family of models corresponding to a single graphical model architecture, it may be possible to use deletion operations in models of more general architecture.