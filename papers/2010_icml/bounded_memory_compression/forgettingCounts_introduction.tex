\section{Introduction}

The task of general file compression with an algorithm requiring constant space is of obvious practical value.  In any application there will be only a finite amount of memory available and many excellent compression algorithms become unfeasible when compressing large files.  In order to address this task directly an algorithm which allows a specified maximum memory allocation prior to running is needed.  In the case of compression through the use of probabilistic models, the algorithm should control the amount of memory required by adapting the complexity of the model in an appropriate manner to accommodate the stated memory bounds.

Probabilistic models have been shown to perform extremely well when combined with an entropy encoder in compression algorithms.  These models work by predicting the next type in a sequence of types, most often bytes, which make up a file.  The better the model is, the more efficient the encoding scheme and the higher the compression ratio.  The model referred to as a stochastic memoizer for sequence data [wood] has recently been shown to outperform several other such models for general compression tasks [Gasthaus].  Unfortunately many of these byte level prediction algorithms require space on the order of the sequence length and thus become unrealistic for very large files.

Even for models with theoretically constant memory requirements, such as n-gram models, the memory restriction cannot be pre-specified and the theoretical value is such a gross overestimate of the likely memory requirements it does not provide insight regarding the parameterization of the model at the start.  In section 1 of this paper we will review the stochastic memoizer for sequence data [wood].
