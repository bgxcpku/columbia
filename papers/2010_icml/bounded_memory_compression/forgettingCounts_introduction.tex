\section{Introduction}

Parametric and nonparametric models differ in several key ways.  One important way is that parametric models have a fixed, finite number of parameters whereas nonparametric models are often described as having an effective number of parameters that grows as a function of the size of the training data.  An example of the former is Gaussian mixture model.  An example of the latter is a kernel density estimator.  In the former the number of mixture components is fixed a priori and the inference goal might be, in the case of density estimation, to find parameters that maximize the probability of the training data under the model.  In the latter, kernel density estimation case, the ``parameters'' of the model are the data points themselves (and perhaps parameters for a kernel function).  Here the effective number of parameters clearly grows linearly in the size of the training data. 

Bayesian nonparametric (BNP) models can be understood as either having an infinite number of parameters or as having an effective number of parameters that grows as a function of the number of training data observations.   This difference in description is perhaps best explained through an example.  In a model with a Dirichlet process prior like $x_i | \G \sim \G, \G|\alpha,\G_0 \sim \DP(\alpha, \G_0),\; i=1\ldots n$,  either the posterior distribution of $\G |  \{x_i\}_{i=1}^{n},\alpha,\G_0$ or the posterior predictive distribution of $x_{n+1} | \{x_i\}_{i=1}^{n},\alpha,\G_0$ may be of interest.   In the former case, since it is known that both a priori and a posteriori that $\G$ has the form $\G = \sum_{k=1}^\infty \pi_k \delta_{\phi_k}$ \cite{sethurman} it is clear that the number of parameters in the model is infinite (there are an infinite number of so-called ``sticks,'' $\pi_k$ and ``atoms'' $\phi_k$).  On the other hand, ``marginal'' posterior predictive inference can be performed in such a model with $\G$ analytically integrated out.  Such inference utilizes representations in which the ``effective'' number of parameters in the model (really the state required to represent a single posterior sample) grows in expectation as a function of the number of training data observations (Polya urn and Chinese restaurant process samplers \cite{plya_urn, crp} for example).  Other BNP models exhibit similar characteristics.

While nonparametric approaches (\cite{lots of shit}) in general, and BNP approaches in particular (\cite{lots more shit}) have exhibited empirical promise in a wide variety of application domains, enthusiasm for such methods should be damped by the realization that even sub-linear growth in the effective number of parameters is extremely problematic.  This is because, for any ``life-long'' learning agent, or any inference procedure exposed to extremely large training data sets, such growth will eventually make inference prohibitively expensive.  

This issue has been most directly addressed in the Gaussian process (GP) literature, where storage and computational complexity grow quadratically and cubically in the training data size.  To apply GP models to datasets of more than a few thousand data points requires using sparse variants \cite{snelson, csato, etc}.  



How much memory must be used to represent such a model is highly dependent on the inference approach (approximate versus exact) and the representation used (one parameter per observation, one parameter per table and assignment to tables)

%(a Gaussian process is parameterized by mean and covariance {\em functions}, a Dirichlet process by a  

%``speak for themselves''


The task of general file compression with an algorithm requiring constant space is of obvious practical value.  In any application there will be only a finite amount of memory available and many excellent compression algorithms become unfeasible when compressing large files.  In order to address this task directly an algorithm which allows a specified maximum memory allocation prior to running is needed.  In the case of compression through the use of probabilistic models, the algorithm should control the amount of memory required by adapting the complexity of the model in an appropriate manner to accommodate the stated memory bounds.

Probabilistic models have been shown to perform extremely well when combined with an entropy encoder in compression algorithms.  These models work by predicting the next type in a sequence of types, most often bytes, which make up a file.  The better the model is, the more efficient the encoding scheme and the higher the compression ratio.  The model referred to as a stochastic memoizer for sequence data [wood] has recently been shown to outperform several other such models for general compression tasks [Gasthaus].  Unfortunately many of these byte level prediction algorithms require space on the order of the sequence length and thus become unrealistic for very large files.

Even for models with theoretically constant memory requirements, such as n-gram models, the memory restriction cannot be pre-specified and the theoretical value is such a gross overestimate of the likely memory requirements it does not provide insight regarding the parameterization of the model at the start.  In section 1 of this paper we will review the stochastic memoizer for sequence data [wood].
