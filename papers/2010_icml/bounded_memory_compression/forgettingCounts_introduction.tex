\section{Introduction}

Nonparametric models, Bayesian or not, are characterized by computational and memory complexities that grow as a function of the size of the training data.  Unfortunately in common practice the relationship between training dataset size and complexity is used to select a dataset whose size is sufficiently small to allow model estimation.  Clearly this is not ideal given a sufficiently large and complex dataset.

When the computer is fixed but the dataset is not of fixed size but instead grows monotonically, two constraints are imposed on the computational complexity of inference and estimation procedures.  First, the computational complexity of estimation and inference cannot be greater than linear in the size of the data.  Second, the memory complexity of such algorithms must be constant.  Also, particularly in the case of growing data, the only suitable estimation and inference procedures are those that are incremental in nature.

In this paper we develop a {\em constant space, linear time} incremental estimation procedure for models of discrete sequence data based on the hierarchical Pitman-Yor process (HPYP) \cite{Teh2006a}.   Our contribution can be described as a ``forgetting'' procedure that retains only a constant-sized subset of the training dataset.  Inference in the resulting model can be interpreted as either proper inference in a sequence of dependent HPYP's or as approximate inference in a single non-varying HPYP.  To achieve this result we draw on recent results in HPYP estimation, namely the marginalization operations highlighted in the sequence memoizer \cite{Wood2009} and results from dependent Dirichlet process estimation, namely the generalized Polya urn scheme \cite{Caron2007}.  

Our motivation for this work is the sequence memoizer (SM) \cite{Wood2009} which was demonstrated to be a good predictive model for discrete sequence data.  An  {\em linear space, linear time} incremental estimator for the SM was developed and used in a file compression setting \cite{Gasthaus2010}.

If the memory complexity of the estimated model is constant with respect to training data size then both no more than a constant number of datapoints can be retained and no more than a constant number of parameters can be used.  It seems strange then to consider constant memory inference for a nonparametric model because, fundamentally, the resulting constant memory model estimate must be parametric.

Regardless, the Gaussian process regression literature, for one example, includes many such approaches anyway \cite{gaussian process regression stuff, lawrence, csato, snelson, etc}.  The reason for this is that estimation of Gaussian process regression models requires $O(n^2)$ space and $O(n^3)$ time where $n$ is the (growing) number of observations,  hindering their widespread use.  

%Inference in a simple (non-sparse) nonparametric kernel density estimator (assuming the kernel has broad support) requires access to all of the training data.  Large, fixed size datasets can often be accommodated by either engineering strategies (parallelism, etc.)~or approximation schemes (careful selection of representative subsets of the data).  Of course, fixed size data 

\comment{
%Bayesian nonparametric models interpolate between nonparametric 

%It can be difficult to distinguish between ``number of parameters,'' model ``complexity,'' and 


Parametric and nonparametric models differ in several key ways.  One important way is that parametric models have a fixed, finite number of parameters whereas nonparametric models are often described as having an effective number of ``parameters'' (often the datapoints or a subset of the datapoints themselves) that grows as a function of the size of the training data.

%An example of the former is Gaussian mixture model.  An example of the latter is a kernel density estimator.  In the former the number of mixture components is fixed a priori and the inference goal might be, in the case of density estimation, to find parameters that maximize the probability of the training data under the model.  In the latter, kernel density estimation case, the ``parameters'' of the model are the data points themselves (and perhaps parameters for a kernel function).  Here the effective number of parameters clearly grows linearly in the size of the training data. 

Bayesian nonparametric (BNP)  models are, somewhat confusingly, described as having ``complexity'' that grows as a function of the number of training data observations \cite{griffiths}.  BNP models are characterized by infinite dimensional parameter spaces \cite{sethurman, gaussian process function space reference}.   The former perspective derives from the fact that many BNP models can be ``collapsed'' in the sense that the infinite dimensional parameter space can be marginalized out yielding a simple, finite (for any finite dataset) representation whose memory complexity grows as a function of the training data size  (Chinese restaurant process \cite{CRP}, Polya urn representation \cite{poly urn}, etc.).  



%As a concrete example consider sequential importance sampling inference for a exponential family (conjugate) Dirichlet process mixture \cite{DP particle filter work}.  a fully collapsed representation can be used in which only the counts per class and sufficient statistics for each class need to be represented.


In such schemes, instead of representing the full infinite dimensional parameter space (or a truncation of it \cite{blei variational stuff}), such a collapsed representation instead only realizes and represents that part of the infinite dimensional parameter space ``responsible'' for generating the training data.     

%This difference in description is perhaps best explained through an example.  In a model with a Dirichlet process prior like $x_i | \G \sim \G, \G|\alpha,\G_0 \sim \DP(\alpha, \G_0),\; i=1\ldots n$,  either the posterior distribution of $\G |  \{x_i\}_{i=1}^{n},\alpha,\G_0$ or the posterior predictive distribution of $x_{n+1} | \{x_i\}_{i=1}^{n},\alpha,\G_0$ may be of interest.   In the former case, since it is known that both a priori and a posteriori that $\G$ has the form $\G = \sum_{k=1}^\infty \pi_k \delta_{\phi_k}$ \cite{sethurman} it is clear that the number of parameters in the model is infinite (there are an infinite number of so-called ``sticks,'' $\pi_k$ and ``atoms'' $\phi_k$).  On the other hand, ``marginal'' posterior predictive inference can be performed in such a model with $\G$ analytically integrated out.  Such inference utilizes representations in which the ``effective'' number of parameters in the model (really the state required to represent a single exact sample) grows in expectation as a function of the number of training data observations (Polya urn and Chinese restaurant process samplers \cite{plya_urn, crp} for example).  Other BNP models exhibit similar characteristics.

There is a connection between the effective number of parameters and the storage requirement for 

While nonparametric approaches (\cite{lots of shit}) in general, and BNP approaches in particular (\cite{lots more shit}) have exhibited empirical promise in a wide variety of application domains, enthusiasm for such methods should be damped by the realization that even sub-linear growth in the effective number of parameters is extremely problematic.  This is because, for any ``life-long'' learning agent, or any inference procedure exposed to extremely large training data sets, such growth will eventually make inference prohibitively expensive.  

In the BNP literature, this issue most acutely arises in Gaussian process (GP) models, where storage and computational complexity grow quadratically and cubically in the training data size.  To apply GP models to datasets of more than a few thousand data points requires using sparse variants that all effectively choose a ``best'' subset of the training data instances \cite{snelson, csato, etc}.  



How much memory must be used to represent such a model is highly dependent on the inference approach (approximate versus exact) and the representation used (one parameter per observation, one parameter per table and assignment to tables)

%(a Gaussian process is parameterized by mean and covariance {\em functions}, a Dirichlet process by a  

%``speak for themselves''


%The task of general file compression with an algorithm requiring constant space is of obvious practical value.  In any application there will be only a finite amount of memory available and many excellent compression algorithms become unfeasible when compressing large files.  In order to address this task directly an algorithm which allows a specified maximum memory allocation prior to running is needed.  In the case of compression through the use of probabilistic models, the algorithm should control the amount of memory required by adapting the complexity of the model in an appropriate manner to accommodate the stated memory bounds.

%Probabilistic models have been shown to perform extremely well when combined with an entropy encoder in compression algorithms.  These models work by predicting the next type in a sequence of types, most often bytes, which make up a file.  The better the model is, the more efficient the encoding scheme and the higher the compression ratio.  The model referred to as a stochastic memoizer for sequence data [wood] has recently been shown to outperform several other such models for general compression tasks [Gasthaus].  Unfortunately many of these byte level prediction algorithms require space on the order of the sequence length and thus become unrealistic for very large files.

%Even for models with theoretically constant memory requirements, such as n-gram models, the memory restriction cannot be pre-specified and the theoretical value is such a gross overestimate of the likely memory requirements it does not provide insight regarding the parameterization of the model at the start.  In section 1 of this paper we will review the stochastic memoizer for sequence data [wood].
}