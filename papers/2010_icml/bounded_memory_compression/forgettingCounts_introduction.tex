\section{Introduction}
\comment{
Nonparametric models, Bayesian or not, are characterized by computational and memory complexity that grows as a function of the size of the training data.  Unfortunately in common practice the relationship between training dataset size and complexity is used to select a dataset whose size is sufficiently small to allow model estimation.  Clearly this is not ideal given a sufficiently large and complex dataset.

When the computer is fixed but the dataset is not of fixed size but instead grows monotonically, two constraints are imposed on the computational complexity of inference and estimation procedures.  First, the computational complexity of estimation and inference cannot be greater than linear in the size of the data.  Second, the memory complexity of such algorithms must be constant.  Also, particularly in the case of growing data, the only suitable estimation and inference procedures are those that are incremental in nature.
}
In this paper we define a dependent hierarchical Pitman-Yor process (HPYP) \cite{Teh2006a} and develop a constant space, linear time incremental inference procedure for models of discrete data based on it.   This contribution can be described as a ``forgetting'' procedure for existing HPYP inference procedures that retains only a ``good,'' constant-sized subset of the training data.  In designing and justifying such a model and inference procedure we extend definitions and inference methods for dependent sequences of Pitman-Yor (PY) processes \cite{Caron2007, Caron2007a} to the hierarchical case.    We propose an incremental inference procedure for the resulting model that has two interpretations: either a valid inference procedure for a sequence of dependent HPYP's or as invalid (but approximately correct) inference procedure for a single non-varying HPYP.   In the latter case, our approach can be described as restricting the estimated model at all times  to the constant size model that ``best'' approximates (in some sense of the word best) a full model that would otherwise grow in the size of the training data.   

Previous work on dependent Dirichlet and Pitman-Yor processes \cite{MacEachern2000, Srebro2005, Griffin2006, Caron2007, Caron2007a} was motivated by the desire to construct generative procedures that induce dependence between related processes.  In our work we are motivated instead by the goal of performing constant space inference in a HPYP.  As the HPYP is a (Bayesian) nonparametric model, in order to achieve this we must ``forget'' data.  Conveniently, forgetting is one of the main mechanisms for generating (and performing inference) in models of sequences of dependent processes.  This hints at why our inference procedure can be interpreted in two ways.  If the true generative process varies in some dependent way, then it is possible that the implicit dependency induced by a forgetting procedure can capture and model that variation.  If, on the other hand, the true generative process does not vary, the constant memory constraint still requires that we forget.  This means choosing an informative subset of the data to retain at the cost of using an estimator that may be inconsistent.  This strategy of selecting a most informative subset of training data is not unique in the Bayesian nonparametric modeling literature.  In sparse Gaussian process modeling, forgetting strategies are used to achieve constant time and space (independent of the number of observations) inference procedures \cite{ Lawrence2003, Csat'o2002, Snelson2006}.

% in HPYP inference, namely efficient ways to identify and represent deep HPYP models \cite{Wood2009}, incremental inference techniques for the same \cite{Gasthaus2010}, and results from 
 
Existing HPYP models have shown excellent predictive performance but are impractical to estimate for large datasets, motivating our exploration of constant space inference.  A linear space, linear time incremental estimator for the sequence memoizer (SM) \cite{Wood2009} (an HPYP model of unbounded depth for discrete sequence data) has been developed \cite{Gasthaus2010}.   A consequence of this development is that a SM could be deployed as the probabilistic sequence prediction model in a general purpose lossless compressor.   A compressor built in this way was shown to be better than other general purpose lossless compressors (including gzip, bzip2, etc.).  Unfortunately an $O(n)$ memory complexity bound (where here $n$ is the length of the sequence) makes the SM impractical for use in a compressor.  Using the SM, arbitrarily long data streams cannot be modeled on a fixed computer and therefore cannot be compressed.  The obvious approach of estimating a new model for each of a number of constant size subsequences achieves the constant memory bound but, as we will show, can consistently be improved upon.   We show that our proposed constant space HPYP model and inference procedure achieves near optimal results for reasonable (practical to implement on modern computers) memory constraints.  This suggests that it might be possible to build and deploy an improved, practical, state-of-the-art compressor based on a constant space HPYP sequence prediction engine.

In the next section we review the Pitman-Yor process, the HPYP, and the SM.   Section~\ref{sec:theory} describes dependent Pitman-Yor processes and defines a dependent hierarchical Pitman-Yor process.   Section~\ref{inference} describes a sequential Monte Carlo inference procedure for the dependent HPYP defined in Section~\ref{sec:theory}.  Finally  Section~\ref{results} compares the predictive performance of the constant memory model to more complex models whose complexity is allowed to grow.  Why the predictive performance of the constant memory model is comparable to that of more complex models for reasonable memory bounds is discussed in Section~\ref{discussion}.

%If the memory complexity of the estimated model is constant with respect to training data size then both no more than a constant number of datapoints can be retained and no more than a constant number of parameters can be used.  It seems strange then to consider constant memory inference for a nonparametric model because, fundamentally, the resulting constant memory model estimate must be parametric.

%Regardless, the Gaussian process regression literature, for one example, includes many such approaches anyway \cite{gaussian process regression stuff, lawrence, csato, snelson, etc}.  The reason for this is that inference of Gaussian process regression models requires $O(n^2)$ space and $O(n^3)$ time where $n$ is the (growing) number of observations,  hindering their widespread use.  

%Inference in a simple (non-sparse) nonparametric kernel density estimator (assuming the kernel has broad support) requires access to all of the training data.  Large, fixed size datasets can often be accommodated by either engineering strategies (parallelism, etc.)~or approximation schemes (careful selection of representative subsets of the data).  Of course, fixed size data 

\comment{
%Bayesian nonparametric models interpolate between nonparametric 

%It can be difficult to distinguish between ``number of parameters,'' model ``complexity,'' and 


Parametric and nonparametric models differ in several key ways.  One important way is that parametric models have a fixed, finite number of parameters whereas nonparametric models are often described as having an effective number of ``parameters'' (often the datapoints or a subset of the datapoints themselves) that grows as a function of the size of the training data.

%An example of the former is Gaussian mixture model.  An example of the latter is a kernel density estimator.  In the former the number of mixture components is fixed a priori and the inference goal might be, in the case of density estimation, to find parameters that maximize the probability of the training data under the model.  In the latter, kernel density estimation case, the ``parameters'' of the model are the data points themselves (and perhaps parameters for a kernel function).  Here the effective number of parameters clearly grows linearly in the size of the training data. 

Bayesian nonparametric (BNP)  models are, somewhat confusingly, described as having ``complexity'' that grows as a function of the number of training data observations \cite{griffiths}.  BNP models are characterized by infinite dimensional parameter spaces \cite{sethurman, gaussian process function space reference}.   The former perspective derives from the fact that many BNP models can be ``collapsed'' in the sense that the infinite dimensional parameter space can be marginalized out yielding a simple, finite (for any finite dataset) representation whose memory complexity grows as a function of the training data size  (Chinese restaurant process \cite{CRP}, Polya urn representation \cite{poly urn}, etc.).  



%As a concrete example consider sequential importance sampling inference for a exponential family (conjugate) Dirichlet process mixture \cite{DP particle filter work}.  a fully collapsed representation can be used in which only the counts per class and sufficient statistics for each class need to be represented.


In such schemes, instead of representing the full infinite dimensional parameter space (or a truncation of it \cite{blei variational stuff}), such a collapsed representation instead only realizes and represents that part of the infinite dimensional parameter space ``responsible'' for generating the training data.     

%This difference in description is perhaps best explained through an example.  In a model with a Dirichlet process prior like $x_i | \G \sim \G, \G|\alpha,\G_0 \sim \DP(\alpha, \G_0),\; i=1\ldots n$,  either the posterior distribution of $\G |  \{x_i\}_{i=1}^{n},\alpha,\G_0$ or the posterior predictive distribution of $x_{n+1} | \{x_i\}_{i=1}^{n},\alpha,\G_0$ may be of interest.   In the former case, since it is known that both a priori and a posteriori that $\G$ has the form $\G = \sum_{k=1}^\infty \pi_k \delta_{\phi_k}$ \cite{sethurman} it is clear that the number of parameters in the model is infinite (there are an infinite number of so-called ``sticks,'' $\pi_k$ and ``atoms'' $\phi_k$).  On the other hand, ``marginal'' posterior predictive inference can be performed in such a model with $\G$ analytically integrated out.  Such inference utilizes representations in which the ``effective'' number of parameters in the model (really the state required to represent a single exact sample) grows in expectation as a function of the number of training data observations (Polya urn and Chinese restaurant process samplers \cite{plya_urn, crp} for example).  Other BNP models exhibit similar characteristics.

There is a connection between the effective number of parameters and the storage requirement for 

While nonparametric approaches (\cite{lots of shit}) in general, and BNP approaches in particular (\cite{lots more shit}) have exhibited empirical promise in a wide variety of application domains, enthusiasm for such methods should be damped by the realization that even sub-linear growth in the effective number of parameters is extremely problematic.  This is because, for any ``life-long'' learning agent, or any inference procedure exposed to extremely large training data sets, such growth will eventually make inference prohibitively expensive.  

In the BNP literature, this issue most acutely arises in Gaussian process (GP) models, where storage and computational complexity grow quadratically and cubically in the training data size.  To apply GP models to datasets of more than a few thousand data points requires using sparse variants that all effectively choose a ``best'' subset of the training data instances \cite{snelson, csato, etc}.  



How much memory must be used to represent such a model is highly dependent on the inference approach (approximate versus exact) and the representation used (one parameter per observation, one parameter per table and assignment to tables)

%(a Gaussian process is parameterized by mean and covariance {\em functions}, a Dirichlet process by a  

%``speak for themselves''


%The task of general file compression with an algorithm requiring constant space is of obvious practical value.  In any application there will be only a finite amount of memory available and many excellent compression algorithms become unfeasible when compressing large files.  In order to address this task directly an algorithm which allows a specified maximum memory allocation prior to running is needed.  In the case of compression through the use of probabilistic models, the algorithm should control the amount of memory required by adapting the complexity of the model in an appropriate manner to accommodate the stated memory bounds.

%Probabilistic models have been shown to perform extremely well when combined with an entropy encoder in compression algorithms.  These models work by predicting the next type in a sequence of types, most often bytes, which make up a file.  The better the model is, the more efficient the encoding scheme and the higher the compression ratio.  The model referred to as a stochastic memoizer for sequence data [wood] has recently been shown to outperform several other such models for general compression tasks [Gasthaus].  Unfortunately many of these byte level prediction algorithms require space on the order of the sequence length and thus become unrealistic for very large files.

%Even for models with theoretically constant memory requirements, such as n-gram models, the memory restriction cannot be pre-specified and the theoretical value is such a gross overestimate of the likely memory requirements it does not provide insight regarding the parameterization of the model at the start.  In section 1 of this paper we will review the stochastic memoizer for sequence data [wood].
}