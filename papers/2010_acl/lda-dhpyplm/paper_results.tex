\section{Results}

We use LDA as the baseline for the performance of our model.  We calculated the average bits per word of the training corpus using the left-to-right particle filter method described in \cite{Wallach08}.  To verify the performance of our LDA implementation we tested the Psychology Review Abstract corpus using MALLET \cite{mallet}.  MALLET splits testing and training data in such a way that there may be types which occurs in the testing corpus which do not occur in the training corpus.  Our data, in contrast, was preprocessed such that all types that occur in the testing corpus must have appeared in the training corpus, otherwise they are collapsed onto an "unseen" type.  We tested both implementations with the same 100 training documents and 50 test documents.  When tested on the data set preprocessed with MALLET, both implementations achieved information rates of approximately 7.8 bits per word with 5 topics.  Using our preprocessing, our LDA implementation achieved information rates of 7.1 bits per word with 5 topics, which we attribute to the elimination of types in the testing corpus which do not appear in the training corpus.

We use the Psychology Review Abstract corpus \ref{Griffiths2005}, the same as \ref{Wallach2006}.  We couldn't reproduce her data set exactly because she chooses documents randomly from the corpus, but it should give a rough sense of how well our approaches compare.  Also, due to the time required to run the algorithm and deadline constraints, we chose to use a smaller training and testing corpus, consisting of only 30 training documents and 15 test documents, with 4010 and 1969 tokens respectively, and 508 unique types.  This leads to a significantly smaller number of bits per word, so we evaluate the relative effectiveness of the different algorithms by comparing the ratio of perplexities between LDA and each of our methods.

We approximate the conditional probability of the test corpus given the training corpus by first sampling a set of topic assignments for the training data, then sampling a label for each word in the test data given the assignments in the training data \textit{and} the assignments in the test data \textit{up to} the word being assigned.  By drawing several samples for the topic labels and averaging over the probabilities of the test data, we can perform approximate Monte Carlo integration.  We draw 10 samples for topic assignments from the training data, and for each draw from the training data we draw 10 samples for the test data.  We choose a maximum context length of two, that is, our language model is a trigram model.  The information rate of the test data for both LDA and LDA-DHPYPLM are shown in figure one.  We note that, consistently across the number of topics chosen, LDA-DHPYPLM performs about 2 bits better than straight LDA, equivalent to one quarter the perplexity.  This gap narrows somewhat for larger topics, though this may be due to the small size of the traning set, as there is a limit to how low the perplexity can go no matter how good the model.  This is superior to the performance of the Bigram Topic Model \ref{Wallach2006} for a small number of topics, though the difference between LDA and the BTM exceeds 2 bits above 40 topics.  However, the different size of the corpuses used means we cannot be sure how effective direct comparison is.

\begin{figure}
\begin{center}
\begin{tabular}{| l || c | c | c | c | c |}
\hline
\# of topics & 1 & 5 & 10 & 20 & 40 \\
\hline
LDA & 5.47 & 4.97 & 4.40 & 4.18 & 3.31\\
\hline
LDA-DHPYPLM & 3.54 & 2.78 & 2.49 & 2.17 & 1.84\\
\hline
\end{tabular}
\caption{The information rate (bits per word) of test data for Psychological Review Abstracts.}
\end{center}
\end{figure}
