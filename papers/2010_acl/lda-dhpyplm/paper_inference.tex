\section{Inference}

In the collapsed representation of  the \ourmodel we can estimate the posterior distribution of the parameters using a Gibb's sampler.  The parameters are the vector of topic assignments for each word, ${\bf z}$,  and the set of distributions in the DHPYPLM, which we denote as $\M$.  In the collapsed representation we use for sampling, all the distributions in $\M$ are integrated out, but the predictive distributions needed for sampling are recoverable. The conditional distributions needed to implement a Gibbs sampler are
%
\begin{eqnarray}
& &P(z_{ij} = t | \mathbf{z_{-ij}},\mathbf{w}, \M) \propto  \label{eqn:gibbs1}\\
& &	\hspace{.5cm} P(w_{ij}|z_{ij}=t, \M_{-w_{ij}})P(z_{ij} = t | \mathbf{z}_{-ij}, {\bf w}) \nonumber \\
& &P(\M |  {\bf z} , {\bf w}) \label{eqn:gibbs2} 
\end{eqnarray}
%
\noindent where $\M_{-w_{ij}}$ is a sample from the posterior distribution of the DHPYPLM conditioned on the corpus with the word $w_{ij}$ removed. 

In Equation~\ref{eqn:gibbs1}, $P(w_{ij}|z_{ij}=t, \M_{-w_{ij}})$ is the probability of generating $w_{ij}$ from the language model specific to domain $z_{ij}$ given the current state of the sampler.  This probability is easily calculated given a static state of the DHPYPLM.  The second term in Equation~\ref{eqn:gibbs1} is
%
\begin{eqnarray*}
P(z_{ij} = t | \mathbf{z}_{-ij}, {\bf w}) \propto \frac{n^{(d_j)}_{-ij,t} + \frac{\alpha}{T}}{n^{(d_j)}_{-ij,.} + \alpha},
\end{eqnarray*}
%
\noindent where $n^{(d_j)}_{-ij,t}$ is the number of times the topic $t$ appears in document $d_j$ excluding $w_{ij}$ and the dot indicates marginalization over topics. Equation~\ref{eqn:gibbs2} is the conditional distribution of the DHPYPLM given all the topic assignments.  Sampling from Equation~\ref{eqn:gibbs2} is exactly like the domain adaptation scenario presented in \cite{Wood2009a} and is performed in the multi-floor Chinese restaurant process representation.

\begin{comment}

The term $P(w_{ij}|z_{ij}=t, \mathbf{z}_{-ij}, \mathbf{w}_{-ij})$ in Equation~\ref{eqn:gibbssample} is the probability of generating $w_{ij}$ from the language model specific to domain $z_{ij}$ given the current state of the sampler.  This probability is easily calculated given that the GPYP is sampled in the collapsed MFCRF representation.

Where $n^{(d_j)}_{-ij,t}$ is the number of times the topic $t$ appears in document $d_j$ excluding the token $w_{ij}$, $n^{(w_{ij})}_{-ij,t}$ is the number of times the type $w_{ij}$ appears in topic $t$, excluding the token $w_{ij}$, and the dot represents marginalization (over topics or types).

Posterior inference in the DHPYPLM also takes place via Gibbs sampling.  Rather than representing each draw from a Pitman-Yor process explicitly, we track the clusters to which each observation belongs, as well as the parameter associated with each cluster.  This is a Chinese Restaurant Process representation of a nonparametric model.  Sampling consists of reseating each "customer" within a "restaurant" at a different "table," where an observation is a customer, a restaurant is a draw from a PYP, and a table is a cluster in that draw. In a hierarchical nonparametric model like this, tables in one restaurant are customers in the parent restaurant.  We also place a Gamma(1,1) prior over the concentration parameters and a uniform prior over the discount and switch parameters.  The code for the DHPYPLM was graciously provided by Frank Wood, and the exact details of Gibbs sampling can be found in \ref{Wood2009}.

To integrate the two models, we notice that the second term on the right in equation (2) is independent of our language model and remains unchanged.  The first term can be expressed as

\begin{equation}
P(w_{ij}|z_{ij}=t, \mathbf{z}_{-ij}, \mathbf{w}_{-ij}) = \int p(w_{ij}|\mathcal{G}^t_{\mathbf{u}_{ij}}) p(\mathcal{G}^t_{\mathbf{u}_{ij}}| \mathbf{w}_{-ij}, \mathbf{z}_{-ij}) d\mathcal{G}^t_{\mathbf{u}_{ij}}
\end{equation}

This integral is intractable, so instead we approximate the posterior of $\mathcal{G}^t_{\mathbf{u}_{ij}}$ with a point estimate, the state of the language model after Gibbs sampling with the current observation removed, and read out the predictive probability of the current word given a topic and context.  Thus we can construct the entire conditional probability distribution that a word is assigned to a particular topic given the other topic assignments and current state of the language model.  By iteratively sampling the topic assignments and language model, we now have a full Gibbs sampler for integrated LDA-DHPYPLM.

\end{comment}
