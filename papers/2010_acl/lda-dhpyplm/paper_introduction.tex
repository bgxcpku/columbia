% !TEX root = paper.tex
\section{Introduction}

%An image contains many categories of objects.  A song can draw influences from many genres.  And an article may cover many topics within it.  The goal of topic modeling is to extract these latent categories directly from the data, in an unsupervised fashion.  Most topic models ignore the temporal structure of data, treating individual items in a document as exchangeable.  Thus in the case of text modeling, a document is equivalent to a "bag of words."  This is a good approximation of how semantic information is distributed in a document, with associated words appearing throughout a document.  Syntactic information, on the other hand, is intimately tied to word order.  Moreover, topic identities that are ambiguous without context become clear when context is known: the word "states" in "The United States" clearly falls into a different topic than in "altered mental states."  

Generative models for natural language text come in many flavors.  Amongst the most widely used and empirically successful are those that  generate the next word in a sequence from conditional probability tables indexed by a recent subsequence of the preceding words.  This class of generative models includes $n$-gram models  and smoothing variants thereof \cite{Kneser1995,MacKay1995a,Chen1998}, including recent Bayesian nonparametric approaches to the same \cite{Teh2006a,Wood2009}.  

The continuing success of $n$-gram modeling approaches can be attributed to the flexibility and expressiveness of such models, however, few would argue that such methods correspond in any intuitive or reasonable way to how generation might actually occur.   For instance there is no explicit notion of topic or grammaticality.  Both are simply subsumed by the $n$-gram modeling assumption that all regularity and typicality (grammatical, topic, and otherwise) is captured by local word co-ocurrence patterns.

It can be argued models that capture more aspects of the true generative procedure should also do a better job of capturing and modeling typicality.  For instance $n$-gram models are good at representing typicality, but obviously lack parameters and structure that would explicitly encode grammatical rules and the evolution of topics.  
%Sampled sequences from $n$-gram models, both a priori and a posteriori, are not really similar to natural language.  Such sequences are always missing at least two things.  First, the sequences will almost certainly not conform to notions of grammaticality.  Second, there will be little or no topic continuity.  
Somehow including either or both would undoubtedly result in improvements in both the generative sense and in the model's ability to describe typicality.

%We note here that nominally the ability to distinguish typicality improves as the fidelity of the generative model improves.

%There are, of course, other approaches to language modeling.  Tree-based (or parsing) approaches \cite{Roark2001,Collins2003} to language modeling form a compelling alternative class of generative models.   Regardless of whether or not one believes in the innateness of grammar, tree-based generative models, particularly grammars that have been lexicalized in some way, are compelling in that productions from them are at least usually grammatically sensible.  Unfortunately though, like for $n$-gram models, sequences samples from such models (a priori or a posteriori) also are typically incoherent as they lack even local topic coherence.

%These too have their own  however grammar and parsing based approaches have been partially described in a generative way \cite{Johnson2007,Liang2007}.  Less well known are \cite{Mnih2007,Mnih2009}

In this paper we present a new way of combining a Bayesian smoothing $n$-gram language model with a topic model.  In particular we combine Latent Dirichlet Allocation \ref{Blei2003} and the Doubly-Hierarchical Pitman-Yor Process Language Model \ref{Wood2009}.   an $n$-gram language model which efficiently shares statistical strength between different domains, where in this case a topic constitutes a single domain.  Previous efforts to integrate topic modeling with word order include \ref{Griffiths2005} and \ref{Wallach2006}.  The approach taken by \ref{Griffiths2005} did not allow individual words to play both a semantic and syntactic role.  The approach of \ref{Wallach2006} is similar in spirit to our own, however she uses the hierarchical Dirichlet language model \ref{Mackay1995} which has since been superseded by Bayesian language models with superior performance.  We hope that the use of state-of-the-art language models in topic modeling will lead to both better predictive power and will discover topics that more accurately reflect semantic categories.
