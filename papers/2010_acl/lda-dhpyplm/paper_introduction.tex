\section{Introduction}

An image contains many categories of objects.  A song can draw influences from many genres.  And an article may cover many topics within it.  The goal of topic modeling is to extract these latent categories directly from the data, in an unsupervised fashion.  Most topic models ignore the temporal structure of data, treating individual items in a document as exchangeable.  Thus in the case of text modeling, a document is equivalent to a "bag of words."  This is a good approximation of how semantic information is distributed in a document, with associated words appearing throughout a document.  Syntactic information, on the other hand, is intimately tied to word order.  Moreover, topic identities that are ambiguous without context become clear when context is known: the word "states" in "The United States" clearly falls into a different topic than in "altered mental states."  

In this paper we present an integrated probabilistic topic-language model that integrates Latent Dirichlet Allocation \ref{Blei2003}, one of the most popular Bayesian topic models, with the Doubly-Hierarchical Pitman-Yor Process Language Model \ref{Wood2009}, an $n$-gram language model which efficiently shares statistical strength between different domains, where in this case a topic constitutes a single domain.  Previous efforts to integrate topic modeling with word order include \ref{Griffiths2005} and \ref{Wallach2006}.  The approach taken by \ref{Griffiths2005} did not allow individual words to play both a semantic and syntactic role.  The approach of \ref{Wallach2006} is similar in spirit to our own, however she uses the hierarchical Dirichlet language model \ref{Mackay1995} which has since been superseded by Bayesian language models with superior performance.  We hope that the use of state-of-the-art language models in topic modeling will lead to both better predictive power and will discover topics that more accurately reflect semantic categories.
