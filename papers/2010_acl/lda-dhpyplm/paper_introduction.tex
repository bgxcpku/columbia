% !TEX root = ContextualTopicModel.tex
\section{Introduction}

%An image contains many categories of objects.  A song can draw influences from many genres.  And an article may cover many topics within it.  The goal of topic modeling is to extract these latent categories directly from the data, in an unsupervised fashion.  Most topic models ignore the temporal structure of data, treating individual items in a document as exchangeable.  Thus in the case of text modeling, a document is equivalent to a "bag of words."  This is a good approximation of how semantic information is distributed in a document, with associated words appearing throughout a document.  Syntactic information, on the other hand, is intimately tied to word order.  Moreover, topic identities that are ambiguous without context become clear when context is known: the word "states" in "The United States" clearly falls into a different topic than in "altered mental states."  

Generative models for natural language text come in many flavors.  Amongst the most widely used and empirically successful are those that  generate the next word in a sequence from conditional probability tables indexed by a recent subsequence of the preceding words.  This class of generative models includes $n$-gram models  and smoothing variants thereof \cite{Kneser1995,MacKay1995,Chen1998}, including recent Bayesian nonparametric approaches to the same \cite{Teh2006a,Wood2009}.  

The continuing success of $n$-gram modeling approaches can be attributed to the flexibility and expressivity of such models. %, however, few would argue that $n$-gram models correspond structurally to any naturally plausible generative scheme.   For one, 
Unfortunately $n$-gram models lack any latent process that might conditionally influence production.  For instance there is no explicit notion of latent semantic classes for words nor is there a latent topic process.  Every such latent process that one might propose is simply subsumed by the $n$-gram modeling assumption that all regularity and typicality in natural language (grammatical, topic, and otherwise) is captured by local word co-ocurrence patterns.  

%It can be argued models that capture more aspects of the true generative procedure should also do a better job of capturing and modeling typicality.  For instance $n$-gram models are good at representing typicality, but obviously lack parameters and structure that would explicitly encode grammatical rules and the evolution of topics.  
%Sampled sequences from $n$-gram models, both a priori and a posteriori, are not really similar to natural language.  Such sequences are always missing at least two things.  First, the sequences will almost certainly not conform to notions of grammaticality.  Second, there will be little or no topic continuity.  
%Somehow including either or both would undoubtedly result in improvements in both the generative sense and in the model's ability to describe typicality.

%We note here that nominally the ability to distinguish typicality improves as the fidelity of the generative model improves.

%There are, of course, many other approaches to language modeling, some of which attempt to address some of these shortcomings.  Tree-based (or parsing) approaches \cite{Roark2001,Collins2003,Johnson2007,Liang2007} to language modeling form a compelling, if computationally expensive, alternative class of generative models.   %Regardless of whether or not one believes in the innateness of grammar, tree-based generative models, particularly grammars that have been lexicalized in some way, are compelling in that productions from them are at least usually grammatically sensible.  Unfortunately though, like for $n$-gram models, sequences sampled from such models (a priori or a posteriori) also are typically incoherent (just in a different way than the way they are incoherent for $n$-gram models) as they lack even local (intra-sentential) topic coherence.  
%Less well known are neural network approaches that make context dependent continuation predictions from large contexts \cite{Mnih2007,Mnih2009}.

%These too have their own  however grammar and parsing based approaches have been partially described in a generative way \cite{Johnson2007,Liang2007}.  Less well known are \cite{Mnih2007,Mnih2009}


In this paper we present a novel natural language model that consists of the combination of a hierarchical nonparametric Bayesian smoothing $n$-gram language model with a latent Dirichlet allocation (LDA) topic model \cite{Blei2003a}.  The point of doing this is to augment the language model with a latent generative process which will capture and represent longer-range dependencies than the $n$-gram model could on its own.  The latent domain (or topic) process can either be seen as way of improving $n$-gram language modeling performance or as a way of avoiding the bag of words assumption assumption typical to LDA topic models (and correspondingly improving topic modeling performance as well).  

The novelty of this work is not in having combined two such models; similar, less expressive combinations have been considered before \cite{Wallach2006}.  Instead, the novelty and contribution here is in the particular choice for each model and in the development of an inference algorithm for the pair.  In prior work, the specific choice of  $n$-gram language model and the chosen method of tying together the language models associated with each topic led to complex estimation procedures and left open the possibility of overfitting.  The fully Bayesian model that we propose there avoids these problems.   Estimation is straightforward and the risk of overfitting is minimal.   This is attributable to our choice of a domain adapting language model that shares statistical strength between smoothing $n$-gram language models in a hierarchical Bayesian way \cite{Wood2009}.   

We review both  the doubly hierarchical language model (DHPYPLM) \cite{Wood2009a} and LDA topic models.  We show that the combination of the two leads to improvements in language modeling relative to both the individual models themselves and in comparison to similar prior art.
% by introducing document specific topic distributions and per word latent topic indicator variables, in other words introducing a latent topic process.

%In particular we combine Latent Dirichlet Allocation \cite{Blei2003a} and the Doubly-Hierarchical Pitman-Yor Process Language Model \cite{Wood2009}.   an $n$-gram language model which efficiently shares statistical strength between different domains, where in this case a topic constitutes a single domain.  Previous efforts to integrate topic modeling with word order include \cite{Griffiths2005} and \cite{Wallach2006}.  The approach taken by \cite{Griffiths2005} did not allow individual words to play both a semantic and syntactic role.  The approach of \cite{Wallach2006} is similar in spirit to our own, however she uses the hierarchical Dirichlet language model \cite{Mackay1995} which has since been superseded by Bayesian language models with superior performance.  We hope that the use of state-of-the-art language models in topic modeling will lead to both better predictive power and will discover topics that more accurately reflect semantic categories.
