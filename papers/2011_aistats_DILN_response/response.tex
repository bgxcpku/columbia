\documentclass[twoside]{article}
\usepackage[accepted]{aistats2e}
\usepackage{natbib}
\usepackage{amsbsy}

% If your paper is accepted, change the options for the package
% aistats2e as follows:
%
%\usepackage[accepted]{aistats2e}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Discussion of ``The Discrete Infinite Logistic Normal Distribution for Mixed-Membership Modeling''}

\aistatsauthor{ Frank Wood }

\aistatsaddress{ Columbia University } ]
\makeatletter
\@copyrightspace
\makeatother
%\begin{abstract}

%\end{abstract}
\section{Introduction}

%\vspace{-.45in}
Mixed-membership models (e.g.~``topic models'') are inarguably popular; especially latent Dirichlet allocation (LDA) \citep{Blei2003} and its variants.  Such models have become a fundamental tool in the analysis and exploration of many types of data.  Originally designed to model text documents as per-word draws from a document-specific weighting of a finite collection of ``topics'' (distributions over words), mixed-membership models now are applied very broadly.  Example usage includes applications in information retrieval, image processing, audio classification, and more.   Because of the wide applicability of mixed-membership modeling, improving models of this type has the potential to have significant impact. The discrete infinite logistic normal distribution (DILN) for mixed-membership modeling is a significant advance in mixed-membership modeling.

A key to the success of mixed-membership models is simplicity.  They are easy to describe mathematically and easy to explain informally.  The latent variables defined by such models are often readily interpretable by lay practitioners and often are visibly fascinating.

This kind of simplicity can also be an Achilles heel of sorts.  To keep such models relatively simple, unreasonable assumptions about the nature of the true generative process must be made.  This kind of trade-off is very common when designing or choosing a statistical model.  The trick to designing good new models  is to do so in such a way as to address the true shortcomings of the models being built upon while retaining desirable traits like interpretability, simplicity, and elegance. DILN is arguably such a contribution to mixed-membership modeling.  DILN directly addresses one of the more pressing problems in mixed-membership modeling, namely, how to model correlations between latent features. % and does so in a way that is mathematically elegant.

%\section{Background}

Mixed membership models are characterized by grouped observations (think of a bag-of-words representation of a document or a bag-of-features representation of an image) generated by a mixture of latent distributions over the observation space (``features'').  In the canonical document modeling example a feature (``topic'') is a distribution over words and a document is a collection of draws from a per-document mixture of topics.  The fact that certain words tend to co-occur in the same document can be used to infer both what topics occur in a corpus and what proportions of each topic are found in each document.  

Original work on mixed-membership modeling  (notably LDA for document modeling) assumed both a fixed and finite number of statistically independent topics.   In other words, the presence of one topic in a document was nearly independent of the presence of other topics, and the number of topics was fixed a priori to a pre-determined finite value.  A great deal of subsequent work went into defining topic models of unbounded topic cardinality including, notably, the hierarchical Dirchlet process (HDP) as applied to mixed-membership modeling (HDP-LDA) \citep{Teh2006b}.   Realistically though, specifying a mixed-membership model with a large number of latent features results in a model that is very nearly like that of a model with an unbounded number of features---Bayesian priors encourage sharing and discourage overfitting whether the number of topics is unbounded or large and fixed. 

 Less work has gone into learning mixed-membership models in which the latent factors are correlated (e.g. work by \citep{Blei2006b,Li2007,doshivelez2009,rai2009}) though arguably this is the more important and less easy to remedy shortcoming of first generation mixed-membership models.   To illustrate what is meant by correlations between latent factors we turn to an illuminating visual scene modeling example from the introduction to \citep{doshivelez2009}.  When modeling a visual scene using a mixed-membership model we can think of an ``image'' as being a collection of observations of a world containing some number of latent features.  Latent features here can be thought of as being in correspondence with physical objects in the world like lamps, desks, elephants, and so forth.     Clearly a model that accounts for correlations in the presence and/or absence of features (desks and lamps often occur together, chairs and elephants less so) should outperform one that does not.  A similar story can be told when modeling document collections using a topic representation, namely that some topics tend to occur together.  DILN is able to account for correlations between feature presence rates for mixed-membership  models of ``bag-of-words'' (order-invariant, discrete) grouped observations.

\section{Key Idea}

Conceptually, the key idea behind DILN is to imbue each latent feature (topic) with an arbitrary, free, latent ``location'' vector.  The purpose for doing this is not to place each feature at some point in space, but instead is to let these vectors self-configure in way such that ``close'' features tend to co-occur more often than those that are ``far'' apart.  This latent space embedding is similar to ideas that have found purchase in other domains such as models of data generated on graphs (e.g.~\cite{hoff2002}).  Closeness in this space is computed using a distance (kernel) function that can be specified as part of the model definition, or, as is the case in this paper, learned implicitly.
 %s The specific mechanism used to perform this embedding associates each topic probability distribution over words with a free-to-vary $d$-dimensional vector.     

%\section{Details}

%At first glance DILN might seem somewhat intimidating.   The reality is that DILN works out to be remarkably elegant mathematically, is simple to understand given familiarity with the HDP, and apparently not especially more difficult to do inference in than HDP-LDA.   DILN is reposed upon a novel gamma process (perhaps best explained by \citep{rao2009}) representation of the HDP-LDA and a Gaussian process \citep{Rasmussen2006} latent space model.  

%As DILN is closely related to the HDP it is worth quickly reviewing the HDP version of LDA here.  Top-down HDP-LDA generation of a collection of documents starts by generating an  weighted infinite cardinality set of distributions (probability vectors, here global ``topics'') over a fixed, countable vocabulary from a Dirichlet process (DP) with a Dirichlet distribution base measure.  It is well known that draws from a DP are atomic measures meaning that they consist of an infinite collection of weighted atoms drawn iid from the DP's base measure.  A per-document distribution over topics is generating using from Dirichlet process whose base distribution is this atomic measure over probability vectors.  This effectively re-weights the probability vectors for each document
%Call this generated distribution $G(\cdot) = \sum_{k=1}^{\infty}w_k \delta_{{\boldsymbol{\pi}_k}}(\cdot)$ where $w_k$ is a weight and ${\boldsymbol{\pi}}_k = [\pi^1_k, \ldots \pi^V_k]$ and $\delta_{{\boldsymbol{\pi}_k}}(\cdot)$ is shorthand for an indicator function that returns one if its argument is ${\boldsymbol{\pi}_k}$

%The 

\section{Questions and Future Work}

While DILN is apparently able to represent correlations between topics, the specific mechanism by which it accomplishes this has what might be in some applications undesirable characteristics.  In particular, the topic prevalence covariance given by Eqn.~12 suggests that marginally prevalent topics are likely to more strongly co-vary with other marginally prevalent topics than with marginally less prevalent topics.  It seems strange that the covariance of two topics should be coupled to their marginal prevalences.  Additionally, the same covariance equation suggests that strong positive correlations are easy to achieve but strong negative correlations are hard to achieve.  It would seem to be desirable to both be able to easily represent strong positive and strong negative correlations.   

Despite the elegance of the combination of a Gamma process construction of the HDP and the latent space Gaussian process, the specific representations used in inference and the specific choice of mean-field variational inference algorithm for a truncated (finite topic cardinality) model suggest room for future improvement.  That the GP kernel matrix is explicitly represented is interesting, in that no specific choice of kernel function must be made, but doing so directly precludes straightforward scaling to variational inference with large topic cardinality truncations.  Likewise, sampling in a non-truncated representation would be computationally infeasible in the model as currently described.

More generally, representing correlations between latent features through distance in kernel space rather than explicitly through, for instance, more levels in a hierarchical probabilistic model, precludes sophisticated inference about latent feature similarities (no similarities between latent features beyond pairwise can be represented by this model).  This is a probable direction for future work.

%On the topic of inference, the algorithm provided is a mean field variational inference algorithm.  Theoretically it should be possible to find incomplete factorizations which can explicitly represent more interesting posterior covariances.  
%Different kernel functions might given rise to different similarities between latent vectors.


\section{Conclusion}

Accounting for covariance between high-level latent features is difficult but central in the development of next-generation latent variable models.  DILN is a significant step in this direction, topic modeling is an excellent framework to ``play with'' the kind of ideas DILN embodies, and empirical evidence suggests the DILN approach to topic modeling is fruitful.



%\subsubsection*{References}

\subsection{Acknowledgements}
Thanks to S. A. Williamson for helpful comments and suggestions.

\begin{small}
\bibliographystyle{plainnat}
\bibliography{../uber} 
%\input{modrefs}
\end{small}

\end{document}
