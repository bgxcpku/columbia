% !TEX root = deplump.tex
\section{Introduction}
\label{sec:introduction}

Deplump \citep{Gasthaus2010} is a general purpose, streaming lossless compressor based on a probabilistic model of discrete sequences called the sequence memoizer \citep{Wood2009}.   \citeauthor{Gasthaus2010} showed that although deplump is algorithmically similar to the PPM and CTW compression algorithms, particularly their unbounded context lengths variants \citep{Cleary1997,Willems1998}, the coherent probabilistic model underlying deplump gives it a consistent empirical advantage.  In particular, \citeauthor{Gasthaus2010} showed that deplump generally outperformed CTW  \citep{Willems2009}, PPMZ \citep{Peltola2002}, and PPM* \citep{Cleary1997} on the large Calgary corpus, the Canterbury corpus, Wikipedia, and Chinese text.  Deplump was shown to underperform in comparison to the PAQ family of compressors \citep{Mahoney2005}, but the point was made that deplump (or more specifically the sequence memoizer) could replace one or all of the mixed, finite-order Markov-model predictors included in PAQ.  

When introduced in \citep{Gasthaus2010}, deplump was reposed on a sequence memoizer whose space complexity was linear in the length of the input stream, rendering deplump inappropriate for stream compression.  Since then, two complimentary approximations to inference in the sequence memoizer have emerged that, when combined as they are in this paper, together result in a constant space sequence memoizer. To review: the sequence memoizer is an incremental method for hierarchically smoothing conditional distribution estimates when the distributions are related to each other in a tree-shaped graphical model.  The space complexity of the sequence memoizer is a function of the number of instantiated nodes in the corresponding graphical model and the storage required at each node. \citeauthor{Bartlett2010} introduced an approximation to the sequence memoizer in which the number of nodes in the tree remains of asymptotically constant order \citep{Bartlett2010}.  Unfortunately, in that work the storage requirement at each node grew as an uncharacterized but not-constant function of the input sequence length.  \citeauthor{Gasthaus2011} later introduced a method for constraining the memory used at each node in the tree to be of constant order  \citep{Gasthaus2011} but did so in a way that requires the computational cost of inference to grow as a super-linear function of the length of the training sequence.

The primary contribution of this work is to marry these two approximations such that constant memory, linear time approximate inference in the sequence memoizer is achieved, thereby rendering deplump appropriate for streaming lossless compression applications.  In addition to simply combining these two approximations, a third and final As the result of this combination is a heretofore unexplored approximation to inference in the sequence memoizer, we include a significant amount of experimentation exploring the parameter space of the approximations.  Additionally, as the resulting compression algorithm is quite complex, we have included a comprehensive algorithmic description of deplump in this paper.  This accompanies a reference implementation which can be explored at \texttt{http://www.deplump.com/}.