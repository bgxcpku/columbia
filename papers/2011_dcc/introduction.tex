% !TEX root = deplump.tex
\section{Introduction}
\label{sec:introduction}

Deplump \citep{Gasthaus2010} is a general purpose, streaming lossless compressor based on a probabilistic model of discrete sequences called the sequence memoizer \citep{Wood2009}.   \citeauthor{Gasthaus2010} showed that although deplump is algorithmically similar to the PPM and CTW compression algorithms, particularly their unbounded context lengths variants \citep{Cleary1997,Willems1998}, the coherent probabilistic model underlying deplump gives it a consistent empirical advantage.  In particular, \citeauthor{Gasthaus2010} showed that deplump generally outperformed CTW  \citep{Willems2009}, PPMZ \citep{Peltola2002}, and PPM* \citep{Cleary1997} on the large Calgary corpus, the Canterbury corpus, Wikipedia, and Chinese text.  Deplump was shown to underperform in comparison to the PAQ family of compressors \citep{Mahoney2005}, but the point was made that deplump (or more specifically the sequence memoizer) could replace one or all of the mixed, finite-order Markov-model predictors included in PAQ.  

When introduced in \citep{Gasthaus2010}, deplump was reposed on a sequence memoizer whose space complexity was linear in the length of the input stream, rendering deplump inappropriate for stream compression.  Since then, two complimentary approximations to inference in the sequence memoizer have emerged that, when combined as they are in this paper, together result in a constant space sequence memoizer and thus a stream-capable deplump compressor. To review: the sequence memoizer is an incremental method for estimating the all conditional distributions in an $n$-gram model in the limit of $n\rightarrow\infty$.  The space complexity of the sequence memoizer is a function of the number of instantiated nodes in the corresponding suffix-tree-shaped graphical model and the storage required at each node. \citeauthor{Bartlett2010} introduced an approximation to the sequence memoizer in which the number of nodes in the tree remains of asymptotically constant order \citep{Bartlett2010}.  Unfortunately, in that work the storage requirement at each node grew as an uncharacterized but not-constant function of the input sequence length.  \citeauthor{Gasthaus2011} later introduced a method for constraining the memory used at each node in the tree to be of constant order  \citep{Gasthaus2011} but did so in a way that requires the computational cost of inference to grow as a super-linear function of the length of the training sequence.

The primary contribution of this work is to marry these two approximations such that constant memory, linear time approximate inference in the sequence memoizer is achieved, thereby rendering deplump appropriate for streaming lossless compression applications.  In addition to combining these two approximations, a third and final approximation is introduced to constrain the computation required by the approximation introduced in  \citep{Gasthaus2011}. As the resulting sequence memoizer approximation is difficult to mathematically characterize, we include significant experimental exploration of the approximation parameter space.  Additionally, as the resulting deplump compression algorithm is of fairly high implementation complexity, we have included a comprehensive algorithmic description of it in this paper.  This accompanies a reference implementation which can be explored at \texttt{http://www.deplump.com/}.

Despite the approximations required to achieve algorithmic asymptotic complexity appropriate for streaming compression, we find that our constant-space deplump compressor performs as well as the original described in \citep{Gasthaus2010}.  Furthermore, as a somewhat unexpected consequence of being able to expose the underlying probabilistic model to more data, we find evidence that the constant-space deplump compressor achieves extremely good long-run streaming compression performance.