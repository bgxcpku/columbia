% !TEX root = deplump.tex
\section{Methodology}
\label{section:methodology}
\newcommand{\PY}{\ensuremath{\mathcal{P}\mathcal{Y}}}

In this section we first review the sequence memoizer model and the inference algorithm used in the batch deplump compressor. We discuss an approximation to inference which allows the model to have a constant space upper bound.  Finally, we introduce a new approximation in the inference procedure to ensure that the computational complexity of the algorithm is a linear function of the length of the input sequence.

\subsection{Review}

Note that a distribution $P$ over sequences can be factorized as $P(S = [s_0, s_1, \ldots, s_m]) = P(s_0)P_{[s_0]}(s_1)P_{[s_0,s_1]}(s_2) \ldots P_{[s_0,s_1,\ldots,s_{m-1}]}(s_m)$, where $P_{\bf u}(s) = P(s | {\bf u})$.  The sequence memoizer \citep{Wood2009} jointly models these conditional distributions using a hierarchical Bayesian framework.  The hierarchy used to define the model makes use of the Pitman-Yor \citep{Pitman1997} distribution over distributions. If $P \sim \PY(d,c,G)$ then we say $P$ follows a Pitman-Yor distribution with discount parameter $d$, concentration parameter $c$, and mean distribution $G$.  The mean distribution $G$ can be understood as the average $P$ from this distribution, i.e. $\mathbb{E}(P(\sigma)) = G(\sigma)$ for $\sigma \in \Sigma$.  The discount parameter is a real value between 0 and 1.  If the discount is close to 1 it is an indication that $P$ is likely to be very close to $G$; d close to 0 indicates $P$ may vary significantly from $G$.  In the sequence memoizer the concentration parameters are restricted to 0 and will be omitted from the remaining discussion. If we define the suffix operator $\sigma$ on discrete sequences as $\sigma([s_0, s_1, \ldots, s_m]) = [s_1,s_2, \ldots, s_m])$ then we can write the sequence memoizer model as 
%
\[
\begin{array}{rcl}
	P_{[ ]} 	|	\delta_0 					& \sim & \PY(\delta_0,0,\mathcal{U}_{\Sigma})\\
	P_{\bf u} 	| 	\delta_{|{\bf u}|}, P_{\sigma({\bf u})} 	& \sim & \PY(\delta_{|{\bf u}|}, 0, P_{\sigma({\bf u})}) \\
	s_n 		|  	P_{[s_0, s_1, \ldots, s_{n-1}] }	& \sim & P_{[s_0, s_1, \ldots, s_{n-1}] }
\end{array}
\]
%
\noindent where $\mathcal{U}_{\Sigma}$ is the uniform distribution over $\Sigma$.  The hierarchy used in the model smooths each conditional distribution $P_{\bf u}$ towards a related, more general distribution $P_{\sigma({\bf u})}$.  Intuitively the hierarchy indicates that the most recent context is the most informative for modeling the conditional distributions.

Given a model and observed data,  the task of inference is to learn likely values of the latent parameters.  The latent parameters of the sequence memoizer are  $\mathcal{G} = \{P_{\bf u} \ | \ {\bf u} \in \Sigma^{+} \}$ and $\delta_n$ for $n \geq 0$.  While $| \mathcal{G}| = \infty$, \citep{Wood2009} show that inference only requires computation on a set $\mathcal{H} \subset \mathcal{G}$ for any finite training sequence $\mathcal{S}$ such that $|\mathcal{H} | \leq 2 |\mathcal{S}|$.  Furthermore, they parameterize the discounts such that $\delta_n$ is a function of $\{\delta_0, \delta_1, \ldots, \delta_{10} \}$ for $n \geq 0$.   Unfortunately the linear bound on $|\mathcal{H}|$ makes using the model intractable for streaming sequences. \citep{Bartlett2010} and \citep{Gasthaus2010} demonstrate that approximate inference using a single particle filter and a pruning scheme yields results comparable to the full model while maintaining a constant bound on$| \mathcal{H}| $ for arbitrary length sequences.  Finally, \cite{Gasthaus2011} show that computation on $\mathcal{H}$ requires maintaining at most most $2|\Sigma| |\mathcal{H}|$ unique counts.  If these techniques are combined the result is an inference algorithm with a fixed spacial complexity for the parameters $P \in \mathcal{H}$.  

%Inference in the model is performed using a compressed representation in which only two counts for each symbol need to be maintained for a subset of the conditional distributions.  We will refer to this subset at element $t$ of the sequence as $\mathcal{H}$.  The cardinality of $\mathcal{H}$ grows linearly in the length of the sequence.  Bayesian inference is typically performed by averaging over the posterior distributions of parameters, but \cite{Gasthaus2010} demonstrate that using a single particle approximation yields excellent empirical results. \cite{Bartlett2010}  show that by making some independence assumptions the approximation can be extended and the cardinality of $\mathcal{H}$ can be bounded from above to provide a constant space inference procedure without much loss in  empirical performance.
%
\subsection{Approximation}

We need to introduce two more approximations to render the algorithm appropriate for streams.  First, the data structure used by the inference algorithms mentioned depends on an underlying reference sequence which grows as a linear function of the input sequence length.  As the algorithm progresses we maintain a fixed upper bound on the length of this sequence by pruning the sections which are most distant from the current predictive context (see Algorithm~\ref{alg:pmfnextsymbol}).  This means we makes the data structure underlying streaming deplump an approximation to the data structures used in the published inference algorithms, but means our algorithm maintains a constant space complexity.

The inference procedure requires maintaining counts $c^P_\sigma$ and $t^P_\sigma$ for each $P_{\bf u} \in \mathcal{H}$ and $\sigma \in \Sigma$.  For each symbol $s$ in the input sequence, $c_{s}$ is incremented in at least one node on the tree.  From this fact it can be seen that $\max_{P \in \mathcal{H}, \sigma \in \Sigma} \{ c^{P}_{\sigma} \}$ grows monotonically as a function of the length of the input sequence. Unfortunately incremental construction of the model includes an operation on node elements known as fragmentation \citep{Wood2009} and to fragment a node corresponding to $P_{\bf u}$ requires computation proportional to $\max_{\sigma \in \Sigma} \{ c_{\sigma}^{P_{\bf u}} \}$ \citep{Gasthaus2011}. Therefore, to make the algorithm computationally tractable with arbitrary length sequences there must be an upper bound on $c^P_\sigma$.  

The counts $c_{\sigma}$ and $t_{\sigma}$ are constrained such that $0 \leq t_{\sigma} \leq c_{\sigma} $. The $c_{\sigma}$ are counts of atoms in the estimation of a discrete distribution over $\Sigma$.  The $t_{\sigma}$ regularize the estimation and create smoothing towards the distribution at a more general context.  Intuitively, limiting $\max_{P \in \mathcal{H}} \{ c^{P} =  \sum_{\sigma \in \Sigma} c^{P}_{\sigma} \} < k$  has the effect of using at most $k$ observations to estimate each discrete distribution in $\mathcal{H}$.  If $k$ is set high relative to $|\Sigma|$ this approximation should have little effect on the estimated distribution. Using a fixed $k$ for all $P \in \mathcal{H}$ forces the computational complexity of the algorithm to be a linear function of the input sequence while maintaining constant space complexity.







