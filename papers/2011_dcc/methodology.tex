% !TEX root = deplump.tex
\section{Methodology}
\label{section:methodology}
\newcommand{\PY}{\ensuremath{\mathcal{P}\mathcal{Y}}}

\subsection{Review}

Note that a distribution $P$ over sequences can be factorized as $P(S = [s_0, s_1, \ldots, s_m]) = P(s_0)P_{[s_0]}(s_1)P_{[s_0,s_1]}(s_2) \ldots P_{[s_0,s_1,\ldots,s_{m-1}]}(s_m)$, where $P_u(s) = P(s | u)$.  The sequence memoizer jointly models these conditional distributions using a hierarchical Bayesian framework.  The hierarchy used to define the model makes use of the Pitman-Yor \citep{Pitman1997} distribution over distributions. If $P \sim \PY(d,c,G)$ then we say $P$ follows a Pitman-Yor distribution with discount parameter $d$, concentration parameter $c$, and mean distribution $G$.  The mean distribution $G$ can be understood as the average $P$ from this distribution, i.e. $\mathbb{E}(P(\sigma)) = G(\sigma)$ for $\sigma \in \Sigma$.  The discount parameter is a real value between 0 and 1.  If the discount is close to 1 it is an indication that $P$ is likely to be very close to $G$; d close to 0 indicates $P$ may vary significantly from $G$.  In the sequence memoizer the concentration parameters are restricted to 0 and will be omitted from the remaining discussion. If we define the operator $\sigma$ on discrete sequences as $\sigma([s_0, s_1, \ldots, s_m]) = [s_1,s_2, \ldots, s_m])$ then we can write the sequence memoizer model as 
%
\[
\begin{array}{rcl}
	P_{[ ]} 	|	\delta_0 					& \sim & \PY(\delta_0,0,\mathcal{U}_{\Sigma})\\
	P_{u} 	| 	\delta_{|u|}, P_{\sigma(u)} 	& \sim & \PY(\delta_{|u|}, 0, P_{\sigma(u)}) \\
	s_n 		|  	P_{[s_0, s_1, \ldots, s_{n-1}] }	& \sim & P_{[s_0, s_1, \ldots, s_{n-1}] }
\end{array}
\]
%
\noindent where $\mathcal{U}_{\Sigma}$ is the uniform distribution over $\Sigma$.  The hierarchy used in the model smooths each conditional distribution $P_u$ towards a related, more general distribution $P_{\sigma(u)}$.  Intuitively the hierarchy indicates that the most recent context is the most informative for modeling the conditional distributions.

Given a model and observed data,  the task of inference is to learn likely values of the latent parameters.  The latent parameters of the sequence memoizer are  $\mathcal{G} = \{P_u \ | \ u \in \Sigma^{+} \}$ and $\delta_n$ for $n \geq 0$.  While $| \mathcal{G}| = \infty$, \cite{Wood2009} show that inference can be restricted to a set $\mathcal{H_\mathcal{S}} \subset \mathcal{G}$ for the training sequence $\mathcal{S}$ such that $|\mathcal{H}_\mathcal{S} | \leq 2 |\mathcal{S}|$.  Furthermore, they parameterize the discounts such that $\delta_n$ is a function of $\{\delta_0, \delta_1, \ldots, \delta_{10} \}$ for $n \geq 0$.   Unfortunately the linear bound on $|\mathcal{H}_\mathcal{S}$ makes using the model intractable for arbitrary length sequences.  Using the results of \cite{Bartlett2010}, \cite{Gasthaus2010} it is known that an approximate inference scheme using a single particle filter and a pruning scheme yields comparable empirical results while $| \mathcal{H}_\mathcal{S}| < C$ for a chosen $C$ and all $S \in \Sigma^{+}$.  Finally, \cite{Gasthaus2011} show that inference of $\mathcal{H}$ requires maintaining at most most $2|\Sigma| |\mathcal{H}|$ unique counts.  These results combined together illustrate an inference technique which has a constant memory upper bound.

%Inference in the model is performed using a compressed representation in which only two counts for each symbol need to be maintained for a subset of the conditional distributions.  We will refer to this subset at element $t$ of the sequence as $\mathcal{H}$.  The cardinality of $\mathcal{H}$ grows linearly in the length of the sequence.  Bayesian inference is typically performed by averaging over the posterior distributions of parameters, but \cite{Gasthaus2010} demonstrate that using a single particle approximation yields excellent empirical results. \cite{Bartlett2010}  show that by making some independence assumptions the approximation can be extended and the cardinality of $\mathcal{H}$ can be bounded from above to provide a constant space inference procedure without much loss in  empirical performance.
%
\subsection{Approximation}

The inference procedure requires maintaining counts $c^P_\sigma,t^P_\sigma$ for each $P_u \in \mathcal{H}$ and $i \in \Sigma$.  The counts $c_{\sigma_i}$ grow as a sub-linear function of the stream length, but are not bounded above.  Unfortunately the computation required to update the model is only bounded above by $K \max_{P \in \mathcal{H}, \sigma \in \Sigma} \{ c^P_\sigma \}$.  Therefore, to make the algorithm computationally tractable with arbitrary length sequences there must be an upper bound on $c^P_\sigma$.  We impose the stronger upper bound $\sum_{\sigma \in \Sigma} c_\sigma^P < k$ for fixed $k$.  The upper bound is enforced using the random decrement strategy developed in \cite{Bartlett2011} and implemented by Algorithm~\ref{alg:ThinCounts}.