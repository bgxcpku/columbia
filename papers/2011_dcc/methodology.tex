% !TEX root = deplump.tex
\section{Methodology}
\label{section:methodology}
\newcommand{\PY}{\ensuremath{\mathcal{P}\mathcal{Y}}}

\subsection{Review}

Note that a distribution $P$ over sequences can be factorized as $P(S = [s_0, s_1, \ldots, s_m]) = P(s_0)P_{[s_0]}(s_1)P_{[s_0,s_1]}(s_2) \ldots P_{[s_0,s_1,\ldots,s_{m-1}]}(s_m)$, where $P_u(s) = P(s | u)$.  The sequence memoizer jointly models these conditional distributions using a hierarchical Bayesian framework.  The hierarchy used to define the model makes use of the Pitman-Yor \citep{Pitman1997} distribution over distributions. If $P \sim \PY(d,c,G)$ then we say $P$ follows a Pitman-Yor distribution with discount parameter $d$, concentration parameter $c$, and mean distribution $G$.  The mean distribution $G$ can be understood as the average $P$ from this distribution, i.e. $\mathbb{E}(P(S)) = G(S)$ for $S \in \Sigma^{+}$.  The discount parameter is a real value between 0 and 1.  If the discount is close to 1 it is an indication that $P$ is likely to be very close to $G$; d close to 0 indicates $P$ may vary significantly from $G$.  In the sequence memoizer the concentration parameters are restricted to 0 and will be omitted from the remaining discussion. If we define the operator $\sigma$ on discrete sequences as $\sigma([s_0, s_1, \ldots, s_m]) = [s_1,s_2, \ldots, s_m])$ then we can write the sequence memoizer model as 
%
\begin{eqnarray*}
	P_{[ ]} &\sim& \PY(\delta_0,0,\mathcal{U}_{\Sigma})\\
	P_{u} &\sim& \PY(\delta_{|u|}, 0, P_{\sigma(u)})
\end{eqnarray*}
\noindent where $\mathcal{U}_{\Sigma}$ is the uniform distribution over $\Sigma$.  The hierarchy used in the model smooths each conditional distribution $P_u$ towards a related, more general distribution $P_{\sigma(u)}$.  Intuitively the hierarchy indicates that the most recent context is the most informative for modeling the conditional distributions.

Given a model and observed data,  the task of inference is to learn likely values of the latent parameters.  The latent parameters of the sequence memoizer are  $\mathcal{G} = \{P_u \ | \ u \in \Sigma^{+} \}$ and $\delta_n$ for $n \geq 0$.  While $| \mathcal{G}| = \infty$, \cite{Wood2009} show that if inference can be restricted to a set $\mathcal{H_\mathcal{S}} \subset \mathcal{G}$ for the training sequence $\mathcal{S}$ such that $|\mathcal{H}_\mathcal{S} | \leq 2 |\mathcal{S}|$.  Furthermore, they parameterize the discounts such that $\delta_n$ is a function of $\{\delta_0, \delta_1, \ldots, \delta_10 \}$ for $n \geq 0$.  



Inference in the model is performed using a compressed representation in which only two counts for each symbol need to be maintained for a subset of the conditional distributions.  We will refer to this subset at element $t$ of the sequence as $\mathcal{H}$.  The cardinality of $\mathcal{H}$ grows linearly in the length of the sequence.  Bayesian inference is typically performed by averaging over the posterior distributions of parameters, but \cite{Gasthaus2010} demonstrate that using a single particle approximation yields excellent empirical results. \cite{Bartlett2010}  show that by making some independence assumptions the approximation can be extended and the cardinality of $\mathcal{H}$ can be bounded from above to provide a constant space inference procedure without much loss in  empirical performance.

\subsection{Approximation}

The counts maintained in the inference procedure grow as a sub-linear function of the stream length, but are not bounded above.  Unfortunately the computation required to update the model as new data is observed grows as a linear function of these counts.  Therefore, to make the algorithm computationally tractable with a  stream, the sum of counts for each distribution in $\mathcal{H}$ must be bounded from above.  This bound is enforced in the algorithm by the function ThinCounts.