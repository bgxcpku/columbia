% !TEX root = main.tex
\section{Methodology}
\label{section_methodology}
%Any discrete sequence can be considered at different levels of modularity.  For example a paragraph of natural language can be considered as a sequence of characters, a sequence of words, or even a sequence of phrases or sentences.  Considering the sequence as a sequence of phrases is appealing based on our knowledge of language construction, but leads to an enormous number of distinct symbols in the discrete alphabet.  Using a model which exploits finer granularity, such as characters is computationally more tractable, but may miss longer ranging dependencies in the data.  We propose a model, defined recursively, to model discrete sequence data by considering it as a sequence of low granular chunks while we regularize the distribution over these low granular chunks using higher granular models.

\subsection{The Pitman-Yor Process\cite{someone}}
The Pitman-Yor process is a distribution over probability measures with three parameters.   If $\G \sim \PY(d,c,\G_0)$ we say $\G$ is distributed according to a Pitman-Yor process with discount parameter $d$, concentration parameter $c$, and base measure $\G_0$. In the case that $d = 0$ the Pitman-Yor process reduces to the Dirichlet process \cite{Pitman1997}.  To demonstrate its use consider the simple model
%
\begin{eqnarray*}
\G | d,c,\G_0 &\sim& \PY(d,c,\G_0)\\
\theta_i | \G &\sim& \G,  \hspace{.5cm} i = 1, \dots, N,
\end{eqnarray*}
%
where $\G$ is a random distribution distributed according to a Pitman-Yor prior and $\{ \theta_i \}_{i = 1}^N$ is an i.i.d. sample drawn from $\G$.  It is possible to draw the sample $\{ \theta_i \}_{i = 1}^N$ from this model with $\G$ analytically marginalized out.  The generative process is a sequential, dependent process known as the Chinese Restaurant Process (CRP).  The process can be described by considering the following analogy.  Imagine $N$ customers being seated sequentially in a Chinese restaurant with infinite capacity.  Customer $i$ sits at an occupied table $j$ with probability proportional to $n^i_j - d$, where $n^i_j$ is the number of customers already seated at table $j$ at time $i$, and chooses to sit alone with probability proportional to $t^i d + c$, where $t^i$ is the number of occupied tables at time $i$.  Each occupied table is served one dish which is independently sampled from the base distribution $\G_0$.  Finally, if customer $i$ is seated at a table serving dish $\psi$, then the parameter $\theta_i$ is given the value $\psi$.  

We can see from the generative process that samples from this model will naturally cluster together since many samples will have identical values.  Another feature of this generative process is that it is exchangeable. Therefore, the order in which customers entered the restaurant has no effect on the probability of the final seating arrangement. In a simple model like this where $\{\theta_i\}_{i = 1}^N$ are observed, this means that the seating assignment of each customer is set of latent parameters describing this model.  We can perform Gibbs sampling to estimate the posterior distribution of these parameters by resampling each customers seating assignment conditioned on the seating assignment of the others.  Since the model is exchangeable, each customer can in turn be thought of as the last customer entering the restaurant and the Gibbs step is straightforward.

\subsection{InDel Probability Model}
\label{subsection_indel_probability_model}
We would like to construct a family of distributions $\{ \likelihood( \cdot | y) \}_{y \in \Sigma^+}$ over $\Sigma^+$ based on insertion and deletion type edit operations.  Edit operations we will consider are insertions, deletions, substitutions, and matches.  The model we used is based on the CRF model for sequence alignment in \cite{McCallum, Ballare, and Pereira}. An alignment $a$ between $x,y \in \Sigma^+$ is defined as a three-tuple $\{a.e, a.ix, a.iy\}$.  The sequence $a.e$ is a sequence of edit operations such that $a.e[i]$ is either a $m$, an $i$, a $d$, or a $s$.  The sequences $a.ix$ and $a.iy$ are sequences of indices into the query and reference strings respectivly.  Each edit operation consumes an element of the reference sequence (deletion), the query sequence (insertion), or both (substitution and match) which means the sequences $a.ix$ and $a.iy$ start at 0 and are nondecreasing.  We say an alignment $a$ aligns $x,y$ if $a$ consumes both the read and query sequences which means that the last elements of $a.ix$ and $a.iy$ are equal to $|x|$ and $|y$ respectively and $a.e$ contain no more than $k$ edits which are not $m$.

%should include figure here to show alignments

To construct a probability model we use probability mass functions $\Phi_d(\cdot)$, $\Phi_i(\cdot)$ over $\Sigma$ and $\Phi_m(\cdot)$ and $\Phi_s(\cdot,\cdot)$ over $\Sigma \times \Sigma$, corresponding to deletion, insertion, match, and substitution edits respectively.  Furthermore, we will make use of positive constants $\Phi_s$, $\Phi_i$ $\Phi_d$ which are all less than one. Finally, for notational convenience we define the general potential function as
%
\[
\Phi(a.e[i], x[a.ix[i]], y[a.iy[i]]) = \left\{
\begin{array}{ll}
	\Phi_m \Phi_m(x[a.ix[i]], y[a.iy[i]])	& : \textrm{ if } a.e[i] \equiv m\\
	\Phi_s \Phi_s(x[a.ix[i]], y[a.iy[i]]) 									& : \textrm{ if } a.e[i] \equiv s\\
	\Phi_i \Phi_i(x[a.ix[i]]) 													& : \textrm{ if } a.e[i] \equiv i\\
	\Phi_d \Phi_d(y[a.iy[i]]) 												& : \textrm{ if } a.e[i] \equiv d\\
\end{array}
\right.
.
\]
%
We can now define a joint probability over reads and alignments given a query string $y$ as $P_k(x,a | y) \propto \Pi_{i = 1}^{|a|}\Phi(a.e[i],x[a.ix[i]], y[a.iy[i]]) \chi(a \textrm{ aligns } x,y)$.  It is then clear that we can define a probability model over reads by marginalizing over alignments, specifically $\likelihood_k(x | y) = \frac{1}{Z_y} \sum_a \Pi_{i = 1}^{|a|}P(x,a|y)$, where $Z_y$ is a normalizing constant.  The probability of a read $x$ under this model can be efficiently computed using a super obvious dynamic programming algorithm. 

\subsection{Generative Model}
\label{subsection_generative_model}
Given a set of symbols $\Sigma$ we will use the notation $\Sigma^+$ to denote the set of all finite length strings composed of symbols in $\Sigma$. Using the example of natural language, if $\Sigma$ is the set of all characters in the written English language, the set of all words is contained in $\Sigma^+$.  Our goal is to create a probability model on the space $\mathcal{X}$ of finite discrete sequences comprised of symbols in $\Sigma^+$, that is $\mathcal{X} = \{ [h_0, h_1, \ldots, h_K] \ || \ h_k \in \Sigma^+, \ K < \infty \}$.

Note that the probability of a sequence $x \in \mathcal{X}$ under the distribution $P$ can be factored as $P(x = [h_0, h_1, \ldots, h_K]) = P(h_0)P(h_1 | h_0) P(h_2 | h_0, h_1) \ldots, P(h_K | h_0, \ldots, h_{K-1})$.  From now on we will use subscript notation to denote conditioning, so that $P_{\vec u}(\cdot)$ is the conditional distribution $P(\cdot  |  \vec u)$. From this factorization we see that we can model discrete sequences by directly modeling the set of conditional distributions $\{P_x(\cdot)\}_{x \in \mathcal{X}}$.  The generative model we propose can be written succinctly as 
%
\[
\begin{array}{rcll}
\G_{\sigma([])}  &=& \HPY(\Sigma) \\
\G_x &\sim& \PY(d_{|x|},0,\G_{\sigma(x)}) & \forall \ x \in \mathcal{X} \\
P_x(h) &=& \sum_{h^\prime \in \Sigma^+}  \G_x(h^\prime) \likelihood(h | h^\prime) & \forall \ x \in \mathcal{X} \textrm{ and } h \in \Sigma^+.
\end{array}
\]
%
The model represents the conditional distribution $P_x(\cdot)$ as a mixture distribution where each mixture component is an InDel \cite{someone} probability model $\likelihood(\cdot | h^\prime)$ with parameter (reference sequence) $h^\prime \in \Sigma^+$.  The conditional distributions $\{ \G_x(\cdot) \}_{x \in \mathcal{X}}$ are discrete distributions over parameters $h^\prime \in \Sigma^+$.  In the model each conditional distribution $\G_x(\cdot)$ is a random distribution distributed according to a Pitman-Yor process law with discount parameter $d_{|x|}$, concentration parameter 0, and base distribution $\G_{\sigma(x)}$.  The function $\sigma(x)$ is a suffix operator for sequences such that $\sigma([x_0, x_1, \ldots, x_m]) = [x_1, \ldots, x_m]$.  The Pitman-Yor prior on the distribution $\G_{[]}$ has a hierarchical Pitman-Yor process distribution with alphabet $\Sigma$ as the base distribution parameter.  Incorporating the hierarchical Pitman-Yor process distribution over $\Sigma^+$ at the base of the model makes the model definition recursive and allows it to be extended to more and more granular levels of sequence decomposition.  For example, the model could be used to model phrases (ordered collections of words), such that the base distribution is a hierarchical Pitman-Yor process distribution over words, while the base of the model over words is a hierarchical Pitman-Yor process distribution over characters.

\subsection{Inference}
\label{subsection_inference}
The model we have proposed is a hierarchical mixture model like the Hierarchical Dirichlet Process Mixture model \cite{teh}.  Inference is similar to inference in the HDPMM aside from a few provisions adopted specifically for our context.  What follows is a brief description of inference in Hierarchical Dirichlet/Pitman-Yor Process mixture models followed by an examination of our specific updates.

Inference is done using the Chinese restaurant representation.  Each $G_x$ corresponds to a restaurant in a hierarchy of restaurants.  Customers within each restaurant are assigned to tables.  Each table is also a customer in the parent restaurant and the dish being served at the table is the same as the dish being served to the corresponding customer in the parent restaurant.  The parameters of the model can thus be seen to be the seating assignment of each customer in each restaurant and one universal list of dishes being served throughout the hierarchy of restaurants.  We can construct a Markov chain with the posterior distribution of the model parameters as its invariant distribution by using Gibb's sampling, Metropolis-Hastings or an auxiliary sampling scheme.  

Consider a Markov chains such that at each time in the chain each restaurant in the hierarchy has several customers assigned to one of perhaps several tables.  The resampling step means conditionally sampling each table assignment in each restaurant.  Consider one restaurant for now and consider the $i^{th}$ customer sitting at table $c_i$ being served dish $\psi_{c_i}$. We use an auxiliary variable sampling scheme similar to algorithm 8 in \cite{Neal sampling paper}.  The persistent variables in the Markov chain consist of the table assignments for each customer $c_1, c_2, \ldots, c_m$ and the labels assigned to occupied tables $\psi_1, \psi_2, \ldots, \psi_k$.  We first propose $m$ auxiliary variables by sampling $m$ independent samples from $\likelihood_1(\cdot | \psi_{c_i})$.  We then consider re-sampling the seating location restricting ourselves to current tables or sampled auxiliary tables using Gibb's sampling. We can see that the conditional prior is
%
\[
p_j = \left\{
\begin{array}{ll}
	\frac{n^-_j - d}{n-1+c} 			& : n^-_j > 0\\
	\frac{k^-d + c}{(m)(n-1+c)}		& : n^-_j =0 \ {\rm and } \ n^-_i > 0\\
	\frac{k^-d + c}{(m+1)(n-1+c)}	& : n^-_j =0 \ {\rm and } \ n^-_i = 0
\end{array}
\right. 
\]
%
where $n^-_j$ is the number of customers at table $j$ not including customer $i$ and  $k^-$ is the number of occupied tables once the $i^{\rm th}$ customer is unseated.  If the $i^{\rm th}$ customer is currently sitting alone we adjust the conditional prior to include an $m+1$ term instead of $m$ for unoccupied tables.  We can then re-sample the seating location of the $i^{\rm th}$ customer using the Gibb's distribution $P(c_i = j) \propto  (parent prob?) * p_j F(x_i | \psi_j)\Pi_{\{k : n^-_k = 0 \ {\rm and} \ k \neq j\}} \likelihood(\psi_k | \psi_j)$.
\[
% first assume c_i is not alone
%\frac{P(c_i^*) \Pi_{j = 1}^m \likelihood_1(}{P(\psi_{k^-+1} | \psic_i}
\]



