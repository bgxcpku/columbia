\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{A Nonparametric Bayesian Approach to Regular Grammar Induction}
\author{David Pfau}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
\section{Introduction}
N-gram models have had great empirical success in sequence prediction.  However, there is no clear way to trade off model complexity with prediction accuracy.  In extensions to n-gram models which can learn from arbitrarily long contexts, the model complexity will grow without bound, even for trivially simple sequences such as repetitions of a few characters.  Bounded memory models perform well in practice, but we would much prefer to learn a model that is as small as possible for very simple data, while growing large for more complex data.  A natural class of models to explore is probabalistic deterministic finite automata (PDFA), which contains n-grams as a special case, as well as simpler models.  We define a prior over PDFAs with a finite number of states and describe a Metropolis-Hastings algorithm to generate samples from the posterior.  We then generalize to the case of PDFAs with a (potentially) infinite number of states and show that the generative model is a type of Hierarchical Dirichlet Process (HDP).  We then (I hope!) describe a state splitting/merging algorithm for posterior inference that mixes more efficiently than the original Metropolis-Hastings algorithm, and show that it defines a natural hierarchy of states for smoothing (fingers crossed...)  The set of strings produced by a PDFA constitutes a probabilistic regular language, thus our inference procedure can also be viewed as a non-greedy algorithm for regular grammar induction.

\section{Probabilistic Deterministic Finite Automata}
A PDFA is formally defined as a 5-tuple $M = (Q,\Sigma,\delta,\pi,q_0)$.  $Q$ is a finite set of states. $\Sigma$ is a finite alphabet of observable symbols. $\delta\,:\,Q\times\Sigma\rightarrow Q$ is the transition function from a state/symbol pair to the next state.  $\pi\,:\,Q\times\Sigma\rightarrow[0,1]$ is the probability of the next symbol given a state.  $q_0$ is the initial state.  For clarity and minimal clutter, we use $i$ to index over elements of $Q$, $j$ to index over elements of $\Sigma$ and $t$ to index elements of an observed string.  For example, $\delta_{ij}$ is shorthand for $\delta(q_i,s_j)$.

The odd-sounding ``probabilistic deterministic" refers to the fact that a string of symbols is generated by a two-stage process.  Given a state $q_i$, the probability that the next symbol is $s$ is given by $\pi(q_i,s)$, hence ``probabilistic."  We denote the multinomial distribution over symbols given a state $q_i$ as $\pi_i$, so we could equivalently say $s \sim \pi_i$.  Given a state $q_i$ and a symbol $s$, however, there is a {\it unique} state $\delta(q_i,s)$ that follows it, hence ``deterministic."  

We can see that n-grams or nth-order Markov models are a particular class of PDFAs.  If we consider a state to be the prefix $x_1 x_2 \ldots x_{n-1}$, then given a state and a symbol $x_n$, the unique next state is $x_2 \ldots x_n$.  Thus nth-order Markov models are a subclass of PDFAs with $|\Sigma|^n$ states.

\section{A Generative Model for PDFAs}

We assume that $Q$, $\Sigma$ and $q_0$ are known, and define a prior over $\delta$ and $\pi$.  We place a Dirichlet prior over every next symbol probability, and assume that each next state given a state/symbol pair is drawn from a multinomial probability for that symbol, which also has a Dirichlet prior over it.  Formally:

\begin{eqnarray}
\mu|\gamma & \sim & \mathrm{Dir}\left(\gamma/|Q|,\ldots,\gamma/|Q|\right) \\
\phi_{j}|\alpha,\mathbf{\mu}  & \sim & \mathrm{Dir}(\alpha\mathbf{\mu}) \\
\pi_{i}|\beta & \sim & \mathrm{Dir}(\beta/|\Sigma|,\ldots,\beta/|\Sigma|)\\
\delta(q_i,s_j) & \sim & \phi_{j}
\end{eqnarray}

And from this we generate a sequence of $T$ symbols recursively:

\begin{eqnarray*}
q^0 & = & q_0 \\
x_0 & \sim & \pi^0 \\
q^t & = & \delta(q^{t-1},x_{t-1}) \\
x_t & \sim & \pi^t
\end{eqnarray*}

With likelihood

\[ p(x_{0:T}|\delta,\pi) = \pi(q^0,x_0)\prod_{t=1}^T \pi(q^t,x_t) \]

Thanks to Multinomial-Dirichlet conjugacy we can integrate out $\pi$, $\mu$ and $\phi$ and express the probability of a sequence given a transition function as follows:

\begin{eqnarray}
 p(x_{0:T}|\delta,\beta) & = & \int p(x_{0:T}|\pi,\delta) p(\pi|\beta) d\pi \\
 & = &  \int \prod_{j=1}^{|Q|} \frac{\Gamma(\beta)}{\Gamma(\frac{\beta}{|\Sigma|})^{|\Sigma|}} \gamma(q_j,s_1)^{\frac{\beta}{|\Sigma|}+c_{j1}-1} \gamma(q_j,s_2)^{\frac{\beta}{|\Sigma|}+c_{j2}-1} \ldots \gamma(q_j,s_{|\Sigma|})^{\frac{\beta}{|\Sigma|}+c_{j|\Sigma|}-1} \\
 & = & \prod_{j=1}^{|Q|} \frac{\Gamma(\beta)}{\Gamma(\frac{\beta}{|\Sigma|})^{|\Sigma|}} \frac{\prod_{i=1}^{|\Sigma|}\Gamma(\frac{\beta}{|\Sigma|} + c_{ji})}{\Gamma(\beta + \sum_{i=1}^{|\Sigma|} c_{ji})}
 \end{eqnarray}
 
 Where $c_{ji}$ is the number of times the symbol $s_i$ is emitted when in the state $q_j$.  
 
 For the transition matrix, the two-stage Dirichlet prior has the effect of tying together distributions over different columns.  This means that the state/symbol transitions are more similar for transitions from the same symbol, but are still shared across completely different state/symbol pairs.  As $|Q|\rightarrow\infty$, Equations 1 and 2 (*replace this with LaTeX equation titles*) have a well-defined limit as long as the number of observations is finite.  This is the Hierarchical Dirichlet Process.  In the case of posterior inference of $\delta$, when only a finite number of state/symbol pairs are visited by the data, we may integrate out all other elements of $\delta$, and the limit in the case of infinite states is well-defined.
 
We can sample incrementally from the joint distribution over $x_{0:T}$ and $\delta$ when $\phi_j$, $\mu$ and $\pi_i$ are integrated out in the $|Q|\rightarrow\infty$ limit.  From the start state $q_0$, we sample a symbol $s_{j_0}$ uniformly and assign $\delta_{0j_0}$ to a new state.  If $q^t = q_i$ then $x_t$ has the probability

 \[P(x_t=s_j|q_i,x_{0:t-1},q^{0:t-1}) = \frac{c_{ij}+\frac{\beta}{|\Sigma|}}{c_{i\cdot} + \beta}\]
 
 where $c_{ij}$ is the number of times so far $s_j$ was emitted from $q_i$ and $c_{i\cdot}$ is the total number of times $q_i$ has been visited so far.  
 
If the state/symbol pair $(q_i,s_j)$ has not been visited before, we have to sample $\delta_{ij}$.  The two-stage generative procedure for elements of $\delta$ means that we have to keep track of counts at two levels.  Each $\delta_{ij}$ belongs to a cluster $v_{kj}$ that contains other $\delta_{i'j}$, while each $v_{kj}$ belongs to a top-level cluster $w_{l}$ that has elements across all $j$.  Each top level cluster has one $q \in Q$ assigned to it, and $\delta_{ij}$ is equal to that $q$ in the top cluster that the cluster with $\delta_{ij}$ belongs to (*might want to make this part clearer...add a figure*).  Let $\delta^t$ denote the elements of $\delta$ that have been visited at time $t$.  as follows:
 
\[P(\delta_{ij} = k|\delta^t) \propto \cases{ & if $k \le |\delta^t|$ \cr  & if $k > |\delta^t|$}\]
 
 This process for sampling from an HDP when $\mu$ and $\phi_j$ are integrated out is known as the {\em Chinese Restaurant Franchise Process}.
 
 \section{Posterior Inference over PDFAs}
 
 We perform posterior inference by sampling assignments for $\delta_{ij}$ individually.  Rather than ordinary Gibbs sampling, we use a mixed Gibbs/Metropolis-Hastings update.  To see why, consider the following case:  $\delta_{ij}$ is the only sampled element of $\delta$ that is assigned to the state $q_{i'}$.  When we removed $\delta_{ij}$ from the counts to sample it, the probability of assigning it back to $q_{i'}$ becomes zero, and even if it is assigned to some new state $q_{i''}$, the probability that $\delta_{i'j} = \delta_{i''j}$ for all $j$ visited by the data is low.  We do not want to forget a good sample, so instead we propose a new $\delta_{ij}$ and accept or reject according to the usual Metropolis-Hastings ratio.
 
Samples from the CRF are exchangeable, so we can remove $\delta_{ij}$ and propose a sample $\delta{ij}^*$ according to the CRF given $\delta_{-ij}^T$, the elements of $\delta$ visited by $x_{0:T}$ excluding $\delta_{ij}$.  This is the prior probability excluding the data, which cancels with the equivalent term in the posterior, meaning that the accept probability $\alpha(\delta_{ij},\delta_{ij}^*)$ is given by the ratio of the likelihood of the data

\[ \alpha(\delta_{ij},\delta_{ij}^*) =  \mathrm{min}\left(1,\frac{p(x_{0:T}|\delta_{ij}^*,\delta_{-ij}^T)}{p(x_{0:T}|\delta_{ij},\delta_{-ij}^T)}\right)\]

In general the numerator cannot be evaluated because changing $\delta_{ij}$ means changing the entire sequence of states visited by the data after $\delta_{ij}$ is first visited.  In practice we estimate the numerator by Monte Carlo approximation, sampling elements of $\delta$ as they are first visited by the data.
 
 %Similarly, for a given transition matrix the probability is
 
% \[ p(\delta|\alpha) = \prod_{i=1}^{|\Sigma|} \frac{\Gamma(\alpha)}{\Gamma(\frac{\alpha}{|Q|})^{|Q|}} \frac{\prod_{j=1}^{|Q|}\Gamma(\frac{\alpha}{|Q|} + n_{ji})}{\Gamma(\alpha + |Q|)} \]
 
% Where $n_{ji}$ is the number of times the state $q_j$ appears in the $i$th column of the transition matrix.  Note that the counts $n_{ji}$ depend only on the transition matrix $\delta$ while the counts $c_{ji}$ depend on both $\delta$ and the data.  From this we can construct a Gibbs sampler for the next state transition matrix.  The conditional probability of one entry in the transition matrix is given below.  We write $\delta(q_j,s_i)$ as $\delta_{ji}$ for compactness, and write the rest of the matrix as $\delta_{-ji}$ .
 
 %\[ p(\delta_{ji} = q_k | \delta_{-ji},x_{0:T},\alpha,\beta) \propto p(x_{0:T}|\delta_{ji}=q_k,\delta_{-ji},\beta) p(\delta_{ji}=q_k,\delta_{-ji}|\alpha) \]

%The case of smoothed n-gram models is more complicated.  Smoothing works by considering more general contexts for which we have more observations.  In the case of an n-gram model, this would correspond to prefixes with fewer than $n-1$ characters.  For example, the predictive distribution given $xy$ is smoothed by the distribution given $y$, which is smoothed by the distribution given no context.  From the DFA point of view, these contexts can be seen as transient states: once we observe more than $n-1$ characters of a sequence, every prefix will be long enough to uniquely assign it to some recurrent state (In the example above, we only observe the null context at the very beginning of a string, and the context $y$ one character into a string).  Thus the way to strengthen the predictive power of learned DFAs is to smooth recurrent states by corresponding transient states.  I haven't yet figured out how to identify which transient states go with which recurrent states, but there is no reason to think it would be infeasible.  I should note, tying together states in this way would go above and beyond Causal State Splitting Reconstruction.  CSSR does not tie predictive distributions together, and simply discards transient states.

%\subsection{}



\end{document}  